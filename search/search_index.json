{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atlas \u00b6 Atlas was developed by Netflix to manage dimensional time series data for near real-time operational insight. Atlas features in-memory data storage, allowing it to gather and report very large numbers of metrics, very quickly. Atlas captures operational intelligence. Whereas business intelligence is data gathered for analyzing trends over time, operational intelligence provides a picture of what is currently happening within a system. Atlas was built because the existing systems Netflix was using for operational intelligence were not able to cope with the increase in metrics we were seeing as we expanded our operations in the cloud. In 2011, we were monitoring 2 million metrics related to our streaming systems. By 2014, we were at 1.2 billion metrics and the numbers continue to rise. Atlas is designed to handle this large quantity of data and can scale with the hardware we use to analyze and store it. For details and background on the project please read through the overview page. Check out the getting started page for an introduction to using Atlas in the cloud environment. Once you've explored the example, check out the stack language references to see the various types of information you can access.","title":"Introduction"},{"location":"#atlas","text":"Atlas was developed by Netflix to manage dimensional time series data for near real-time operational insight. Atlas features in-memory data storage, allowing it to gather and report very large numbers of metrics, very quickly. Atlas captures operational intelligence. Whereas business intelligence is data gathered for analyzing trends over time, operational intelligence provides a picture of what is currently happening within a system. Atlas was built because the existing systems Netflix was using for operational intelligence were not able to cope with the increase in metrics we were seeing as we expanded our operations in the cloud. In 2011, we were monitoring 2 million metrics related to our streaming systems. By 2014, we were at 1.2 billion metrics and the numbers continue to rise. Atlas is designed to handle this large quantity of data and can scale with the hardware we use to analyze and store it. For details and background on the project please read through the overview page. Check out the getting started page for an introduction to using Atlas in the cloud environment. Once you've explored the example, check out the stack language references to see the various types of information you can access.","title":"Atlas"},{"location":"getting-started/","text":"The instructions on this page are for quickly getting a sample backend server running on a local machine. For other common tasks see: Querying Data: Examples Tutorial Instrumenting Code Run a Demo Instance \u00b6 Prerequisites These instructions assume a unix based machine with curl . Other systems may work, but have not been tried. Java 8 or higher is required. To quickly run a version with some synthetic sample data: $ curl -LO https://github.com/Netflix/atlas/releases/download/v1.6.5/atlas-1.6.5-standalone.jar $ java -jar atlas-1.6.5-standalone.jar Explore Available Tags \u00b6 The tags API is used to explore available tags and the relationships between them. # show all tags $ curl -s 'http://localhost:7101/api/v1/tags' # show all values of the name, nf.app and type tags $ curl -s 'http://localhost:7101/api/v1/tags/name' $ curl -s 'http://localhost:7101/api/v1/tags/nf.app' $ curl -s 'http://localhost:7101/api/v1/tags/type' # show all name tags that also have the type tag $ curl -s 'http://localhost:7101/api/v1/tags/name?q=type,:has' # show all name tags that have an nf.app tag with a value of nccp $ curl -s 'http://localhost:7101/api/v1/tags/name?q=nf.app,nccp,:eq' Generate Graphs \u00b6 These graph API URLs show off a couple of the capabilities of the Atlas backend. See the Examples page for more detailed use cases. # graph all metrics with a name tag value of ssCpuUser, using an :avg aggregation $ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg' # duplicate the ssCpuUser signal, check if it is greater than 22.8 and display the result as a vertical span with 30% alpha $ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg,:dup,22.8,:gt,:vspan,30,:alpha' Running Demo with Memory Storage \u00b6 Run an instance with a configuration to use the memory storage: $ curl -Lo memory.conf https://raw.githubusercontent.com/Netflix/atlas/v1.6.x/conf/memory.conf $ java -jar atlas-1.6.5-standalone.jar memory.conf Now we can send some data to it. To quickly get started there is a sample script to send in some data: $ curl -Lo publish-test.sh https://raw.githubusercontent.com/Netflix/atlas/v1.6.x/scripts/publish-test.sh $ chmod 755 publish-test.sh $ ./publish-test.sh Then view the data in a web browser: $ open 'http://localhost:7101/api/v1/graph?q=name,randomValue,:eq,:sum,(,name,),:by'","title":"Getting Started"},{"location":"getting-started/#run-a-demo-instance","text":"Prerequisites These instructions assume a unix based machine with curl . Other systems may work, but have not been tried. Java 8 or higher is required. To quickly run a version with some synthetic sample data: $ curl -LO https://github.com/Netflix/atlas/releases/download/v1.6.5/atlas-1.6.5-standalone.jar $ java -jar atlas-1.6.5-standalone.jar","title":"Run a Demo Instance"},{"location":"getting-started/#explore-available-tags","text":"The tags API is used to explore available tags and the relationships between them. # show all tags $ curl -s 'http://localhost:7101/api/v1/tags' # show all values of the name, nf.app and type tags $ curl -s 'http://localhost:7101/api/v1/tags/name' $ curl -s 'http://localhost:7101/api/v1/tags/nf.app' $ curl -s 'http://localhost:7101/api/v1/tags/type' # show all name tags that also have the type tag $ curl -s 'http://localhost:7101/api/v1/tags/name?q=type,:has' # show all name tags that have an nf.app tag with a value of nccp $ curl -s 'http://localhost:7101/api/v1/tags/name?q=nf.app,nccp,:eq'","title":"Explore Available Tags"},{"location":"getting-started/#generate-graphs","text":"These graph API URLs show off a couple of the capabilities of the Atlas backend. See the Examples page for more detailed use cases. # graph all metrics with a name tag value of ssCpuUser, using an :avg aggregation $ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg' # duplicate the ssCpuUser signal, check if it is greater than 22.8 and display the result as a vertical span with 30% alpha $ curl -Lo graph.png 'http://localhost:7101/api/v1/graph?q=name,ssCpuUser,:eq,:avg,:dup,22.8,:gt,:vspan,30,:alpha'","title":"Generate Graphs"},{"location":"getting-started/#running-demo-with-memory-storage","text":"Run an instance with a configuration to use the memory storage: $ curl -Lo memory.conf https://raw.githubusercontent.com/Netflix/atlas/v1.6.x/conf/memory.conf $ java -jar atlas-1.6.5-standalone.jar memory.conf Now we can send some data to it. To quickly get started there is a sample script to send in some data: $ curl -Lo publish-test.sh https://raw.githubusercontent.com/Netflix/atlas/v1.6.x/scripts/publish-test.sh $ chmod 755 publish-test.sh $ ./publish-test.sh Then view the data in a web browser: $ open 'http://localhost:7101/api/v1/graph?q=name,randomValue,:eq,:sum,(,name,),:by'","title":"Running Demo with Memory Storage"},{"location":"overview/","text":"Atlas is the system Netflix uses to manage dimensional time-series data for near real-time operational insight. It was primarily created to address issues with scale and query capability in the previous system. History \u00b6 In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume: Rolling pushes to Red/Black deployments . Leveraging auto-scaling for large clusters . Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe . This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed. Since that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years: The growth in raw volume required increased query capability to actually use the data. Goals \u00b6 The main goals for Atlas were to build a system that provided: A Common API Scale Dimensionality Common API \u00b6 Epic did a number of things really well that we didn't want to lose when transitioning. In particular: Normalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking Many of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data. Scale \u00b6 As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request. Dimensionality \u00b6 This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like: com.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US That breaks down to: Key Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US Since it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions. Query Layer \u00b6 In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup: We have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point. Allowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system. Stack Language \u00b6 One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include: Embedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency Graph Example \u00b6 To illustrate, this is a sample graph image: This graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing . If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability): http://atlas/api/v1/graph ?tz=UTC &e=2012-01-01T08:00 &s=e-8h &w=500 &h=150 &l=0 &q=nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, :dup,10,0.1,0.02,:des, 0.85,:mul, :2over,:lt, :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend Adding some comments to the stack expression to explain a bit what is going on: # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction # using the copy on the top of the stack. For # a description of the parameters see the DES # reference page. 10,0.1,0.02,:des, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Before After # 4. 4. actual # 3. 3. prediction # 2. actual 2. actual # 1. prediction 1. prediction :2over, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend See the stack language page for more information. Memory Storage \u00b6 Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap. Speed \u00b6 The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small. As an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line. Though obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation. Resilience \u00b6 What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb: Data becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully As a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain. With this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn't need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror. The publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API . Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can't update data we would have partial loss for a single shard if the same shard was missing in another mirror. Historical data can also fail in which case graphs would not be able to show data for some older windows. This doesn't have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn't show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle. After data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters. Cost \u00b6 Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs. The first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica. The second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly: Cluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases Using these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed. Snapshot of index sizes for one region in our environment: < 6h < 4d < 16d Ecosystem \u00b6 Internally there is a lot of tooling and infrastructure built up around Atlas. We are planning to open source many of these tools as time permits. This project is the first step for that with the query layer and some of the in-heap memory storage. Some additional parts that should come in the future: User interfaces Main UI for browsing data and constructing queries. Dashboards Alerts Platform Inline aggregation of reported data before storage layer Storage options using off-heap memory and lucene Percentile backend Publish and persistence applications EMR processing for computing rollups and analysis Poller for SNMP, healthchecks, etc Client Supports integrating servo with Atlas Local rollups and alerting Analytics Metrics volume report Canary analysis Outlier and anomaly detection These projects were originally developed and run internally and thus only needed to be setup by our team and assume many internal infrastructure pieces to run. There is a goal to try and make this easier, but it will take some time.","title":"Overview"},{"location":"overview/#history","text":"In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume: Rolling pushes to Red/Black deployments . Leveraging auto-scaling for large clusters . Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe . This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed. Since that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years: The growth in raw volume required increased query capability to actually use the data.","title":"History"},{"location":"overview/#goals","text":"The main goals for Atlas were to build a system that provided: A Common API Scale Dimensionality","title":"Goals"},{"location":"overview/#common-api","text":"Epic did a number of things really well that we didn't want to lose when transitioning. In particular: Normalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking Many of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data.","title":"Common API"},{"location":"overview/#scale","text":"As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request.","title":"Scale"},{"location":"overview/#dimensionality","text":"This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like: com.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US That breaks down to: Key Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US Since it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions.","title":"Dimensionality"},{"location":"overview/#query-layer","text":"In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup: We have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point. Allowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system.","title":"Query Layer"},{"location":"overview/#stack-language","text":"One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include: Embedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency","title":"Stack Language"},{"location":"overview/#graph-example","text":"To illustrate, this is a sample graph image: This graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing . If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability): http://atlas/api/v1/graph ?tz=UTC &e=2012-01-01T08:00 &s=e-8h &w=500 &h=150 &l=0 &q=nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, :dup,10,0.1,0.02,:des, 0.85,:mul, :2over,:lt, :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend Adding some comments to the stack expression to explain a bit what is going on: # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction # using the copy on the top of the stack. For # a description of the parameters see the DES # reference page. 10,0.1,0.02,:des, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Before After # 4. 4. actual # 3. 3. prediction # 2. actual 2. actual # 1. prediction 1. prediction :2over, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend See the stack language page for more information.","title":"Graph Example"},{"location":"overview/#memory-storage","text":"Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap.","title":"Memory Storage"},{"location":"overview/#speed","text":"The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small. As an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line. Though obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation.","title":"Speed"},{"location":"overview/#resilience","text":"What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb: Data becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully As a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain. With this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn't need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror. The publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API . Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can't update data we would have partial loss for a single shard if the same shard was missing in another mirror. Historical data can also fail in which case graphs would not be able to show data for some older windows. This doesn't have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn't show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle. After data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters.","title":"Resilience"},{"location":"overview/#cost","text":"Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs. The first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica. The second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly: Cluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases Using these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed. Snapshot of index sizes for one region in our environment: < 6h < 4d < 16d","title":"Cost"},{"location":"overview/#ecosystem","text":"Internally there is a lot of tooling and infrastructure built up around Atlas. We are planning to open source many of these tools as time permits. This project is the first step for that with the query layer and some of the in-heap memory storage. Some additional parts that should come in the future: User interfaces Main UI for browsing data and constructing queries. Dashboards Alerts Platform Inline aggregation of reported data before storage layer Storage options using off-heap memory and lucene Percentile backend Publish and persistence applications EMR processing for computing rollups and analysis Poller for SNMP, healthchecks, etc Client Supports integrating servo with Atlas Local rollups and alerting Analytics Metrics volume report Canary analysis Outlier and anomaly detection These projects were originally developed and run internally and thus only needed to be setup by our team and assume many internal infrastructure pieces to run. There is a goal to try and make this easier, but it will take some time.","title":"Ecosystem"},{"location":"api/fetch/","text":"","title":"Fetch"},{"location":"api/tags/","text":"This page is a reference for the tags API provided by Atlas. URI \u00b6 /api/v1/tags?q=<expr>&[OPTIONS] Query Parameters \u00b6 Callback (callback) \u00b6 If the format is json , the callback is used for providing JSONP output. This parameter is ignored for all other formats. Format (format) \u00b6 Specifies the output format to use. The default is json . Value Description json Outputs the graph data as a JSON object. txt Uses mime-type text/plain so it will render in the browser. Limit (limit) \u00b6 Maximum number of results to return before paging the response. If the response is paged a x-nflx-atlas-next-offset will be set to indicate the next offset. Pass the value with an offset param to get the next part of the list. If the header is not present there is no more data. Offset (offset) \u00b6 If the response is paged this param is used to indicate where the next request should pick up from. Query (q) \u00b6 Query expression used to select a set of metrics and manipulate them for presentation in a graph. The query expression can use query and std commands described in the reference.","title":"Tags"},{"location":"api/tags/#uri","text":"/api/v1/tags?q=<expr>&[OPTIONS]","title":"URI"},{"location":"api/tags/#query-parameters","text":"","title":"Query Parameters"},{"location":"api/tags/#callback-callback","text":"If the format is json , the callback is used for providing JSONP output. This parameter is ignored for all other formats.","title":"Callback (callback)"},{"location":"api/tags/#format-format","text":"Specifies the output format to use. The default is json . Value Description json Outputs the graph data as a JSON object. txt Uses mime-type text/plain so it will render in the browser.","title":"Format (format)"},{"location":"api/tags/#limit-limit","text":"Maximum number of results to return before paging the response. If the response is paged a x-nflx-atlas-next-offset will be set to indicate the next offset. Pass the value with an offset param to get the next part of the list. If the header is not present there is no more data.","title":"Limit (limit)"},{"location":"api/tags/#offset-offset","text":"If the response is paged this param is used to indicate where the next request should pick up from.","title":"Offset (offset)"},{"location":"api/tags/#query-q","text":"Query expression used to select a set of metrics and manipulate them for presentation in a graph. The query expression can use query and std commands described in the reference.","title":"Query (q)"},{"location":"api/time-parameters/","text":"APIs that accept time ranges support three parameters: Start time ( s ) End time ( e ) Time zone ( tz ) Time Zone \u00b6 Time zone can be any valid time zone id string. Time \u00b6 Absolute Times \u00b6 Absolute times can be specified by name or as a timestamp . Named Times \u00b6 Named times are references that will get resolved to a timestamp when a query is executed. For example, with graphs it is common to set the end time to now . Name Description s User specified start time. Can only be used as part of the end parameter. e User specified end time. Can only be used as part of the start parameter. now Current time. epoch January 1, 1970 UTC. Timestamps \u00b6 Explicit timestamps can use the following formats : Format Description %Y-%m-%d Date using the timezone for the query. The time will be 00:00. %Y-%m-%dT%H:%M Date time using the timezone for the query. The seconds will be 00. %Y-%m-%dT%H:%M:%S Date time using the timezone for the query. %s Seconds since January 1, 1970 UTC. %s (ms) Milliseconds since January 1, 1970 UTC. For times since the epoch both seconds and milliseconds are supported because both are in common use and it helps to avoid confusion when copy and pasting from another source. Values less than or equal 2,147,483,648 (2 31 ) will be treated as a timestamp in seconds. Values above that will be treated as a timestamp in milliseconds. So times from the epoch to 1970-01-25T20:31:23 cannot be represented in the millisecond form. In practice, this limitation has not been a problem. The first three formats above can also be used with an explicit time zone . Zone Offsets \u00b6 An explicit time zone can be specified as Z to indicate UTC or by using an offset in hours and minutes. For example: 2012-01-12T01:37Z 2012-01-12T01:37-00:00 2012-01-12T01:37-07:00 2012-01-12T01:37-07:42 A common format recommended for logs at Netflix is an ISO timestamp in UTC: 2012-01-12T01:37:27Z These can be copy and pasted to quickly check a graph for a timestamp from a log file. For practical purposes in Atlas a -00:00 offset timezone can be thought of as UTC, but depending on the source may have some additional meaning . Relative Times \u00b6 Relative times consist of a named time used for an anchor and an offset duration. <named=time> '-' <duration> <named-time> '+' <duration> For example: Pattern Description now-1w One week ago. e-1w One week before the end time. s+6h Six hours after the start time. s+P2DT6H5M Two days, 6 hours, and 5 minutes after the start time. Durations \u00b6 Duration vs Period \u00b6 This section is using the definition of duration and period from the java time libraries . In short: Durations are a fixed number of seconds. Periods represent a length of time in a given calendar. For example, the length of a day will vary if there is a daylight savings transition. The offset used for relative times in Atlas are durations because: It is primarily focused on shorter time spans (~ 2 weeks) where drift is less of an issue. In this range the variation is most commonly seen for the daylight savings time transitions. For time shifts day over day and week over week are the most common for operational purposes. During daylight savings time transitions a fixed duration seems to cause the least confusion, especially when the transition time is within the window being displayed. The primary use-case where periods were found to be more beneficial and less confusing is for week over week when looking at a small window that does not include the transition. In those cases if the signal reflects human behavior, such as playing movies, then the week over week pattern will typically line up better if using a period. Simple Duration \u00b6 A simple offset uses a positive integer followed by one of these units: s , second , or seconds m , min , minute , or minutes h , hour , or hours d , day , or days w , week , or weeks month or months y , year , or years All durations are a fixed number of seconds. A day is 24 hours, week is 7 days, month is 30 days, and a year is 365 days. ISO Duration \u00b6 The duration can also be specified as an ISO duration string , but day ( D ) is the largest part that can be used within the duration. Others such as week ( W ), month ( M ), and year ( Y ) are not supported. Examples: Pattern Description P1D One day of exactly 24 hours. P1DT37M One day and 37 minutes. PT5H6M Five hours and six minutes. For more details see docs on parsing durations .","title":"Time Parameters"},{"location":"api/time-parameters/#time-zone","text":"Time zone can be any valid time zone id string.","title":"Time Zone"},{"location":"api/time-parameters/#time","text":"","title":"Time"},{"location":"api/time-parameters/#absolute-times","text":"Absolute times can be specified by name or as a timestamp .","title":"Absolute Times"},{"location":"api/time-parameters/#named-times","text":"Named times are references that will get resolved to a timestamp when a query is executed. For example, with graphs it is common to set the end time to now . Name Description s User specified start time. Can only be used as part of the end parameter. e User specified end time. Can only be used as part of the start parameter. now Current time. epoch January 1, 1970 UTC.","title":"Named Times"},{"location":"api/time-parameters/#timestamps","text":"Explicit timestamps can use the following formats : Format Description %Y-%m-%d Date using the timezone for the query. The time will be 00:00. %Y-%m-%dT%H:%M Date time using the timezone for the query. The seconds will be 00. %Y-%m-%dT%H:%M:%S Date time using the timezone for the query. %s Seconds since January 1, 1970 UTC. %s (ms) Milliseconds since January 1, 1970 UTC. For times since the epoch both seconds and milliseconds are supported because both are in common use and it helps to avoid confusion when copy and pasting from another source. Values less than or equal 2,147,483,648 (2 31 ) will be treated as a timestamp in seconds. Values above that will be treated as a timestamp in milliseconds. So times from the epoch to 1970-01-25T20:31:23 cannot be represented in the millisecond form. In practice, this limitation has not been a problem. The first three formats above can also be used with an explicit time zone .","title":"Timestamps"},{"location":"api/time-parameters/#zone-offsets","text":"An explicit time zone can be specified as Z to indicate UTC or by using an offset in hours and minutes. For example: 2012-01-12T01:37Z 2012-01-12T01:37-00:00 2012-01-12T01:37-07:00 2012-01-12T01:37-07:42 A common format recommended for logs at Netflix is an ISO timestamp in UTC: 2012-01-12T01:37:27Z These can be copy and pasted to quickly check a graph for a timestamp from a log file. For practical purposes in Atlas a -00:00 offset timezone can be thought of as UTC, but depending on the source may have some additional meaning .","title":"Zone Offsets"},{"location":"api/time-parameters/#relative-times","text":"Relative times consist of a named time used for an anchor and an offset duration. <named=time> '-' <duration> <named-time> '+' <duration> For example: Pattern Description now-1w One week ago. e-1w One week before the end time. s+6h Six hours after the start time. s+P2DT6H5M Two days, 6 hours, and 5 minutes after the start time.","title":"Relative Times"},{"location":"api/time-parameters/#durations","text":"","title":"Durations"},{"location":"api/time-parameters/#duration-vs-period","text":"This section is using the definition of duration and period from the java time libraries . In short: Durations are a fixed number of seconds. Periods represent a length of time in a given calendar. For example, the length of a day will vary if there is a daylight savings transition. The offset used for relative times in Atlas are durations because: It is primarily focused on shorter time spans (~ 2 weeks) where drift is less of an issue. In this range the variation is most commonly seen for the daylight savings time transitions. For time shifts day over day and week over week are the most common for operational purposes. During daylight savings time transitions a fixed duration seems to cause the least confusion, especially when the transition time is within the window being displayed. The primary use-case where periods were found to be more beneficial and less confusing is for week over week when looking at a small window that does not include the transition. In those cases if the signal reflects human behavior, such as playing movies, then the week over week pattern will typically line up better if using a period.","title":"Duration vs Period"},{"location":"api/time-parameters/#simple-duration","text":"A simple offset uses a positive integer followed by one of these units: s , second , or seconds m , min , minute , or minutes h , hour , or hours d , day , or days w , week , or weeks month or months y , year , or years All durations are a fixed number of seconds. A day is 24 hours, week is 7 days, month is 30 days, and a year is 365 days.","title":"Simple Duration"},{"location":"api/time-parameters/#iso-duration","text":"The duration can also be specified as an ISO duration string , but day ( D ) is the largest part that can be used within the duration. Others such as week ( W ), month ( M ), and year ( Y ) are not supported. Examples: Pattern Description P1D One day of exactly 24 hours. P1DT37M One day and 37 minutes. PT5H6M Five hours and six minutes. For more details see docs on parsing durations .","title":"ISO Duration"},{"location":"api/graph/anonymization/","text":"Occasionally it is useful to show a graph, but the exact values need to be suppressed. This can be useful for communicating with external support or including in a presentation. To avoid showing the actual values disable tick labels using tick_labels=off and either disable the legend or disable the legend stats . /api/v1/graph? e=2012-01-01T00:00 &no_legend_stats=1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels=off If you also want to suppress the time axis, then use the only_graph option: /api/v1/graph? e=2012-01-01T00:00 & only_graph=1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w","title":"Anonymization"},{"location":"api/graph/axis-bounds/","text":"The upper and lower bounds for an axis can be set to an explicit floating point value or: auto-style : automatically determine the bounds based on the data and the style settings for that data. In particular, if the line style is area or stack , then the bounds will be adjusted to show the filled area. This is the default behavior. auto-data : automatically determine the bounds based on the data. This will only take into account the values of the lines. In the case of stack it will account for the position of the stacked lines, but not the filled area. When selecting bounds it is important to think about how it can impact the perception of what is shown. Automatic bounds can be useful for zooming in on the data, but can also lead to mis-perceptions for someone quickly scanning a dashboard. Consider these two graphs showing percent CPU usage on an instance: Automatic Bounds Explicit Bounds The automatic bounds allows us to see much more detail, but could lead a casual observer to think there were frequent large spikes in CPU usage rather than just noise on a machine with very little load. Default Lower \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by Default Lower Stack \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack Default Upper \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :neg Default Upper Stack \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :neg , :stack Explicit Bounds \u00b6 Note the &l=0 and &u=60e3 parameters. /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &l=0 &u=60e3 &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by Note It is possible to define the boundaries beyond the range of the data source so that a graph appears empty. Auto Lower \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC & l=auto-data & &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack Auto Upper \u00b6 /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC & u=auto-data & &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack","title":"Axis Bounds"},{"location":"api/graph/axis-bounds/#default-lower","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by","title":"Default Lower"},{"location":"api/graph/axis-bounds/#default-lower-stack","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack","title":"Default Lower Stack"},{"location":"api/graph/axis-bounds/#default-upper","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :neg","title":"Default Upper"},{"location":"api/graph/axis-bounds/#default-upper-stack","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :neg , :stack","title":"Default Upper Stack"},{"location":"api/graph/axis-bounds/#explicit-bounds","text":"Note the &l=0 and &u=60e3 parameters. /api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC &l=0 &u=60e3 &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by Note It is possible to define the boundaries beyond the range of the data source so that a graph appears empty.","title":"Explicit Bounds"},{"location":"api/graph/axis-bounds/#auto-lower","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC & l=auto-data & &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack","title":"Auto Lower"},{"location":"api/graph/axis-bounds/#auto-upper","text":"/api/v1/graph? e=2012-01-01T09:00 &s=e-1d &tz=UTC & u=auto-data & &q= name,sps, :eq , nf.cluster,(,nccp-xbox,nccp-silverlight,), :in , :and , :sum , (,nf.cluster,), :by , :stack","title":"Auto Upper"},{"location":"api/graph/axis-scale/","text":"Scales determine how the data value for a line will get mapped to the Y-Axis. There are currently four scales that can be used for an axis: Linear Logarithmic Power of 2 Square Root Linear \u00b6 A linear scale uniformly maps the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v+b where m and b are automatically chosen based on the domain and range. This is the default scale for an axis and will get used if no explicit scale is set. Since 1.6, it can also be used explicitly: /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=linear Logarithmic \u00b6 A logarithmic scale emphasizes smaller values when mapping the input values (domain) to the Y-axis location (range). This is often used if two lines with significantly different magnitudes are on the same axis. If v is datapoint in a time series, then y=m*log(v)+b where m and b are automatically chosen based on the domain and range. In many cases, using a separate Y-axis can be a better option that doesn't distort the line as much. To use this mode, add scale=log (prior to 1.6 use o=1 ). /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=log Power of 2 \u00b6 Since 1.6. A power scale that emphasizes larger values when mapping the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v 2 +b where m and b are automatically chosen based on the domain and range. To emphasize smaller values see the square root scale . To use this mode, add scale=pow2 . /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=pow2 Square Root \u00b6 Since 1.6. A power scale that emphasizes smaller values when mapping the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v 0.5 +b where m and b are automatically chosen based on the domain and range. To emphasize larger values see the power of 2 scale . To use this mode, add scale=sqrt . /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=sqrt","title":"Axis Scale"},{"location":"api/graph/axis-scale/#linear","text":"A linear scale uniformly maps the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v+b where m and b are automatically chosen based on the domain and range. This is the default scale for an axis and will get used if no explicit scale is set. Since 1.6, it can also be used explicitly: /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=linear","title":"Linear"},{"location":"api/graph/axis-scale/#logarithmic","text":"A logarithmic scale emphasizes smaller values when mapping the input values (domain) to the Y-axis location (range). This is often used if two lines with significantly different magnitudes are on the same axis. If v is datapoint in a time series, then y=m*log(v)+b where m and b are automatically chosen based on the domain and range. In many cases, using a separate Y-axis can be a better option that doesn't distort the line as much. To use this mode, add scale=log (prior to 1.6 use o=1 ). /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=log","title":"Logarithmic"},{"location":"api/graph/axis-scale/#power-of-2","text":"Since 1.6. A power scale that emphasizes larger values when mapping the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v 2 +b where m and b are automatically chosen based on the domain and range. To emphasize smaller values see the square root scale . To use this mode, add scale=pow2 . /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=pow2","title":"Power of 2"},{"location":"api/graph/axis-scale/#square-root","text":"Since 1.6. A power scale that emphasizes smaller values when mapping the input values (domain) to the Y-axis location (range). If v is datapoint in a time series, then y=m*v 0.5 +b where m and b are automatically chosen based on the domain and range. To emphasize larger values see the power of 2 scale . To use this mode, add scale=sqrt . /api/v1/graph? e=2012-01-01T00:00 &q= minuteOfHour, :time , 1e3, :add , minuteOfHour, :time & scale=sqrt","title":"Square Root"},{"location":"api/graph/basics/","text":"This section gives some examples to get started quickly creating simple graphs. Single Line Adding a Title Multiple Lines Group By Simple Math Binary Operations Single Line \u00b6 The only required parameter is q which specifies the query expression for a line. The other two common parameters are for setting the start time, s , and the end time, e , for the data being shown. Usually the start time will be set relative to the end time, such as e-3h , which indicates 3 hours before the end time. See time parameters for more details on time ranges. Putting it all together: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d Adding a Title \u00b6 The graph title can be set using the title parameter. Similarly, a Y-axis label can be set using the ylabel parameter. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d & title =Starts+Per+Second &ylabel=sps Multiple Lines \u00b6 Multiple expressions can be placed on a chart by concatenating the expressions, e.g., showing a query expression along with a constant value: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 500e3 &s=e-2d Group By \u00b6 Multiple lines can also be a result of a single expression via group by . /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-2d Simple Math \u00b6 A number of operators are provided to manipulate a line. See the math section of the stack language tutorial for a complete list. Example that negates the value of a line: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , :neg &s=e-2d Example that negates and then applies absolute value to get the original value back (since all values were positive in the input): /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , :neg , :abs &s=e-2d Binary Operations \u00b6 Lines can be combined using binary math operators such as add or multiply . Example using divide : /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 1000, :div &s=e-2d If used with a group by, then either: Both sides have the same group by clause. In this case an inner join will be preformed and the binary operation will be applied to the corresponding entries from both sides. One side is not a grouped expression, and the binary operation will be applied for each instance in the grouped result set. Both Sides Grouped \u00b6 Dividing by self with both sides grouped: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , :dup , :div &s=e-2d One Side Grouped \u00b6 Dividing a grouped expression by a constant: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , 1000, :div &s=e-2d Equivalent to the previous expression, but the right-hand side is grouped and it uses multiply instead of divide: /api/v1/graph? e=2012-01-01T00:00 &q= 0.001,name,sps, :eq , (,nf.cluster,), :by , :mul &s=e-2d","title":"Basics"},{"location":"api/graph/basics/#single-line","text":"The only required parameter is q which specifies the query expression for a line. The other two common parameters are for setting the start time, s , and the end time, e , for the data being shown. Usually the start time will be set relative to the end time, such as e-3h , which indicates 3 hours before the end time. See time parameters for more details on time ranges. Putting it all together: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d","title":"Single Line"},{"location":"api/graph/basics/#adding-a-title","text":"The graph title can be set using the title parameter. Similarly, a Y-axis label can be set using the ylabel parameter. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d & title =Starts+Per+Second &ylabel=sps","title":"Adding a Title"},{"location":"api/graph/basics/#multiple-lines","text":"Multiple expressions can be placed on a chart by concatenating the expressions, e.g., showing a query expression along with a constant value: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 500e3 &s=e-2d","title":"Multiple Lines"},{"location":"api/graph/basics/#group-by","text":"Multiple lines can also be a result of a single expression via group by . /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-2d","title":"Group By"},{"location":"api/graph/basics/#simple-math","text":"A number of operators are provided to manipulate a line. See the math section of the stack language tutorial for a complete list. Example that negates the value of a line: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , :neg &s=e-2d Example that negates and then applies absolute value to get the original value back (since all values were positive in the input): /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , :neg , :abs &s=e-2d","title":"Simple Math"},{"location":"api/graph/basics/#binary-operations","text":"Lines can be combined using binary math operators such as add or multiply . Example using divide : /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 1000, :div &s=e-2d If used with a group by, then either: Both sides have the same group by clause. In this case an inner join will be preformed and the binary operation will be applied to the corresponding entries from both sides. One side is not a grouped expression, and the binary operation will be applied for each instance in the grouped result set.","title":"Binary Operations"},{"location":"api/graph/basics/#both-sides-grouped","text":"Dividing by self with both sides grouped: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , :dup , :div &s=e-2d","title":"Both Sides Grouped"},{"location":"api/graph/basics/#one-side-grouped","text":"Dividing a grouped expression by a constant: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , 1000, :div &s=e-2d Equivalent to the previous expression, but the right-hand side is grouped and it uses multiply instead of divide: /api/v1/graph? e=2012-01-01T00:00 &q= 0.001,name,sps, :eq , (,nf.cluster,), :by , :mul &s=e-2d","title":"One Side Grouped"},{"location":"api/graph/color-palettes/","text":"The following color palettes are supported: armytage epic blues greens oranges purples reds custom There is also a hashed selection mode that can be used so that a line with a given label will always get the same color. Armytage \u00b6 This is the default color palette, it comes from the paper A Colour Alphabet and the Limits of Colour Coding by Paul Green-Armytage. Two colors, Xanthin and Yellow, are excluded because users found them hard to distinguish from a white background when used for a single pixel line. So overall there are 24 distinct colors with this palette. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=armytage &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 Epic \u00b6 This is a legacy palette that alternates between shades of red, green, and blue. It is supported for backwards compatibility, but not recommended. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=epic &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 Blues \u00b6 Shades of blue. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=blues &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Greens \u00b6 Shades of green. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=greens &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Oranges \u00b6 Shades of orange. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=oranges &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Purples \u00b6 Shades of purple. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=purples &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Reds \u00b6 Shades of red. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=reds &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Custom \u00b6 A custom color palette can be provided for a graph by using a prefix of colors: followed by a comma separated list of hex color values. This is mainly used to customize the colors for the result of a group by where you cannot set the color for each line using :color . /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &palette=colors:1a9850,91cf60,d9ef8b,fee08b,fc8d59,d73027 &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1 Hashed Selection \u00b6 Any of the palettes above can be prefixed with hash: to select the color using a hashing function on the label rather than picking the next color from the list. The primary advantage is that the selected color will always be the same for a given label using a particular palette. However, some nice properties of the default mode are lost: Colors can be duplicated even with a small number of lines. Hash collisions will result in the same color. The palettes are ordered to try and make the stacked appearance and legends easier to read. For the armytage palette it is ordered so adjacent colors are easy to distinguish. For the palettes that are shades of a color they are ordered from dark to light shades to create a gradient effect. Hashing causes an arbitrary ordering of the colors from the palette. The table below illustrates the difference by adding some additional lines to a chart for the second row: armytage hash:armytage Example: /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &palette=hash:armytage &q= name,sps, :eq , (,nf.cluster,), :by , :stack &tz=UTC","title":"Color Palettes"},{"location":"api/graph/color-palettes/#armytage","text":"This is the default color palette, it comes from the paper A Colour Alphabet and the Limits of Colour Coding by Paul Green-Armytage. Two colors, Xanthin and Yellow, are excluded because users found them hard to distinguish from a white background when used for a single pixel line. So overall there are 24 distinct colors with this palette. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=armytage &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1","title":"Armytage"},{"location":"api/graph/color-palettes/#epic","text":"This is a legacy palette that alternates between shades of red, green, and blue. It is supported for backwards compatibility, but not recommended. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=epic &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1","title":"Epic"},{"location":"api/graph/color-palettes/#blues","text":"Shades of blue. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=blues &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Blues"},{"location":"api/graph/color-palettes/#greens","text":"Shades of green. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=greens &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Greens"},{"location":"api/graph/color-palettes/#oranges","text":"Shades of orange. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=oranges &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Oranges"},{"location":"api/graph/color-palettes/#purples","text":"Shades of purple. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=purples &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Purples"},{"location":"api/graph/color-palettes/#reds","text":"Shades of red. /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 & palette=reds &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Reds"},{"location":"api/graph/color-palettes/#custom","text":"A custom color palette can be provided for a graph by using a prefix of colors: followed by a comma separated list of hex color values. This is mainly used to customize the colors for the result of a group by where you cannot set the color for each line using :color . /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &palette=colors:1a9850,91cf60,d9ef8b,fee08b,fc8d59,d73027 &stack=1 &tz=UTC &q= 1,1,1,1,1,1,1","title":"Custom"},{"location":"api/graph/color-palettes/#hashed-selection","text":"Any of the palettes above can be prefixed with hash: to select the color using a hashing function on the label rather than picking the next color from the list. The primary advantage is that the selected color will always be the same for a given label using a particular palette. However, some nice properties of the default mode are lost: Colors can be duplicated even with a small number of lines. Hash collisions will result in the same color. The palettes are ordered to try and make the stacked appearance and legends easier to read. For the armytage palette it is ordered so adjacent colors are easy to distinguish. For the palettes that are shades of a color they are ordered from dark to light shades to create a gradient effect. Hashing causes an arbitrary ordering of the colors from the palette. The table below illustrates the difference by adding some additional lines to a chart for the second row: armytage hash:armytage Example: /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &palette=hash:armytage &q= name,sps, :eq , (,nf.cluster,), :by , :stack &tz=UTC","title":"Hashed Selection"},{"location":"api/graph/examples/","text":"Browse the sidebar to get a good overview of graph options. It is recommended to at least go through the basics section. There is also a quick visual index below: Line Area Stack Stacked Percent VSpan Transparency Line Width Palettes Bounds Scales Multi Y Time Zones","title":"Examples"},{"location":"api/graph/graph/","text":"The Graph API is the primary means to retrieve data from an Atlas store. The default response is a PNG image plotting data matching the Atlas Stack Language expression along with optional parameters to control time ranges, size, style, labels, etc. For a quick overview by example see the examples page. If graphs look familiar, that's because the design and language were inspired by RRDtool . RRD style graphs offer concise and highly customizable views of time series data. While a number of observability tools offer dynamic charts, a major benefit of these PNG graphs is the ability to snapshot data in time, particularly when that data may expire from a high throughput data store; PNGs are forever. Additionally, the majority of email and on-call systems support images out of the box without having to worry about porting a dynamic graphing library to various browsers and clients. The API only supports HTTP query strings at this time. This makes it easy to construct queries with tooling and share the URIs with other users. No JSON request payloads needed. Additional Output formats, including JSON, can be found in Outputs . URI \u00b6 /api/v1/graph?q=<expr>[&OPTIONS] HTTP Method \u00b6 GET - Only the GET method is allowed at this time. Query Parameters \u00b6 Data \u00b6 The only required query param is q which is the query expression used by the user to select and manipulate data. The simplest API query you can make is /api/v1/graph?q=42 . This will produce a graph from Atlas with a straight line having a value of 42 for 3 hours * with a legend including statistics for the query period. All query params related to fetching data: Name Description Default Type q Query expression must be specified by user expr step Step size for data auto duration Warning In most cases users should not set step directly. The step parameter is deprecated. Time \u00b6 There are three parameters to control the time range used for a graph: Name Description Default Type s Start time e-3h * Time e End time now * Time tz Time zone US/Pacific * Time zone ID For more information on the behavior see the time parameters page. Image Flags \u00b6 Name Description Default Type title Set the graph title no title String no_legend Suppresses the legend 0 boolean no_legend_stats Suppresses summary stats for the legend 0 boolean axis_per_line Put each line on a separate Y-axis 0 boolean only_graph Only show the graph canvas 0 boolean vision Simulate different vision types normal vision type Image Size \u00b6 There are four parameters to control the image size and layout used for a graph: Name Description Default Type layout Mode for controlling exact or relative sizing canvas layout mode w Width of the canvas or image 700 * int h Height of the canvas or image 300 * int zoom Transform the size by a zoom factor 1.0 float For more information on the behavior see the graph layout page. Y-Axis \u00b6 Name Description Default Type stack Set the default line style to stack 0 boolean l Lower bound for the axis auto-style axis bound u Upper bound for the axis auto-style axis bound ylabel Label for the axis no label String palette Color palette to use armytage palette o Use a logarithmic scale (deprecated in 1.6) 0 boolean scale Set the axis scale to use (since 1.6) linear scale tick_labels Set the mode to use for tick labels decimal tick label mode sort Set the mode to use for sorting the legend expr order sort mode order Set the order ascending or descending for the sort asc order Output Format \u00b6 Name Description Default Type format Output format to use png output format callback Method name to use for JSONP callback none String Defaults \u00b6 If marked with an * the default shown can be changed by the administrator for the Atlas server. As a result the default in the table may not match the default you see. The defaults listed do match those used for the primary Atlas backends in use at Netflix. For users running their own server, the config settings and corresponding query params are: Key Query Param atlas.webapi.graph.start-time s atlas.webapi.graph.end-time e atlas.webapi.graph.timezone tz atlas.webapi.graph.width w atlas.webapi.graph.height h atlas.webapi.graph.palette palette Boolean Flags \u00b6 Flags with a true or false value are specified using 1 for true and 0 for false.","title":"Introduction"},{"location":"api/graph/graph/#uri","text":"/api/v1/graph?q=<expr>[&OPTIONS]","title":"URI"},{"location":"api/graph/graph/#http-method","text":"GET - Only the GET method is allowed at this time.","title":"HTTP Method"},{"location":"api/graph/graph/#query-parameters","text":"","title":"Query Parameters"},{"location":"api/graph/graph/#data","text":"The only required query param is q which is the query expression used by the user to select and manipulate data. The simplest API query you can make is /api/v1/graph?q=42 . This will produce a graph from Atlas with a straight line having a value of 42 for 3 hours * with a legend including statistics for the query period. All query params related to fetching data: Name Description Default Type q Query expression must be specified by user expr step Step size for data auto duration Warning In most cases users should not set step directly. The step parameter is deprecated.","title":"Data"},{"location":"api/graph/graph/#time","text":"There are three parameters to control the time range used for a graph: Name Description Default Type s Start time e-3h * Time e End time now * Time tz Time zone US/Pacific * Time zone ID For more information on the behavior see the time parameters page.","title":"Time"},{"location":"api/graph/graph/#image-flags","text":"Name Description Default Type title Set the graph title no title String no_legend Suppresses the legend 0 boolean no_legend_stats Suppresses summary stats for the legend 0 boolean axis_per_line Put each line on a separate Y-axis 0 boolean only_graph Only show the graph canvas 0 boolean vision Simulate different vision types normal vision type","title":"Image Flags"},{"location":"api/graph/graph/#image-size","text":"There are four parameters to control the image size and layout used for a graph: Name Description Default Type layout Mode for controlling exact or relative sizing canvas layout mode w Width of the canvas or image 700 * int h Height of the canvas or image 300 * int zoom Transform the size by a zoom factor 1.0 float For more information on the behavior see the graph layout page.","title":"Image Size"},{"location":"api/graph/graph/#y-axis","text":"Name Description Default Type stack Set the default line style to stack 0 boolean l Lower bound for the axis auto-style axis bound u Upper bound for the axis auto-style axis bound ylabel Label for the axis no label String palette Color palette to use armytage palette o Use a logarithmic scale (deprecated in 1.6) 0 boolean scale Set the axis scale to use (since 1.6) linear scale tick_labels Set the mode to use for tick labels decimal tick label mode sort Set the mode to use for sorting the legend expr order sort mode order Set the order ascending or descending for the sort asc order","title":"Y-Axis"},{"location":"api/graph/graph/#output-format","text":"Name Description Default Type format Output format to use png output format callback Method name to use for JSONP callback none String","title":"Output Format"},{"location":"api/graph/graph/#defaults","text":"If marked with an * the default shown can be changed by the administrator for the Atlas server. As a result the default in the table may not match the default you see. The defaults listed do match those used for the primary Atlas backends in use at Netflix. For users running their own server, the config settings and corresponding query params are: Key Query Param atlas.webapi.graph.start-time s atlas.webapi.graph.end-time e atlas.webapi.graph.timezone tz atlas.webapi.graph.width w atlas.webapi.graph.height h atlas.webapi.graph.palette palette","title":"Defaults"},{"location":"api/graph/graph/#boolean-flags","text":"Flags with a true or false value are specified using 1 for true and 0 for false.","title":"Boolean Flags"},{"location":"api/graph/layout/","text":"The diagram below shows the parts of an Atlas graph and will be used when describing the behavior for various options. The layout for graph images is trying to accomplish two main goals: Usable Canvas Size \u00b6 Keep the canvas usable regardless of the number of lines, axes, etc that are competing for space. For example, the canvas area should not become too small due to the number of lines on the chart. Good Layout Poor Layout Canvas Alignment \u00b6 Make it easy to align the canvas portion of several graphs on an html page. This is important because it makes it easier to find visual correlations between multiple graphs on a dashboard. In particular if arranged in a grid with the image in the top left of each cell, then the canvas should line up vertically for columns: And horizontally for rows: In the graph layout diagram at the top, this is why variable components such as multi axes, legend entries, and warnings are positioned on either the right side or the bottom of the canvas. Modes \u00b6 There are four supported layout modes that can be used with the layout query parameter : canvas : the width or height are for the canvas component within the chart. The actual image size will be calculated based on the number of entries in the legend, number of axes, etc. This is the default behavior. image : the width or height are for the final image not including the zoom parameter. To try and adhere to layout goals when using this mode everything below the X-axes will automatically be suppressed. Vertical alignment will still hold as long as all graphs use the same number of Y-axes. Horizontal alignment will still hold as long as all graphs use the same number of X-axes. iw : use exact image sizing for the width and canvas sizing for the height. ih : use exact image sizing for the height and canvas sizing for the width. Examples \u00b6 Canvas \u00b6 /api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =canvas &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400 Image \u00b6 /api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =image &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400 Image Width \u00b6 /api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =iw &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400 Image Height \u00b6 /api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =ih &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400","title":"Layout"},{"location":"api/graph/layout/#usable-canvas-size","text":"Keep the canvas usable regardless of the number of lines, axes, etc that are competing for space. For example, the canvas area should not become too small due to the number of lines on the chart. Good Layout Poor Layout","title":"Usable Canvas Size"},{"location":"api/graph/layout/#canvas-alignment","text":"Make it easy to align the canvas portion of several graphs on an html page. This is important because it makes it easier to find visual correlations between multiple graphs on a dashboard. In particular if arranged in a grid with the image in the top left of each cell, then the canvas should line up vertically for columns: And horizontally for rows: In the graph layout diagram at the top, this is why variable components such as multi axes, legend entries, and warnings are positioned on either the right side or the bottom of the canvas.","title":"Canvas Alignment"},{"location":"api/graph/layout/#modes","text":"There are four supported layout modes that can be used with the layout query parameter : canvas : the width or height are for the canvas component within the chart. The actual image size will be calculated based on the number of entries in the legend, number of axes, etc. This is the default behavior. image : the width or height are for the final image not including the zoom parameter. To try and adhere to layout goals when using this mode everything below the X-axes will automatically be suppressed. Vertical alignment will still hold as long as all graphs use the same number of Y-axes. Horizontal alignment will still hold as long as all graphs use the same number of X-axes. iw : use exact image sizing for the width and canvas sizing for the height. ih : use exact image sizing for the height and canvas sizing for the width.","title":"Modes"},{"location":"api/graph/layout/#examples","text":"","title":"Examples"},{"location":"api/graph/layout/#canvas","text":"/api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =canvas &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400","title":"Canvas"},{"location":"api/graph/layout/#image","text":"/api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =image &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400","title":"Image"},{"location":"api/graph/layout/#image-width","text":"/api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =iw &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400","title":"Image Width"},{"location":"api/graph/layout/#image-height","text":"/api/v1/graph? e=2012-01-01T09:00 &h=175 & layout =ih &q= name,sps, :eq , :sum , (,nf.cluster,), :by &s=e-1d &tz=UTC &w=400","title":"Image Height"},{"location":"api/graph/legends/","text":"Options for adjusting legend: Automatic Explicit Variables Disable Disable Stats Sorting Automatic \u00b6 If no explicit legend is specified, then the system will generate an automatic legend that summarizes the expression. There is no particular guarantee about what it will contain and in some cases it is difficult to generate a usable legend automatically. Example: /api/v1/graph? e=2012-01-01T00:00 &q= hourOfDay, :time , 100, :mul , minuteOfHour, :time , :add &s=e-1w Explicit \u00b6 The legend for a line can be explicitly set using the :legend operator. /api/v1/graph? e=2012-01-01T00:00 &q= hourOfDay, :time , 100, :mul , minuteOfHour, :time , :add , time+value, :legend &s=e-1w Variables \u00b6 Tag keys can be used as variables to plug values into the legend. This is useful when working with group by operations to customize the legend for each output. The variable can be expressed as a $ followed by the tag key if it is the only part of the legend: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend &s=e-1w Or as $( the tag key and a closing ) if combined with other text: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , cluster+$(nf.cluster)+sps, :legend &s=e-1w Disable \u00b6 Legends can be disabled using the no_legend graph parameter. /api/v1/graph? e=2012-01-01T00:00 &no_ legend =1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w Disable Stats \u00b6 You can also save veritical space and keep the legend by disabling the summary stats shown in the legend using the no_legend_stats graph parameter. /api/v1/graph? e=2012-01-01T00:00 & no_legend_stats =1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w Sorting \u00b6 By default the legend for an axis will be ordered based on the order of the expressions on the stack. If an expression results in multple lines, i.e. a group by , then they will be sorted by the label. /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h Overall \u00b6 To sort all lines on a given axis using a different mode use the sort URL parameter. /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h & sort =max To change it to descending order use the order parameter, e.g.: /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h &sort=max & order =desc Group By Expression \u00b6 If more control is needed, then sorting can be applied to a particular group by expression. This can be useful for things like alerting visualizations where some common lines like the threshold and trigger indicator should be pinned to the top, but it is desirable to sort other results based on a stat like max . For example: /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by , :dup , :max , 120e3, :gt , 30, :alpha , :vspan , trigger, :legend , :swap , max, :sort , desc, :order , $nf.cluster, :legend &s=e-12h Sorting Modes \u00b6 legend : alphabetically based on the label used in the legend. This is the default. min : using the minimum value shown the legend stats. max : using the maximum value shown the legend stats. avg : using the average value shown the legend stats. count : using the count value shown the legend stats. total : using the total value shown the legend stats. last : using the last value shown the legend stats. Sorting Order \u00b6 asc : use ascending order. This is the default. desc : used descending order.","title":"Legends"},{"location":"api/graph/legends/#automatic","text":"If no explicit legend is specified, then the system will generate an automatic legend that summarizes the expression. There is no particular guarantee about what it will contain and in some cases it is difficult to generate a usable legend automatically. Example: /api/v1/graph? e=2012-01-01T00:00 &q= hourOfDay, :time , 100, :mul , minuteOfHour, :time , :add &s=e-1w","title":"Automatic"},{"location":"api/graph/legends/#explicit","text":"The legend for a line can be explicitly set using the :legend operator. /api/v1/graph? e=2012-01-01T00:00 &q= hourOfDay, :time , 100, :mul , minuteOfHour, :time , :add , time+value, :legend &s=e-1w","title":"Explicit"},{"location":"api/graph/legends/#variables","text":"Tag keys can be used as variables to plug values into the legend. This is useful when working with group by operations to customize the legend for each output. The variable can be expressed as a $ followed by the tag key if it is the only part of the legend: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend &s=e-1w Or as $( the tag key and a closing ) if combined with other text: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , cluster+$(nf.cluster)+sps, :legend &s=e-1w","title":"Variables"},{"location":"api/graph/legends/#disable","text":"Legends can be disabled using the no_legend graph parameter. /api/v1/graph? e=2012-01-01T00:00 &no_ legend =1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w","title":"Disable"},{"location":"api/graph/legends/#disable-stats","text":"You can also save veritical space and keep the legend by disabling the summary stats shown in the legend using the no_legend_stats graph parameter. /api/v1/graph? e=2012-01-01T00:00 & no_legend_stats =1 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w","title":"Disable Stats"},{"location":"api/graph/legends/#sorting","text":"By default the legend for an axis will be ordered based on the order of the expressions on the stack. If an expression results in multple lines, i.e. a group by , then they will be sorted by the label. /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h","title":"Sorting"},{"location":"api/graph/legends/#overall","text":"To sort all lines on a given axis using a different mode use the sort URL parameter. /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h & sort =max To change it to descending order use the order parameter, e.g.: /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by &s=e-12h &sort=max & order =desc","title":"Overall"},{"location":"api/graph/legends/#group-by-expression","text":"If more control is needed, then sorting can be applied to a particular group by expression. This can be useful for things like alerting visualizations where some common lines like the threshold and trigger indicator should be pinned to the top, but it is desirable to sort other results based on a stat like max . For example: /api/v1/graph? e=2012-01-01T00:00 &q= 120e3,threshold, :legend , name,sps, :eq , (,nf.cluster,), :by , :dup , :max , 120e3, :gt , 30, :alpha , :vspan , trigger, :legend , :swap , max, :sort , desc, :order , $nf.cluster, :legend &s=e-12h","title":"Group By Expression"},{"location":"api/graph/legends/#sorting-modes","text":"legend : alphabetically based on the label used in the legend. This is the default. min : using the minimum value shown the legend stats. max : using the maximum value shown the legend stats. avg : using the average value shown the legend stats. count : using the count value shown the legend stats. total : using the total value shown the legend stats. last : using the last value shown the legend stats.","title":"Sorting Modes"},{"location":"api/graph/legends/#sorting-order","text":"asc : use ascending order. This is the default. desc : used descending order.","title":"Sorting Order"},{"location":"api/graph/line-attributes/","text":"In addition to the line style and legend the following attributes can be adjusted: Color Transparency Line Width Color \u00b6 By default the color will come from the palette that is in use. However the color for a line can also be set explicitly using the :color operator: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , f00, color /\">: color &s=e-1w Note, that for a group by all results will get the same attributes, so in this case all would end up being the same color: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , f00, color /\">: color &s=e-1w Transparency \u00b6 The transparency of a line can be set using the :alpha operator or by explicitly setting the alpha channel as part of the color . /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , :area , 40, alpha /\">: alpha &s=e-2d Setting the alpha explicitly as part of the color: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , :area , 40ff0000, color /\">: color &s=e-2d Line Width \u00b6 Adjust the stroke width used for a line: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , 3, lw /\">: lw &s=e-1w","title":"Line Attributes"},{"location":"api/graph/line-attributes/#color","text":"By default the color will come from the palette that is in use. However the color for a line can also be set explicitly using the :color operator: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , f00, color /\">: color &s=e-1w Note, that for a group by all results will get the same attributes, so in this case all would end up being the same color: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , f00, color /\">: color &s=e-1w","title":"Color"},{"location":"api/graph/line-attributes/#transparency","text":"The transparency of a line can be set using the :alpha operator or by explicitly setting the alpha channel as part of the color . /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , :area , 40, alpha /\">: alpha &s=e-2d Setting the alpha explicitly as part of the color: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , :area , 40ff0000, color /\">: color &s=e-2d","title":"Transparency"},{"location":"api/graph/line-attributes/#line-width","text":"Adjust the stroke width used for a line: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 6h, :offset , 3, lw /\">: lw &s=e-1w","title":"Line Width"},{"location":"api/graph/line-styles/","text":"There are four line styles available: Line Area Stack Vertical Span Multiple styles can be used in the same chart or combined with other operations. Stacked Percentage Combinations Layering Line \u00b6 The default style is line. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :line &s=e-1w Area \u00b6 Area will fill the space between the line and 0 on the Y-axis. The alpha setting is just used to help visualize the overlap. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :area , 40, :alpha &s=e-1w Similarly for negative values: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :neg , :area , 40, :alpha &s=e-1w Stack \u00b6 Stack is similar to area , but will stack the filled areas on top of each other. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :stack &s=e-1w Similarly for negative values: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :neg , :stack &s=e-1w Stacked Percentage \u00b6 The stack style can be combined with the :pct operator to get a stacked percentage chart for a group by: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :pct , :stack &s=e-1w Vertical Span \u00b6 The vertical span style converts non-zero to spans. This is often used to highlight some portion of another line. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , 50e3, :gt , :vspan &s=e-1w Combinations \u00b6 Line styles can be combined, e.g., to highlight the portion of a line that is above a threshold: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 5003, :gt , :vspan , 40, :alpha , 50e3 &s=e-1w Layering \u00b6 The z-order is based on the order of the expression on the stack. /api/v1/graph? e=2015-03-10T13:13 &no_legend=1 &q= t,name,sps, :eq , :sum , :set , t, :get , :stack , t, :get , 1.1, :mul , 6h, :offset , t, :get , 4, :div , :stack &s=e-2d","title":"Line Styles"},{"location":"api/graph/line-styles/#line","text":"The default style is line. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :line &s=e-1w","title":"Line"},{"location":"api/graph/line-styles/#area","text":"Area will fill the space between the line and 0 on the Y-axis. The alpha setting is just used to help visualize the overlap. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :area , 40, :alpha &s=e-1w Similarly for negative values: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :neg , :area , 40, :alpha &s=e-1w","title":"Area"},{"location":"api/graph/line-styles/#stack","text":"Stack is similar to area , but will stack the filled areas on top of each other. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :stack &s=e-1w Similarly for negative values: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :neg , :stack &s=e-1w","title":"Stack"},{"location":"api/graph/line-styles/#stacked-percentage","text":"The stack style can be combined with the :pct operator to get a stacked percentage chart for a group by: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , (,nf.cluster,), :by , :pct , :stack &s=e-1w","title":"Stacked Percentage"},{"location":"api/graph/line-styles/#vertical-span","text":"The vertical span style converts non-zero to spans. This is often used to highlight some portion of another line. /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , 50e3, :gt , :vspan &s=e-1w","title":"Vertical Span"},{"location":"api/graph/line-styles/#combinations","text":"Line styles can be combined, e.g., to highlight the portion of a line that is above a threshold: /api/v1/graph? e=2012-01-01T00:00 &no_legend=1 &q= name,sps, :eq , :dup , 5003, :gt , :vspan , 40, :alpha , 50e3 &s=e-1w","title":"Combinations"},{"location":"api/graph/line-styles/#layering","text":"The z-order is based on the order of the expression on the stack. /api/v1/graph? e=2015-03-10T13:13 &no_legend=1 &q= t,name,sps, :eq , :sum , :set , t, :get , :stack , t, :get , 1.1, :mul , 6h, :offset , t, :get , 4, :div , :stack &s=e-2d","title":"Layering"},{"location":"api/graph/multi-y/","text":"Examples for using multiple Y-axes: Explicit Explicit Bounds Axis Per Line Palettes Explicit \u00b6 By default all lines will go on axis 0 , the one on the left side. A different axis can be specified using the :axis operation. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 42,1, :axis Explicit Bounds \u00b6 By default all axes will pick up axis settings with no qualifier: /api/v1/graph? e=2012-01-01T00:00 &l=0 &q= name,sps, :eq , 42,1, :axis Bounds and other axis settings can be set per axis, e.g., this graph moves the constant line for 42 to a separate axis and sets the lower bound to 0 via the &l.1=0 parameter. This would work as well for &u.1=100e3 . Append the index after the l. or u. : /api/v1/graph? e=2012-01-01T00:00 &l.1=0 &q= name,sps, :eq , 42,1, :axis Axis Per Line \u00b6 There is a convenience operation to plot each line on a separate axis. /api/v1/graph? e=2012-01-01T00:00 & axis_per_line=1 &q= name,sps, :eq , nf.cluster,nccp-p, :re , :and , (,nf.cluster,), :by If there are too many lines and it would be over the max Y-axis limit, then a warning will be shown: /api/v1/graph? e=2012-01-01T00:00 & axis_per_line=1 &q= name,sps, :eq , (,nf.cluster,), :by Palettes \u00b6 The color of the first line on an axis will get used as the color of the axis. The intention is to make it easy to understand which axis a line is associated with and in an image dynamic clues like hover cannot be used. Generally it is recommended to only have one line per axis when using multi-Y. Example: /api/v1/graph? e=2012-01-01T00:00 &l=01 &q= name,sps, :eq , (,nf.cluster,), :by , minuteOfHour, :time , 1, :axis Though we recommend not using more than one line per axis with multi-Y, a color palette can be specified for a specific axis. This can be used to select shades of a color for an axis so it is still easy to visually associate which axis a line belongs to: /api/v1/graph? e=2012-01-01T00:00 &l=01 &palette.0=reds &palette.1=blues &q= name,sps, :eq , (,nf.cluster,), :by , :stack , minuteOfHour, :time , 1, :axis","title":"Multi Y Axis"},{"location":"api/graph/multi-y/#explicit","text":"By default all lines will go on axis 0 , the one on the left side. A different axis can be specified using the :axis operation. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , 42,1, :axis","title":"Explicit"},{"location":"api/graph/multi-y/#explicit-bounds","text":"By default all axes will pick up axis settings with no qualifier: /api/v1/graph? e=2012-01-01T00:00 &l=0 &q= name,sps, :eq , 42,1, :axis Bounds and other axis settings can be set per axis, e.g., this graph moves the constant line for 42 to a separate axis and sets the lower bound to 0 via the &l.1=0 parameter. This would work as well for &u.1=100e3 . Append the index after the l. or u. : /api/v1/graph? e=2012-01-01T00:00 &l.1=0 &q= name,sps, :eq , 42,1, :axis","title":"Explicit Bounds"},{"location":"api/graph/multi-y/#axis-per-line","text":"There is a convenience operation to plot each line on a separate axis. /api/v1/graph? e=2012-01-01T00:00 & axis_per_line=1 &q= name,sps, :eq , nf.cluster,nccp-p, :re , :and , (,nf.cluster,), :by If there are too many lines and it would be over the max Y-axis limit, then a warning will be shown: /api/v1/graph? e=2012-01-01T00:00 & axis_per_line=1 &q= name,sps, :eq , (,nf.cluster,), :by","title":"Axis Per Line"},{"location":"api/graph/multi-y/#palettes","text":"The color of the first line on an axis will get used as the color of the axis. The intention is to make it easy to understand which axis a line is associated with and in an image dynamic clues like hover cannot be used. Generally it is recommended to only have one line per axis when using multi-Y. Example: /api/v1/graph? e=2012-01-01T00:00 &l=01 &q= name,sps, :eq , (,nf.cluster,), :by , minuteOfHour, :time , 1, :axis Though we recommend not using more than one line per axis with multi-Y, a color palette can be specified for a specific axis. This can be used to select shades of a color for an axis so it is still easy to visually associate which axis a line belongs to: /api/v1/graph? e=2012-01-01T00:00 &l=01 &palette.0=reds &palette.1=blues &q= name,sps, :eq , (,nf.cluster,), :by , :stack , minuteOfHour, :time , 1, :axis","title":"Palettes"},{"location":"api/graph/outputs/","text":"The following output formats are supported by default for graphing : png csv txt json std.json stats.json png \u00b6 This is the default and creates a PNG image for the graph. The mime type is image/png . /api/v1/graph? e=2012-01-01T09:00 & format=png &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-3m &tz=UTC csv \u00b6 Comma separated value output. The mime type is text/csv . /api/v1/graph? e=2012-01-01T09:00 & format=csv &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC \"timestamp\",\"hourOfDay\",\"minuteOfHour\",\"NaN\" 2012-01-01T08:56:00Z,8.000000,56.000000,NaN 2012-01-01T08:57:00Z,8.000000,57.000000,NaN 2012-01-01T08:58:00Z,8.000000,58.000000,NaN 2012-01-01T08:59:00Z,8.000000,59.000000,NaN 2012-01-01T09:00:00Z,9.000000,0.000000,NaN txt \u00b6 Same as csv except that the separator is a tab character instead of a comma. The mime type will be text/plain so it is more likely to render directly in the browser rather than trigger a download. /api/v1/graph? e=2012-01-01T09:00 & format=txt &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC \"timestamp\" \"hourOfDay\" \"minuteOfHour\" \"NaN\" 2012-01-01T08:56:00Z 8.000000 56.000000 NaN 2012-01-01T08:57:00Z 8.000000 57.000000 NaN 2012-01-01T08:58:00Z 8.000000 58.000000 NaN 2012-01-01T08:59:00Z 8.000000 59.000000 NaN 2012-01-01T09:00:00Z 9.000000 0.000000 NaN json \u00b6 JSON output representing the data. Note that it is not standard json as numeric values like NaN will not get quoted. /api/v1/graph? e=2012-01-01T09:00 & format=json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"values\" : [ [ 8.0, 56.0, NaN ], [ 8.0, 57.0, NaN ], [ 8.0, 58.0, NaN ], [ 8.0, 59.0, NaN ], [ 9.0, 0.0, NaN ] ], \"notices\" : [ ] } std.json \u00b6 Same as json except that numeric values which are not recognized by standard json will be quoted. The mime type is application/json . /api/v1/graph? e=2012-01-01T09:00 & format=std.json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"values\" : [ [ 8.0, 56.0, \"NaN\" ], [ 8.0, 57.0, \"NaN\" ], [ 8.0, 58.0, \"NaN\" ], [ 8.0, 59.0, \"NaN\" ], [ 9.0, 0.0, \"NaN\" ] ], \"notices\" : [ ] } stats.json \u00b6 Provides the summary stats for each line, but not all of the data points. The mime type is application/json . /api/v1/graph? e=2012-01-01T09:00 & format=stats.json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"end\" : 1325408460000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"stats\" : [ { \"count\" : 5, \"avg\" : 8.2, \"total\" : 41.0, \"max\" : 9.0, \"min\" : 8.0, \"last\" : 9.0 }, { \"count\" : 5, \"avg\" : 46.0, \"total\" : 230.0, \"max\" : 59.0, \"min\" : 0.0, \"last\" : 0.0 }, { \"count\" : 0, \"avg\" : NaN, \"total\" : NaN, \"max\" : NaN, \"min\" : NaN, \"last\" : NaN } ], \"notices\" : [ ] }","title":"Output Formats"},{"location":"api/graph/outputs/#png","text":"This is the default and creates a PNG image for the graph. The mime type is image/png . /api/v1/graph? e=2012-01-01T09:00 & format=png &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-3m &tz=UTC","title":"png"},{"location":"api/graph/outputs/#csv","text":"Comma separated value output. The mime type is text/csv . /api/v1/graph? e=2012-01-01T09:00 & format=csv &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC \"timestamp\",\"hourOfDay\",\"minuteOfHour\",\"NaN\" 2012-01-01T08:56:00Z,8.000000,56.000000,NaN 2012-01-01T08:57:00Z,8.000000,57.000000,NaN 2012-01-01T08:58:00Z,8.000000,58.000000,NaN 2012-01-01T08:59:00Z,8.000000,59.000000,NaN 2012-01-01T09:00:00Z,9.000000,0.000000,NaN","title":"csv"},{"location":"api/graph/outputs/#txt","text":"Same as csv except that the separator is a tab character instead of a comma. The mime type will be text/plain so it is more likely to render directly in the browser rather than trigger a download. /api/v1/graph? e=2012-01-01T09:00 & format=txt &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC \"timestamp\" \"hourOfDay\" \"minuteOfHour\" \"NaN\" 2012-01-01T08:56:00Z 8.000000 56.000000 NaN 2012-01-01T08:57:00Z 8.000000 57.000000 NaN 2012-01-01T08:58:00Z 8.000000 58.000000 NaN 2012-01-01T08:59:00Z 8.000000 59.000000 NaN 2012-01-01T09:00:00Z 9.000000 0.000000 NaN","title":"txt"},{"location":"api/graph/outputs/#json","text":"JSON output representing the data. Note that it is not standard json as numeric values like NaN will not get quoted. /api/v1/graph? e=2012-01-01T09:00 & format=json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"values\" : [ [ 8.0, 56.0, NaN ], [ 8.0, 57.0, NaN ], [ 8.0, 58.0, NaN ], [ 8.0, 59.0, NaN ], [ 9.0, 0.0, NaN ] ], \"notices\" : [ ] }","title":"json"},{"location":"api/graph/outputs/#stdjson","text":"Same as json except that numeric values which are not recognized by standard json will be quoted. The mime type is application/json . /api/v1/graph? e=2012-01-01T09:00 & format=std.json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"values\" : [ [ 8.0, 56.0, \"NaN\" ], [ 8.0, 57.0, \"NaN\" ], [ 8.0, 58.0, \"NaN\" ], [ 8.0, 59.0, \"NaN\" ], [ 9.0, 0.0, \"NaN\" ] ], \"notices\" : [ ] }","title":"std.json"},{"location":"api/graph/outputs/#statsjson","text":"Provides the summary stats for each line, but not all of the data points. The mime type is application/json . /api/v1/graph? e=2012-01-01T09:00 & format=stats.json &q= hourOfDay, :time , minuteOfHour, :time , NaN &s=e-5m &tz=UTC { \"start\" : 1325408160000, \"end\" : 1325408460000, \"step\" : 60000, \"legend\" : [ \"hourOfDay\", \"minuteOfHour\", \"NaN\" ], \"metrics\" : [ { \"atlas.offset\" : \"0w\", \"name\" : \"hourOfDay\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"minuteOfHour\" }, { \"atlas.offset\" : \"0w\", \"name\" : \"NaN\" } ], \"stats\" : [ { \"count\" : 5, \"avg\" : 8.2, \"total\" : 41.0, \"max\" : 9.0, \"min\" : 8.0, \"last\" : 9.0 }, { \"count\" : 5, \"avg\" : 46.0, \"total\" : 230.0, \"max\" : 59.0, \"min\" : 0.0, \"last\" : 0.0 }, { \"count\" : 0, \"avg\" : NaN, \"total\" : NaN, \"max\" : NaN, \"min\" : NaN, \"last\" : NaN } ], \"notices\" : [ ] }","title":"stats.json"},{"location":"api/graph/tick/","text":"The following tick label modes are supported: decimal binary off Decimal \u00b6 This is the default mode. Y-axis tick labels will be formatted using the metric prefix to indicate the magnitude for values that are greater than one thousand or less than one. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =decimal Really large values will fallback to scientific notation, e.g.: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , 1e180, :mul &s=e-1w & tick_labels =decimal Binary \u00b6 For values such as memory sizes it is sometimes more convenient to view the label using a power of 1024 rather than a power of 1000. If the tick label mode is set to binary , then the IEC binary prefix will be used. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =binary Off \u00b6 For presentations or sharing it is sometimes useful to anonymize the chart. One way of doing that is to disable the Y-axis labels by setting the tick label mode to off . /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =off","title":"Tick Labels"},{"location":"api/graph/tick/#decimal","text":"This is the default mode. Y-axis tick labels will be formatted using the metric prefix to indicate the magnitude for values that are greater than one thousand or less than one. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =decimal Really large values will fallback to scientific notation, e.g.: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by , 1e180, :mul &s=e-1w & tick_labels =decimal","title":"Decimal"},{"location":"api/graph/tick/#binary","text":"For values such as memory sizes it is sometimes more convenient to view the label using a power of 1024 rather than a power of 1000. If the tick label mode is set to binary , then the IEC binary prefix will be used. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =binary","title":"Binary"},{"location":"api/graph/tick/#off","text":"For presentations or sharing it is sometimes useful to anonymize the chart. One way of doing that is to disable the Y-axis labels by setting the tick label mode to off . /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , (,nf.cluster,), :by &s=e-1w & tick_labels =off","title":"Off"},{"location":"api/graph/time-shift/","text":"A common use-case is to compare a given line with a shifted line to compare week-over-week or day-over-day. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , nf.cluster,nccp-silverlight, :eq , :and , :sum , :dup , 1w, :offset The $(atlas.offset) variable can be used to show the offset in a custom legend: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq , nf.cluster,nccp-silverlight, :eq , :and , :sum , :dup , 1w, :offset , :list , (,$nf.cluster+(offset=$atlas.offset), :legend , ), :each","title":"Time Shift"},{"location":"api/graph/time-zone/","text":"Examples for specifying the time zone: Single Zone Multi Zone Daylight Savings Time Single Zone \u00b6 Most graphs will only show a single time zone. By default the zone is US/Pacific . To set to another zone such as UTC use the tz query parameter: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq & tz=UTC Multi Zone \u00b6 The tz parameter can be specified multiple times in which case one X-axis will be shown per zone. Start and end times will be based on the first time zone listed. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d & tz =US/Eastern & tz =US/Pacific & tz =UTC Daylight Savings Time \u00b6 If using a time zone that changes for daylight savings time, then you will see duplicate or missing hours on the time axis labels during the transition period. For example, a duplicate hour: /api/v1/graph? e=2015-11-01T08:00 &q= name,sps, :eq &s=e-12h & tz =US/Pacific & tz =UTC A missing hour: /api/v1/graph? e=2015-03-08T08:00 &q= name,sps, :eq &s=e-12h & tz =US/Pacific & tz =UTC If looking at a longer time frame, then it can also throw off the alignment so ticks will not be on significant time boundaries, e.g.: /api/v1/graph? e=2015-11-05T08:00 &q= name,sps, :eq &s=e-1w & tz =US/Pacific & tz =UTC","title":"Time Zones"},{"location":"api/graph/time-zone/#single-zone","text":"Most graphs will only show a single time zone. By default the zone is US/Pacific . To set to another zone such as UTC use the tz query parameter: /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq & tz=UTC","title":"Single Zone"},{"location":"api/graph/time-zone/#multi-zone","text":"The tz parameter can be specified multiple times in which case one X-axis will be shown per zone. Start and end times will be based on the first time zone listed. /api/v1/graph? e=2012-01-01T00:00 &q= name,sps, :eq &s=e-2d & tz =US/Eastern & tz =US/Pacific & tz =UTC","title":"Multi Zone"},{"location":"api/graph/time-zone/#daylight-savings-time","text":"If using a time zone that changes for daylight savings time, then you will see duplicate or missing hours on the time axis labels during the transition period. For example, a duplicate hour: /api/v1/graph? e=2015-11-01T08:00 &q= name,sps, :eq &s=e-12h & tz =US/Pacific & tz =UTC A missing hour: /api/v1/graph? e=2015-03-08T08:00 &q= name,sps, :eq &s=e-12h & tz =US/Pacific & tz =UTC If looking at a longer time frame, then it can also throw off the alignment so ticks will not be on significant time boundaries, e.g.: /api/v1/graph? e=2015-11-05T08:00 &q= name,sps, :eq &s=e-1w & tz =US/Pacific & tz =UTC","title":"Daylight Savings Time"},{"location":"api/graph/vision/","text":"The vision parameter can be used to simulate different types of color blindness . Permitted values are: normal protanopia protanomaly deuteranopia deuteranomaly tritanopia tritanomaly achromatopsia achromatomaly Normal \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =normal Protanopia \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =protanopia Protanomaly \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =protanomaly Deuteranopia \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =deuteranopia Deuteranomaly \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =deuteranomaly Tritanopia \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =tritanopia Tritanomaly \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =tritanomaly Achromatopsia \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =achromatopsia Achromatomaly \u00b6 /api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =achromatomaly","title":"Color Blindness"},{"location":"api/graph/vision/#normal","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =normal","title":"Normal"},{"location":"api/graph/vision/#protanopia","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =protanopia","title":"Protanopia"},{"location":"api/graph/vision/#protanomaly","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =protanomaly","title":"Protanomaly"},{"location":"api/graph/vision/#deuteranopia","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =deuteranopia","title":"Deuteranopia"},{"location":"api/graph/vision/#deuteranomaly","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =deuteranomaly","title":"Deuteranomaly"},{"location":"api/graph/vision/#tritanopia","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =tritanopia","title":"Tritanopia"},{"location":"api/graph/vision/#tritanomaly","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =tritanomaly","title":"Tritanomaly"},{"location":"api/graph/vision/#achromatopsia","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =achromatopsia","title":"Achromatopsia"},{"location":"api/graph/vision/#achromatomaly","text":"/api/v1/graph? e=2012-01-01T09:00 &no_legend=1 &q= 1,1,1,1,1,1,1 &stack=1 &tz=UTC & vision =achromatomaly","title":"Achromatomaly"},{"location":"asl/alerting-expressions/","text":"The stack language provides some basic techniques to convert an input line into a set of signals that can be used to trigger and visualize alert conditions. This section assumes a familiarity with the stack language and the alerting philosophy . Signal Line \u00b6 A signal line is a time series that indicates whether or not a condition is true for a particular interval. They are modelled by having zero indicate false and non-zero, typically 1, indicating true. Alerting expressions map some input time series to a set of signal lines that indicate true when in a triggering state. Threshold Alerts \u00b6 To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum Lets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query: The result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine. Dampening \u00b6 Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurrences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count. Input Rolling Count Dampened Signal nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count , 4, :gt Visualization \u00b6 A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph. nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :2over , :gt , :vspan , 40, :alpha , triggered, :legend , :rot , input, :legend , :rot , threshold, :legend , :rot Summary \u00b6 You should now know the basics of crafting an alert expression using the stack language. Other topics that may be of interest: Alerting Philosophy : overview of best practices associated with alerts. Stack Language Tutorial : comprehensive list of available operators. DES : double exponential smoothing. A technique for detecting anomalies in normally clean input signals where a precise threshold is unknown. For example, the requests per second hitting a service.","title":"Alerting Expressions"},{"location":"asl/alerting-expressions/#signal-line","text":"A signal line is a time series that indicates whether or not a condition is true for a particular interval. They are modelled by having zero indicate false and non-zero, typically 1, indicating true. Alerting expressions map some input time series to a set of signal lines that indicate true when in a triggering state.","title":"Signal Line"},{"location":"asl/alerting-expressions/#threshold-alerts","text":"To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum Lets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query: The result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine.","title":"Threshold Alerts"},{"location":"asl/alerting-expressions/#dampening","text":"Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurrences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count. Input Rolling Count Dampened Signal nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count , 4, :gt","title":"Dampening"},{"location":"asl/alerting-expressions/#visualization","text":"A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph. nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :2over , :gt , :vspan , 40, :alpha , triggered, :legend , :rot , input, :legend , :rot , threshold, :legend , :rot","title":"Visualization"},{"location":"asl/alerting-expressions/#summary","text":"You should now know the basics of crafting an alert expression using the stack language. Other topics that may be of interest: Alerting Philosophy : overview of best practices associated with alerts. Stack Language Tutorial : comprehensive list of available operators. DES : double exponential smoothing. A technique for detecting anomalies in normally clean input signals where a precise threshold is unknown. For example, the requests per second hitting a service.","title":"Summary"},{"location":"asl/alerting-philosophy/","text":"It is recommended for all alerts to adhere to the follow guidelines: Keep conditions simple . Alerts should be actionable . Check for measured failure on critical paths rather than a lack of success. Alerts should not have special cases for routine maintenance. Consider how the alert check can fail . Keep It Simple \u00b6 When an alert triggers, it should be easy to understand why. Similarly, if an alert doesn't fire, then it should be easy to check and see what happened. The more complicated an alert condition becomes, the harder it is to understand and debug. It is recommended to keep alert rules as a simple expression with a threshold and number of occurrences. An example of this is the following rule: CPU Usage > 80% for at least 5 minutes Multiple signals should only be combined if it improves the effectiveness of the alert. For example, what is an appropriate threshold for the number of requests that have error responses? What happens to that threshold if your cluster auto-scales? It is more effective to define the threshold as a percentage of total requests: (Num Errors / Num Total) > 0.01 for at least 5 minutes In some cases, a low volume can make the percentages less meaningful and result in false positives. For example, if your daily traffic pattern follows a sine curve, then the troughs may not represent a meaningful error percentage. Another example might be during failover exercises, if traffic has been failed over to another cluster. One way to compensate for this is to check the failure rate and overall volume: Percentage of Failures > X AND Volume > Y As a general rule, bias towards simplicity. If you are creating more complex expressions, then stop and think about why that complexity is needed. Are there other signals available that are easier to use? Can the application be changed so that it reports metrics which make it easier to diagnose? Actionable Alerts \u00b6 If an alert fires and sends a notification to users, someone should be motivated to investigate the problem. Alerts that are noisy or not actionable train people to ignore or filter out alert notifications. For cases where the response to an alert can be automated, such as terminating a bad instance, it shouldn't send out a notification unless there is a failure to perform the action. If you want a summary of cluster health, then use dashboards or reporting tools for this function; don't attempt to do this via alert notifications. Alerts should check something important. To setup effective alerts, you need to understand the application and have ways to detect failures for critical functionality. Avoid general system-type alerts that won't be investigated. For example, should you alert on high CPU usage? If you have done squeeze testing and you have information to indicate how CPU usage impacts the application, then it can be useful and it will provide a way to know a problem is coming before it impacts clients of the service. If you do not have this knowledge, then your alert may be under-tuned, leading to noisy notifications that may be ignored. Check for Measured Failure \u00b6 It is better to check for failures rather than trying to trigger based on an absence of information or a reduction in the amount of success. Absence of Information \u00b6 A typical example of this is a process that runs over a longer time period. For example, suppose we have an application that updates a metadata cache once per day and it takes an hour to refresh. It is not recommended to send an event on refresh success and then configure alerts based on the absence of the success event. Design the signals so you have a clear way to understand what error conditions may be occurring on and then alert if there is a problem. In this example, a better design would use a gauge that reports the loading time and a gauge that reports the age of the cache. You can then add alerts when the gauges for these error conditions exceed unacceptable thresholds. Reduction in Success \u00b6 Let's say we have a server that is taking traffic and we want to know if users are experiencing problems. How should we go about this? It is often tempting to look for things like a drop in the number of successful requests, because this can be a generic catch-all for many types of problems. However, alerts of this sort are inherently noisy. How do you know what the number of requests should be? While there are various schemes for trying to predict the behavior, you will spend a lot of time tuning alerts of this nature to get them to the point where they are not too noisy, but they still catch real issues. Further, these schemes cannot differentiate between problems for the service and unrelated drops such as a client having problems and failing to make the request in the first place. If you're not going to investigate these alerts when they fire or invest in tuning and maintaining them, just avoid this type of alert altogether. A better approach is to alert on the number of failures you are seeing from a service. Thresholds can often be determined automatically by looking at the percent of all requests that are failures. For middle tier services, it is also likely that data from the clients can be used to see a percentage of failure from a client perspective instead of, or in addition to, the server side view. Avoid Special Cases \u00b6 Alerts shouldn't have to be tuned or suppressed during regular maintenance such as replacing instance or doing deployments. As a simple example, consider an alert on the rate of failures. The general assumption would be that a deployment should not be noticed by clients and therefore the alert is still relevant. Alerts that are actionable and look for measured failure tend to work well. If a new instance is coming up, a lack of activity will mean a lack of failures until traffic is being received. At that time if there are failures they should be noticed. Startup Behavior \u00b6 What about different behavior during startup? Consider some examples for an application that has a long initialization time (~20 minutes) before it can take traffic: Discovery service state during initialization. Healthcheck failures during initialization. Performance may be different while starting. CPU usage is high while initializing but stabilizes and remains low during normal operation. For a discovery service like Eureka , the duration of the startup time shouldn't be an issue because the state clearly indicates if it is STARTING vs DOWN. If the healthcheck is used for a load balancer, then the decision to send traffic to instances should be fairly sensitive in order to minimize the impact to users. The bigger concern is the number of occurrences of healthcheck failures in a row, which can trigger automated actions like terminating an instance. When evaluating healthcheck failures, there are two distinct conditions to evaluate: non-200 responses and connection timeouts. The healthcheck logic should be tied to the Eureka heartbeat so that if the healthcheck is failing due to a non-200 response, the discovery state will be DOWN after initialization is complete. For the first condition, the alert should check for the number of occurrence of the DOWN state in the discovery service which will not trigger for the STARTING state used during application initialization. For the second condition, you would need to check for a disparity between the published discovery state and the healthcheck state: (DiscoveryStatus is UP) AND (Healthcheck != 200) for N minutes Note, unless you really need to do this it is probably better to just look at the healthcheck and have the num occurrences set to be longer than the typical startup time. For the CPU example, first reconsider whether general system check alerts are actually useful. Is it going to help you catch a real problem and be investigated when it triggers? If not, don't setup an alert on CPU and rely on alerts that check for failures on the critical path. If it is useful and you have squeeze testing results or other information so you know when a proxy metric like CPU actually indicates a problem, then you can configure it restricted with some signal that indicates the status. However, keep in mind that not all systems will allow complex expressions. For example, if you are auto-scaling will you be able to send the data such that it doesn't incorrectly skew the alarm? The more signals that are combined the harder it is to understand the alert and the more likely it is to fail in unexpected ways. Before adding more layers of duct tape think hard about the application and if you can change it to be easier to monitor and diagnose. Deployments \u00b6 At Netflix, a common deployment model is red/black. In this model, a new auto-scaling group the same size as the existing one will be created, traffic will transition over, and eventually the old auto-scaling group (ASG) will be deleted. This can create false alarms if you haven't thought about the signals being used to fire alerts. The most common alerting problem that occurs during deployments is related the use of averages. For example, the average request rate will drop in half if a new ASG comes up and you are aggregating across a cluster consisting of both old and new ASGs. If you follow the advice given earlier about crafting alerts based on a percentage of errors reported by clients of the application, then aggregating across clusters by sum usually won't be a problem. If the deployment is going well, then the overall failure rate seen by clients shouldn't be impacted. Another example of a deployment alerting problem is latency measurements. How can you tell the average latency across a cluster composed of new and old ASGs? Rather than trying to special case or exclude the new group of instances, you should define the alert signal based on the actual activity seen. If there is no activity within an ASG, then it will not impact the signal. Metrics libraries like Spectator send both a totalTime and count measurement separately to the backend. This allows the average to be computed using a simple sum aggregate with division: Sum(totalTime per instance in cluster) / Sum(count per instance in cluster) This calculation demonstrates how instances that are not receiving traffic will not contribute anything to the sums. Think About Failure \u00b6 An effective alert needs to be able to fire when there is a problem. However, when problems occur, it is possible that the problem will also impact the underlying data or mechanisms used to detect issues for the alert. It is worthwhile to spend time thinking about the ways in which your alerts can fail to detect events. How Can Signals Fail? \u00b6 The simplest area to think about is what is collecting and reporting the data. For example, if data is being reported by the plugin running in the application, then it won't work if the application crashes or cannot start. It is recommended to have some basic alerts using a data pipeline that will fail independently from the application. At Netflix, this typically involves checking the following conditions: The healthcheck is responding with 200. This signal indicates that a remote system was able to connect and query the application healthcheck. So the application is running and inbound traffic made it in. The application is registered with Eureka. Eureka uses a heartbeat mechanism, so checking the registration tells you the application is running and it is able to successfully send the heartbeat request. The metric data for those signals comes from a separate poller application. If these succeed, then the application should be healthy enough that alerts triggered from data local to the instance should be working. Alerting Scopes \u00b6 At Netflix, alert expressions for Atlas can be checked in three places: Backend. Alerts checked against the main backend server. Plugin. Alerts are checked by the plugin running on the instance. Poller. Alerts are checked by a poller service that collects data about instances. In practice, for a given application, the alerting scopes look like: Alerting scopes can be used to provide some level of redundancy with different failure modes. For example, the failure rate could be checked against the server stats and the client stats. Further, it is recommended to check alerts as close as possible to where the data is initially measured and collected. In other words, it is better to check the alerts on the plugin or poller rather than against the backend. The advantages of doing this are: Lower mean-time to detection (MTTD). Data going to the backend server has to be handled by several layers, which need to allow time for data from all nodes to arrive, time to index, etc. Alerts checked locally using the plugin or poller will get checked as data is being published and so they can trigger at the same time that data would hit the first step in the backend data pipeline. More robust to failure. When there are problems with the monitoring backends, server side alerts won't work or may have incorrect or partial data. Alerts checked locally on the plugin are immune to all problems off the instance other than being able to forward to the alert server. If the Atlas plugin or the instance running it are having issues, then it is likely that problems for the local alert check would also impact publishing, so server side alerts are not likely to provide a better view. Also, keep in mind that for most middle-tier services, the alert can be checked on the instances that call the service and thus can still fire if the instance has crashed. High-level instance health can be verified by an alert checked on the poller. Scales better as the amount of data and number of alerts increases. Many alerts, in particular if checked per node, require expensive queries to run on the backends. By checking alert using the plugin, the computation is spread out so each node is checking the alerts for that instance. So why not check all alerts on the client or poller? The primary disadvantages: Client and poller scopes can only use data that is available at that location. For a client, that means only the data that is reported by the plugin on that instance. For the poller, it means only data about health checks, discovery, and system stats from SNMP. Data cannot be aggregated across nodes for the cluster. This can make it harder to do things like outlier detection using a cluster-level aggregate as a baseline. However, keep in mind that for middle-tier services there is often an option to check on the plugin for the client.","title":"Alerting Philosophy"},{"location":"asl/alerting-philosophy/#keep-it-simple","text":"When an alert triggers, it should be easy to understand why. Similarly, if an alert doesn't fire, then it should be easy to check and see what happened. The more complicated an alert condition becomes, the harder it is to understand and debug. It is recommended to keep alert rules as a simple expression with a threshold and number of occurrences. An example of this is the following rule: CPU Usage > 80% for at least 5 minutes Multiple signals should only be combined if it improves the effectiveness of the alert. For example, what is an appropriate threshold for the number of requests that have error responses? What happens to that threshold if your cluster auto-scales? It is more effective to define the threshold as a percentage of total requests: (Num Errors / Num Total) > 0.01 for at least 5 minutes In some cases, a low volume can make the percentages less meaningful and result in false positives. For example, if your daily traffic pattern follows a sine curve, then the troughs may not represent a meaningful error percentage. Another example might be during failover exercises, if traffic has been failed over to another cluster. One way to compensate for this is to check the failure rate and overall volume: Percentage of Failures > X AND Volume > Y As a general rule, bias towards simplicity. If you are creating more complex expressions, then stop and think about why that complexity is needed. Are there other signals available that are easier to use? Can the application be changed so that it reports metrics which make it easier to diagnose?","title":"Keep It Simple"},{"location":"asl/alerting-philosophy/#actionable-alerts","text":"If an alert fires and sends a notification to users, someone should be motivated to investigate the problem. Alerts that are noisy or not actionable train people to ignore or filter out alert notifications. For cases where the response to an alert can be automated, such as terminating a bad instance, it shouldn't send out a notification unless there is a failure to perform the action. If you want a summary of cluster health, then use dashboards or reporting tools for this function; don't attempt to do this via alert notifications. Alerts should check something important. To setup effective alerts, you need to understand the application and have ways to detect failures for critical functionality. Avoid general system-type alerts that won't be investigated. For example, should you alert on high CPU usage? If you have done squeeze testing and you have information to indicate how CPU usage impacts the application, then it can be useful and it will provide a way to know a problem is coming before it impacts clients of the service. If you do not have this knowledge, then your alert may be under-tuned, leading to noisy notifications that may be ignored.","title":"Actionable Alerts"},{"location":"asl/alerting-philosophy/#check-for-measured-failure","text":"It is better to check for failures rather than trying to trigger based on an absence of information or a reduction in the amount of success.","title":"Check for Measured Failure"},{"location":"asl/alerting-philosophy/#absence-of-information","text":"A typical example of this is a process that runs over a longer time period. For example, suppose we have an application that updates a metadata cache once per day and it takes an hour to refresh. It is not recommended to send an event on refresh success and then configure alerts based on the absence of the success event. Design the signals so you have a clear way to understand what error conditions may be occurring on and then alert if there is a problem. In this example, a better design would use a gauge that reports the loading time and a gauge that reports the age of the cache. You can then add alerts when the gauges for these error conditions exceed unacceptable thresholds.","title":"Absence of Information"},{"location":"asl/alerting-philosophy/#reduction-in-success","text":"Let's say we have a server that is taking traffic and we want to know if users are experiencing problems. How should we go about this? It is often tempting to look for things like a drop in the number of successful requests, because this can be a generic catch-all for many types of problems. However, alerts of this sort are inherently noisy. How do you know what the number of requests should be? While there are various schemes for trying to predict the behavior, you will spend a lot of time tuning alerts of this nature to get them to the point where they are not too noisy, but they still catch real issues. Further, these schemes cannot differentiate between problems for the service and unrelated drops such as a client having problems and failing to make the request in the first place. If you're not going to investigate these alerts when they fire or invest in tuning and maintaining them, just avoid this type of alert altogether. A better approach is to alert on the number of failures you are seeing from a service. Thresholds can often be determined automatically by looking at the percent of all requests that are failures. For middle tier services, it is also likely that data from the clients can be used to see a percentage of failure from a client perspective instead of, or in addition to, the server side view.","title":"Reduction in Success"},{"location":"asl/alerting-philosophy/#avoid-special-cases","text":"Alerts shouldn't have to be tuned or suppressed during regular maintenance such as replacing instance or doing deployments. As a simple example, consider an alert on the rate of failures. The general assumption would be that a deployment should not be noticed by clients and therefore the alert is still relevant. Alerts that are actionable and look for measured failure tend to work well. If a new instance is coming up, a lack of activity will mean a lack of failures until traffic is being received. At that time if there are failures they should be noticed.","title":"Avoid Special Cases"},{"location":"asl/alerting-philosophy/#startup-behavior","text":"What about different behavior during startup? Consider some examples for an application that has a long initialization time (~20 minutes) before it can take traffic: Discovery service state during initialization. Healthcheck failures during initialization. Performance may be different while starting. CPU usage is high while initializing but stabilizes and remains low during normal operation. For a discovery service like Eureka , the duration of the startup time shouldn't be an issue because the state clearly indicates if it is STARTING vs DOWN. If the healthcheck is used for a load balancer, then the decision to send traffic to instances should be fairly sensitive in order to minimize the impact to users. The bigger concern is the number of occurrences of healthcheck failures in a row, which can trigger automated actions like terminating an instance. When evaluating healthcheck failures, there are two distinct conditions to evaluate: non-200 responses and connection timeouts. The healthcheck logic should be tied to the Eureka heartbeat so that if the healthcheck is failing due to a non-200 response, the discovery state will be DOWN after initialization is complete. For the first condition, the alert should check for the number of occurrence of the DOWN state in the discovery service which will not trigger for the STARTING state used during application initialization. For the second condition, you would need to check for a disparity between the published discovery state and the healthcheck state: (DiscoveryStatus is UP) AND (Healthcheck != 200) for N minutes Note, unless you really need to do this it is probably better to just look at the healthcheck and have the num occurrences set to be longer than the typical startup time. For the CPU example, first reconsider whether general system check alerts are actually useful. Is it going to help you catch a real problem and be investigated when it triggers? If not, don't setup an alert on CPU and rely on alerts that check for failures on the critical path. If it is useful and you have squeeze testing results or other information so you know when a proxy metric like CPU actually indicates a problem, then you can configure it restricted with some signal that indicates the status. However, keep in mind that not all systems will allow complex expressions. For example, if you are auto-scaling will you be able to send the data such that it doesn't incorrectly skew the alarm? The more signals that are combined the harder it is to understand the alert and the more likely it is to fail in unexpected ways. Before adding more layers of duct tape think hard about the application and if you can change it to be easier to monitor and diagnose.","title":"Startup Behavior"},{"location":"asl/alerting-philosophy/#deployments","text":"At Netflix, a common deployment model is red/black. In this model, a new auto-scaling group the same size as the existing one will be created, traffic will transition over, and eventually the old auto-scaling group (ASG) will be deleted. This can create false alarms if you haven't thought about the signals being used to fire alerts. The most common alerting problem that occurs during deployments is related the use of averages. For example, the average request rate will drop in half if a new ASG comes up and you are aggregating across a cluster consisting of both old and new ASGs. If you follow the advice given earlier about crafting alerts based on a percentage of errors reported by clients of the application, then aggregating across clusters by sum usually won't be a problem. If the deployment is going well, then the overall failure rate seen by clients shouldn't be impacted. Another example of a deployment alerting problem is latency measurements. How can you tell the average latency across a cluster composed of new and old ASGs? Rather than trying to special case or exclude the new group of instances, you should define the alert signal based on the actual activity seen. If there is no activity within an ASG, then it will not impact the signal. Metrics libraries like Spectator send both a totalTime and count measurement separately to the backend. This allows the average to be computed using a simple sum aggregate with division: Sum(totalTime per instance in cluster) / Sum(count per instance in cluster) This calculation demonstrates how instances that are not receiving traffic will not contribute anything to the sums.","title":"Deployments"},{"location":"asl/alerting-philosophy/#think-about-failure","text":"An effective alert needs to be able to fire when there is a problem. However, when problems occur, it is possible that the problem will also impact the underlying data or mechanisms used to detect issues for the alert. It is worthwhile to spend time thinking about the ways in which your alerts can fail to detect events.","title":"Think About Failure"},{"location":"asl/alerting-philosophy/#how-can-signals-fail","text":"The simplest area to think about is what is collecting and reporting the data. For example, if data is being reported by the plugin running in the application, then it won't work if the application crashes or cannot start. It is recommended to have some basic alerts using a data pipeline that will fail independently from the application. At Netflix, this typically involves checking the following conditions: The healthcheck is responding with 200. This signal indicates that a remote system was able to connect and query the application healthcheck. So the application is running and inbound traffic made it in. The application is registered with Eureka. Eureka uses a heartbeat mechanism, so checking the registration tells you the application is running and it is able to successfully send the heartbeat request. The metric data for those signals comes from a separate poller application. If these succeed, then the application should be healthy enough that alerts triggered from data local to the instance should be working.","title":"How Can Signals Fail?"},{"location":"asl/alerting-philosophy/#alerting-scopes","text":"At Netflix, alert expressions for Atlas can be checked in three places: Backend. Alerts checked against the main backend server. Plugin. Alerts are checked by the plugin running on the instance. Poller. Alerts are checked by a poller service that collects data about instances. In practice, for a given application, the alerting scopes look like: Alerting scopes can be used to provide some level of redundancy with different failure modes. For example, the failure rate could be checked against the server stats and the client stats. Further, it is recommended to check alerts as close as possible to where the data is initially measured and collected. In other words, it is better to check the alerts on the plugin or poller rather than against the backend. The advantages of doing this are: Lower mean-time to detection (MTTD). Data going to the backend server has to be handled by several layers, which need to allow time for data from all nodes to arrive, time to index, etc. Alerts checked locally using the plugin or poller will get checked as data is being published and so they can trigger at the same time that data would hit the first step in the backend data pipeline. More robust to failure. When there are problems with the monitoring backends, server side alerts won't work or may have incorrect or partial data. Alerts checked locally on the plugin are immune to all problems off the instance other than being able to forward to the alert server. If the Atlas plugin or the instance running it are having issues, then it is likely that problems for the local alert check would also impact publishing, so server side alerts are not likely to provide a better view. Also, keep in mind that for most middle-tier services, the alert can be checked on the instances that call the service and thus can still fire if the instance has crashed. High-level instance health can be verified by an alert checked on the poller. Scales better as the amount of data and number of alerts increases. Many alerts, in particular if checked per node, require expensive queries to run on the backends. By checking alert using the plugin, the computation is spread out so each node is checking the alerts for that instance. So why not check all alerts on the client or poller? The primary disadvantages: Client and poller scopes can only use data that is available at that location. For a client, that means only the data that is reported by the plugin on that instance. For the poller, it means only data about health checks, discovery, and system stats from SNMP. Data cannot be aggregated across nodes for the cluster. This can make it harder to do things like outlier detection using a cluster-level aggregate as a baseline. However, keep in mind that for middle-tier services there is often an option to check on the plugin for the client.","title":"Alerting Scopes"},{"location":"asl/des/","text":"Double exponential smoothing (DES) is a simple technique for generating a smooth trend line from another time series. This technique is often used to generate a dynamic threshold for alerting. Warning Alerts on dynamic thresholds should be expected to be noisy. They are looking for strange behavior rather than an actual problem causing impact. Make sure you will actually spend the time to tune and investigate the alarms before using this approach. See the alerting philosophy guide for more information on best practices. Tuning \u00b6 The :des operator takes 4 parameters: An input time series training - the number of intervals to use for warming up before generating an output alpha - is a data smoothing factor beta - is a trend smoothing factor Training \u00b6 The training parameter defines how many intervals to allow the DES to warmup. In the graph below the gaps from the start of the chart to the smoothed lines reflects the training window used: Typically a training window of 10 has been sufficient as DES will adjust to the input fairly quick. However, in some cases if there is a massive change in the input it can cause DES to oscillate, for example: Alpha \u00b6 Alpha is the data smoothing factor. A value of 1 means no smoothing. The closer the value gets to 0 the smoother the line should get. Example: Beta \u00b6 Beta is a trend smoothing factor. Visually it is most apparent when alpha is small. Example with alpha = 0.01 : Recommended Values \u00b6 Experimentally we have converged on 3 sets of values based on how quickly it should adjust to changing levels in the input signal. Helper Alpha Beta :des-fast 0.1 0.02 :des-slower 0.05 0.03 :des-slow 0.03 0.04 Here is an example of how they behave for a sharp drop and recovery: For a more gradual drop: If the drop is smooth enough then DES can adjust without ever triggering. Alerting \u00b6 For alerting purposes the DES line will typically get multiplied by a fraction and then checked to see whether the input line drops below the DES value for a given interval. # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction :des-fast, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, The vertical spans show when the expression would have triggered with due to the input dropping below the DES line at 85%: Epic Macros \u00b6 There are two helper macros, des-epic-signal and des-epic-viz , that match the behavior of the previous epic DES alarms. The first generates a signal line for the alarm. The second creates a visualization to make it easier to see what is happening. Both take the following arguments: line - input line trainingSize - training size parameter for DES alpha - alpha parameter for DES beta - beta parameter for DES maxPercent - percentage offset to use for the upper bound. Can be set to NaN to disable the upper bound check. minPercent - percentage offset to use for the lower bound. Can be set to NaN to disable the lower bound check. noise - a fixed offset that is the minimum difference between the signal and prediction that is required before the signal should trigger. This is primarily used to avoid false alarms where the percentage bound can become ineffective for routine noise during the troughs. Examples: nf.cluster,alerttest, :eq , name,requestsPerSecond, :eq , :and , :sum , 10,0.1,0.02,0.15,0.15,10, :des-epic-viz Example with no lower bound: nf.cluster,alerttest, :eq , name,requestsPerSecond, :eq , :and , :sum , 10,0.1,0.02,0.15,NaN,10, :des-epic-viz","title":"Double Exponential Smoothing"},{"location":"asl/des/#tuning","text":"The :des operator takes 4 parameters: An input time series training - the number of intervals to use for warming up before generating an output alpha - is a data smoothing factor beta - is a trend smoothing factor","title":"Tuning"},{"location":"asl/des/#training","text":"The training parameter defines how many intervals to allow the DES to warmup. In the graph below the gaps from the start of the chart to the smoothed lines reflects the training window used: Typically a training window of 10 has been sufficient as DES will adjust to the input fairly quick. However, in some cases if there is a massive change in the input it can cause DES to oscillate, for example:","title":"Training"},{"location":"asl/des/#alpha","text":"Alpha is the data smoothing factor. A value of 1 means no smoothing. The closer the value gets to 0 the smoother the line should get. Example:","title":"Alpha"},{"location":"asl/des/#beta","text":"Beta is a trend smoothing factor. Visually it is most apparent when alpha is small. Example with alpha = 0.01 :","title":"Beta"},{"location":"asl/des/#recommended-values","text":"Experimentally we have converged on 3 sets of values based on how quickly it should adjust to changing levels in the input signal. Helper Alpha Beta :des-fast 0.1 0.02 :des-slower 0.05 0.03 :des-slow 0.03 0.04 Here is an example of how they behave for a sharp drop and recovery: For a more gradual drop: If the drop is smooth enough then DES can adjust without ever triggering.","title":"Recommended Values"},{"location":"asl/des/#alerting","text":"For alerting purposes the DES line will typically get multiplied by a fraction and then checked to see whether the input line drops below the DES value for a given interval. # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction :des-fast, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, The vertical spans show when the expression would have triggered with due to the input dropping below the DES line at 85%:","title":"Alerting"},{"location":"asl/des/#epic-macros","text":"There are two helper macros, des-epic-signal and des-epic-viz , that match the behavior of the previous epic DES alarms. The first generates a signal line for the alarm. The second creates a visualization to make it easier to see what is happening. Both take the following arguments: line - input line trainingSize - training size parameter for DES alpha - alpha parameter for DES beta - beta parameter for DES maxPercent - percentage offset to use for the upper bound. Can be set to NaN to disable the upper bound check. minPercent - percentage offset to use for the lower bound. Can be set to NaN to disable the lower bound check. noise - a fixed offset that is the minimum difference between the signal and prediction that is required before the signal should trigger. This is primarily used to avoid false alarms where the percentage bound can become ineffective for routine noise during the troughs. Examples: nf.cluster,alerttest, :eq , name,requestsPerSecond, :eq , :and , :sum , 10,0.1,0.02,0.15,0.15,10, :des-epic-viz Example with no lower bound: nf.cluster,alerttest, :eq , name,requestsPerSecond, :eq , :and , :sum , 10,0.1,0.02,0.15,NaN,10, :des-epic-viz","title":"Epic Macros"},{"location":"asl/tutorial/","text":"Atlas Stack Language is designed to be a stable method of representing complex data queries in a URL-friendly format. It is loosely based on the RPN expressions supported by Tobias Oetiker 's rrdtool . The following is an example of a stack language expression: nf.cluster,discovery,:eq,(,nf.zone,),:by This example pushes two strings nf.cluster and discovery onto the stack and then executes the command :eq . The equal command pops two strings from the stack and pushes a query object onto the stack. The behavior can be described by the stack effect String:key String:value \u2013 Query . We then push a list of tag keys to the stack and execute the command :by to group the results. Parts \u00b6 There are only four reserved symbols used for structuring the expression: ,:() Commas separate items on the stack. So a,b puts two strings on the stack with values \"a\" and \"b\" . Colon is used to prefix operations. If the first character is a colon the item will be treated as a command to run. For example, a,:dup , will push \"a\" on the stack and then execute the duplicate operation. Parenthesis are used to indicate the start and end of a list. The expression (,) puts an empty list on the stack. Commands inside of a list will not be executed unless the list is passed to the call command. For example, (,:dup,) will push a list with a single string value of \":dup\" on to the stack. Data Model \u00b6 The stack language is primarily used for representing expressions over tagged time series data. A tag is a string key value pair used to describe a measurement. Atlas requires at least one tag with a key of name . Example tags represented as a JSON map: { \"name\" : \"jvm.gc.pause\" , \"cause\" : \"Allocation_Failure\" , \"statistic\" : \"count\" , \"nf.app\" : \"www\" , \"nf.cluster\" : \"www-main\" , \"nf.asg\" : \"www-main-v001\" , \"nf.stack\" : \"main\" , \"nf.node\" : \"i-01\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1a\" } Typically, tags should be dimensions that allow you to use the name as a pivot and other tags to drill down into the data. The tag keys are similar to columns in a traditional table, however, it is important to note that not all time series will have the same set of tag keys. The tags are used to identify a time series, which conceptually is a set of timestamp value pairs. Here is a simplified data set shown as a table: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] cpuUsage www i-02 [(05:00, 20.0), (05:01, 37.0)] cpuUsage db i-03 [(05:00, 57.0), (05:01, 62.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] diskUsage www i-02 [(05:00, 7.0), (05:01, 8.0)] requestRate www [(05:00, 33.0), (05:01, 31.0)] The table above will be used for the examples in later sections. Simple Expressions \u00b6 All expressions generally have four parts: Choosing: selects a set of time series. Aggregation: defines how to combine the selected time series. Math: manipulate the time series values or combine aggregated results with binary operations. Presentation: adjust how the data is presented in a chart. Choosing \u00b6 The \"choosing\" or predicate section is used to select a set of time series. The primary predicate operators are :eq and :and . Sample query to select all time series where the key node is equal to i-01 : node,i-01, :eq If you are familiar with SQL and assume that tag keys are column names, then this would be equivalent to: select * from time_series where node = 'i-01'; Using the example data set this query would return the following subset: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] To get just the cpu usage for that node, use :and : node,i-01, :eq , name,cpuUsage, :eq , :and This would result in: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] Aggregation \u00b6 An aggregation function maps a set of time series that matched the predicate to a single time series. Atlas supports four aggregate functions: sum , min , max , and count . If no aggregate is specified on an expression, then sum will be used implicitly. Using the example data set , these two expressions would be equivalent: app,www, :eq , name,cpuUsage, :eq , :and app,www, :eq , name,cpuUsage, :eq , :and , :sum And would result in a single output time series: name app values cpuUsage www [(05:00, 53.0), (05:01, 68.0)] Note that the node is not present in the output. The set of tags on the output will be ones with exact matches in the predicate clause or explicitly listed in the group by . If you wanted the max cpu for the application, then you would write: app,www, :eq , name,cpuUsage, :eq , :and , :max What if we want the average? The count aggregate is used to determine how many time series had a value for a given time. To get the average we divide the sum by the count. app,www, :eq , name,cpuUsage, :eq , :and , :dup , :sum , :swap , :count , :div There is a helper macro :avg that will do this for you, so you can write: app,www, :eq , name,cpuUsage, :eq , :and , :avg Group By \u00b6 In many cases we want to group the results that were selected and return one aggregate per group. As an example suppose I want to see maximum cpu usage by application: name,cpuUsage, :eq , :max , (,app,), :by Using the example data set , this would result in a two output time series: name app values cpuUsage www [(05:00, 33.0), (05:01, 37.0)] cpuUsage db [(05:00, 57.0), (05:01, 62.0)] Math \u00b6 Once you have a set of lines, it can be useful to manipulate them. The supported operations generally fall into two categories: unary operations to alter a single time series and binary operations that combine two time series. Examples of unary operations are negate and absolute value . To apply the absolute value: app,web, :eq , name,cpu, :eq , :and , :sum , :abs Multiple operations can be applied, for example, negating the line then applying the absolute value: app,web, :eq , name,cpu, :eq , :and , :sum , :neg , :abs Common binary operations are add , subtract , multiply , and divide . The aggregation section has an example of using divide to compute the average. Presentation \u00b6 Once you have a final expression, you can apply presentation settings to alter how a time series is displayed in the chart. One of the most common examples is setting the label to use for the legend : app,www, :eq , name,cpuUsage, :eq , :and , :avg , average+cpu+usage, :legend You can also use tag keys as variables in the legend text, for example, setting the legend to the application: app,www, :eq , name,cpuUsage, :eq , :and , :avg , (,app,), :by , $(app), :legend It is also common to adjust the how the lines are shown. For example, to stack each of the lines we can use the :stack command to adjust the line style: app,www, :eq , name,cpuUsage, :eq , :and , :avg , (,app,), :by , :stack , $(app), :legend","title":"Tutorial"},{"location":"asl/tutorial/#parts","text":"There are only four reserved symbols used for structuring the expression: ,:() Commas separate items on the stack. So a,b puts two strings on the stack with values \"a\" and \"b\" . Colon is used to prefix operations. If the first character is a colon the item will be treated as a command to run. For example, a,:dup , will push \"a\" on the stack and then execute the duplicate operation. Parenthesis are used to indicate the start and end of a list. The expression (,) puts an empty list on the stack. Commands inside of a list will not be executed unless the list is passed to the call command. For example, (,:dup,) will push a list with a single string value of \":dup\" on to the stack.","title":"Parts"},{"location":"asl/tutorial/#data-model","text":"The stack language is primarily used for representing expressions over tagged time series data. A tag is a string key value pair used to describe a measurement. Atlas requires at least one tag with a key of name . Example tags represented as a JSON map: { \"name\" : \"jvm.gc.pause\" , \"cause\" : \"Allocation_Failure\" , \"statistic\" : \"count\" , \"nf.app\" : \"www\" , \"nf.cluster\" : \"www-main\" , \"nf.asg\" : \"www-main-v001\" , \"nf.stack\" : \"main\" , \"nf.node\" : \"i-01\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1a\" } Typically, tags should be dimensions that allow you to use the name as a pivot and other tags to drill down into the data. The tag keys are similar to columns in a traditional table, however, it is important to note that not all time series will have the same set of tag keys. The tags are used to identify a time series, which conceptually is a set of timestamp value pairs. Here is a simplified data set shown as a table: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] cpuUsage www i-02 [(05:00, 20.0), (05:01, 37.0)] cpuUsage db i-03 [(05:00, 57.0), (05:01, 62.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] diskUsage www i-02 [(05:00, 7.0), (05:01, 8.0)] requestRate www [(05:00, 33.0), (05:01, 31.0)] The table above will be used for the examples in later sections.","title":"Data Model"},{"location":"asl/tutorial/#simple-expressions","text":"All expressions generally have four parts: Choosing: selects a set of time series. Aggregation: defines how to combine the selected time series. Math: manipulate the time series values or combine aggregated results with binary operations. Presentation: adjust how the data is presented in a chart.","title":"Simple Expressions"},{"location":"asl/tutorial/#choosing","text":"The \"choosing\" or predicate section is used to select a set of time series. The primary predicate operators are :eq and :and . Sample query to select all time series where the key node is equal to i-01 : node,i-01, :eq If you are familiar with SQL and assume that tag keys are column names, then this would be equivalent to: select * from time_series where node = 'i-01'; Using the example data set this query would return the following subset: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)] diskUsage www i-01 [(05:00, 9.0), (05:01, 9.0)] To get just the cpu usage for that node, use :and : node,i-01, :eq , name,cpuUsage, :eq , :and This would result in: name app node values cpuUsage www i-01 [(05:00, 33.0), (05:01, 31.0)]","title":"Choosing"},{"location":"asl/tutorial/#aggregation","text":"An aggregation function maps a set of time series that matched the predicate to a single time series. Atlas supports four aggregate functions: sum , min , max , and count . If no aggregate is specified on an expression, then sum will be used implicitly. Using the example data set , these two expressions would be equivalent: app,www, :eq , name,cpuUsage, :eq , :and app,www, :eq , name,cpuUsage, :eq , :and , :sum And would result in a single output time series: name app values cpuUsage www [(05:00, 53.0), (05:01, 68.0)] Note that the node is not present in the output. The set of tags on the output will be ones with exact matches in the predicate clause or explicitly listed in the group by . If you wanted the max cpu for the application, then you would write: app,www, :eq , name,cpuUsage, :eq , :and , :max What if we want the average? The count aggregate is used to determine how many time series had a value for a given time. To get the average we divide the sum by the count. app,www, :eq , name,cpuUsage, :eq , :and , :dup , :sum , :swap , :count , :div There is a helper macro :avg that will do this for you, so you can write: app,www, :eq , name,cpuUsage, :eq , :and , :avg","title":"Aggregation"},{"location":"asl/tutorial/#group-by","text":"In many cases we want to group the results that were selected and return one aggregate per group. As an example suppose I want to see maximum cpu usage by application: name,cpuUsage, :eq , :max , (,app,), :by Using the example data set , this would result in a two output time series: name app values cpuUsage www [(05:00, 33.0), (05:01, 37.0)] cpuUsage db [(05:00, 57.0), (05:01, 62.0)]","title":"Group By"},{"location":"asl/tutorial/#math","text":"Once you have a set of lines, it can be useful to manipulate them. The supported operations generally fall into two categories: unary operations to alter a single time series and binary operations that combine two time series. Examples of unary operations are negate and absolute value . To apply the absolute value: app,web, :eq , name,cpu, :eq , :and , :sum , :abs Multiple operations can be applied, for example, negating the line then applying the absolute value: app,web, :eq , name,cpu, :eq , :and , :sum , :neg , :abs Common binary operations are add , subtract , multiply , and divide . The aggregation section has an example of using divide to compute the average.","title":"Math"},{"location":"asl/tutorial/#presentation","text":"Once you have a final expression, you can apply presentation settings to alter how a time series is displayed in the chart. One of the most common examples is setting the label to use for the legend : app,www, :eq , name,cpuUsage, :eq , :and , :avg , average+cpu+usage, :legend You can also use tag keys as variables in the legend text, for example, setting the legend to the application: app,www, :eq , name,cpuUsage, :eq , :and , :avg , (,app,), :by , $(app), :legend It is also common to adjust the how the lines are shown. For example, to stack each of the lines we can use the :stack command to adjust the line style: app,www, :eq , name,cpuUsage, :eq , :and , :avg , (,app,), :by , :stack , $(app), :legend","title":"Presentation"},{"location":"asl/ref/-rot/","text":"Input Stack: b a ... \u21e8 Output Stack: a ... b Rotate the stack so that the item at the top is now at the bottom. Example: a,b,c,d, :-rot Pos Input Output 0 d c 1 c b 2 b a 3 a d","title":"-rot"},{"location":"asl/ref/2over/","text":"Input Stack: b a \u21e8 Output Stack: b a b a Shorthand equivalent to writing: :over,:over Example: a,b, :2over Pos Input Output 0 b b 1 a a 2 b 3 a","title":"2over"},{"location":"asl/ref/abs/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute a new time series where each interval has the absolute value of the input time series. Examples: 0 64 -64 0, :abs 64, :abs -64, :abs","title":"abs"},{"location":"asl/ref/add/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 + ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a addNaN b) where a and b are the corresponding intervals in the input time series. Sample: :add 3.0 0.0 1.0 1.0 NaN Input 1 1.0 0.0 1.0 1.0 NaN Input 2 2.0 0.0 0.0 NaN NaN Use the fadd operator to get strict floating point behavior. Examples Example adding a constant: Before After name,sps, :eq , 30e3 name,sps, :eq , 30e3, :add Example adding two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :add","title":"add"},{"location":"asl/ref/all/","text":"Warning Deprecated: use :by instead. This operation is primarily intended for debugging and results can be confusing unless you have detailed understanding of Atlas internals. Input Stack: Query \u21e8 Output Stack: DataExpr Avoid aggregation and output all time series that match the query.","title":"all"},{"location":"asl/ref/alpha/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Set the alpha value for the colors on the line. The value should be a two digit hex number where 00 is transparent and ff is opague. This setting will be ignored if the color setting is used for the same line. Before After name,sps, :eq , :sum , :stack name,sps, :eq , :sum , :stack , 40, :alpha Before After name,sps, :eq , :sum , :stack , f00, :color name,sps, :eq , :sum , :stack , f00, :color , 40, :alpha","title":"alpha"},{"location":"asl/ref/and/","text":"There are two variants of the :and operator. Choosing \u00b6 Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 AND q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if both of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 AND ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a AND b) where a and b are the corresponding intervals in the input time series. For example: Time a b a AND b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a and b are both non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt , :and","title":"and"},{"location":"asl/ref/and/#choosing","text":"Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 AND q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if both of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/and/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 AND ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a AND b) where a and b are the corresponding intervals in the input time series. For example: Time a b a AND b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a and b are both non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt , :and","title":"Math"},{"location":"asl/ref/area/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: StyleExpr Change the line style to be area. In this mode the line will be filled to 0 on the Y-axis. See the line style examples page for more information. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :area","title":"area"},{"location":"asl/ref/as/","text":"Input Stack: replacement: String original: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Map a tag key name to an alternate name. This can be useful for cases where it is desirable to perform a binary math operation, but the two sides use different tag keys for the same concept. The common IPC metrics are an example where it might be desirable to compare RPS for servers and their clients. The server side RPS would group by nf.app while the client side view would group by ipc.server.app . Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , nf.cluster,c, :as , $c, :legend","title":"as"},{"location":"asl/ref/avg/","text":"Average or mean aggregation operator. There are two variants of the :avg operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr A helper method that computes the average or mean from one or more time series using the count aggregate to determine how many time series have data at an interval and dividing the sum of the values by the count. This avoids issues where one or time series are missing data at a specific time resulting in an artificially low average. E.g. the expression: name,ssCpuUser, :eq , :avg when matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the average. This leads to a final result of: Name Data ssCpuUser [3.33, 3.66, 4.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the average of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :avg","title":"avg"},{"location":"asl/ref/avg/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr A helper method that computes the average or mean from one or more time series using the count aggregate to determine how many time series have data at an interval and dividing the sum of the values by the count. This avoids issues where one or time series are missing data at a specific time resulting in an artificially low average. E.g. the expression: name,ssCpuUser, :eq , :avg when matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the average. This leads to a final result of: Name Data ssCpuUser [3.33, 3.66, 4.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/avg/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the average of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :avg","title":"Math"},{"location":"asl/ref/axis/","text":"Input Stack: Int TimeSeriesExpr \u21e8 Output Stack: StyleExpr Specify which Y-axis to use for the line. The value specified is the axis number and should be an integer in the range 0 to 4 inclusive. Example: Before After name,sps, :eq , :sum , 42 name,sps, :eq , :sum , 42,1, :axis","title":"axis"},{"location":"asl/ref/bottomk-others-avg/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes an average aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-avg","title":"bottomk-others-avg"},{"location":"asl/ref/bottomk-others-max/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a max aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-max","title":"bottomk-others-max"},{"location":"asl/ref/bottomk-others-min/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a min aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-min","title":"bottomk-others-min"},{"location":"asl/ref/bottomk-others-sum/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a sum aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-sum","title":"bottomk-others-sum"},{"location":"asl/ref/bottomk/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic . Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk In some cases it can be useful to see an aggregate summary of the other time series that were not part of the bottom set. This can be accomplished using the :bottomk-others-$(aggr) operators. For more details see: :bottomk-others-avg :bottomk-others-max :bottomk-others-min :bottomk-others-sum","title":"bottomk"},{"location":"asl/ref/by/","text":"Group by operator. There are two variants of the :by operator. Aggregation \u00b6 Input Stack: keys: List[String] AggregationFunction \u21e8 Output Stack: DataExpr Groups the matching time series by a set of keys and applies an aggregation to matches of the group. name,ssCpu, :re , (,name,), :by When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpu User alerttest i-0123 [1.0, 2.0, NaN] ssCpu System alerttest i-0123 [3.0, 4.0, 5.0] ssCpu User nccp i-0abc [8.0, 7.0, 6.0] ssCpu System nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpu User api i-0456 [1.0, 2.0, 2.0] The aggregation function will be applied independently for each group. In this example above there are two matching values for the group by key name . This leads to a final result of: Name Data ssCpuSystem [9.0, 11.0, 13.0] ssCpuUser [10.0, 11.0, 8.0] The name tag is included in the result set since it is used for the grouping. Math \u00b6 Input Stack: keys: List[String] TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Groups the time series from the input expression by a set of keys and applies an aggregation to matches of the group. The keys used for this grouping must be a subset of keys from the initial group by clause. Example: Before After name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by , :count , (,nf.cluster,), :by","title":"by"},{"location":"asl/ref/by/#aggregation","text":"Input Stack: keys: List[String] AggregationFunction \u21e8 Output Stack: DataExpr Groups the matching time series by a set of keys and applies an aggregation to matches of the group. name,ssCpu, :re , (,name,), :by When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpu User alerttest i-0123 [1.0, 2.0, NaN] ssCpu System alerttest i-0123 [3.0, 4.0, 5.0] ssCpu User nccp i-0abc [8.0, 7.0, 6.0] ssCpu System nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpu User api i-0456 [1.0, 2.0, 2.0] The aggregation function will be applied independently for each group. In this example above there are two matching values for the group by key name . This leads to a final result of: Name Data ssCpuSystem [9.0, 11.0, 13.0] ssCpuUser [10.0, 11.0, 8.0] The name tag is included in the result set since it is used for the grouping.","title":"Aggregation"},{"location":"asl/ref/by/#math","text":"Input Stack: keys: List[String] TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Groups the time series from the input expression by a set of keys and applies an aggregation to matches of the group. The keys used for this grouping must be a subset of keys from the initial group by clause. Example: Before After name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by , :count , (,nf.cluster,), :by","title":"Math"},{"location":"asl/ref/call/","text":"Input Stack: ? List \u21e8 Output Stack: ? Pops a list off the stack and executes it as a program. Example: (,a,), :call Pos Input Output 0 List(a) a","title":"call"},{"location":"asl/ref/cf-avg/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be average.","title":"cf-avg"},{"location":"asl/ref/cf-max/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be max.","title":"cf-max"},{"location":"asl/ref/cf-min/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be min.","title":"cf-min"},{"location":"asl/ref/cf-sum/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be sum.","title":"cf-sum"},{"location":"asl/ref/cg/","text":"Input Stack: keys: List[String] Expr \u21e8 Output Stack: Expr Recursively add a list of keys to group by expressions. This can be useful for tooling that needs to adjust existing expressions to include keys in the grouping. Before After name,sps, :eq , (,nf.app,), :by name,sps, :eq , (,nf.app,), :by , (,nf.cluster,), :cg","title":"cg"},{"location":"asl/ref/clamp-max/","text":"Input Stack: Double TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Restricts the maximum value of the output time series to the specified value. Values from the input time series that are less than or equal to the maximum will not be changed. A common use-case is to allow for auto-scaled axis up to a specified bound. The axis parameters for controlling the axis bounds have the following limitations: They apply to everything on the axis and cannot be targeted to a specific line. Are either absolute or set based on the data. For data with occasional spikes this can hide important details. Consider the following graph: The spike makes it difficult to make out any detail for other times. One option to handle this is to use an alternate axis scale such as logarithmic that gives a higher visual weight to the smaller values. However, it is often easier for a user to reason about a linear scale, in particular, for times when there is no spike in the graph window. If there is a known max reasonable value, then the :clamp-max operator can be used to restrict the line if and only if it exceeds the designated max. For example, if we limit the graph above to 25: Before After name,sps, :eq , :sum name,sps, :eq , :sum , 60e3, :clamp-max","title":"clamp-max"},{"location":"asl/ref/clamp-min/","text":"Input Stack: Double TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Restricts the minimum value of the output time series to the specified value. Values from the input time series that are greater than or equal to the minimum will not be changed. A common use-case is to allow for auto-scaled axis up to a specified bound. For more details see :clamp-max . Example: Before After name,sps, :eq , :sum name,sps, :eq , :sum , 200e3, :clamp-min","title":"clamp-min"},{"location":"asl/ref/clear/","text":"Input Stack: ... \u21e8 Output Stack: Remove all items from the stack. Example: a,b,c, :clear Pos Input Output 0 c 1 b 2 a","title":"clear"},{"location":"asl/ref/color/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Set the color for the line. The value should be one of: Hex triplet , e.g. f00 is red. 6 digit hex RBG, e.g. ff0000 is red. 8 digit hex ARGB, e.g. ffff0000 is red. The first byte is the alpha setting to use with the color. For queries with multiple time series, color palettes are available to automatically assign different colors to the various series. See Color Palettes . Before After name,sps, :eq name,sps, :eq , ff0000, :color","title":"color"},{"location":"asl/ref/const/","text":"Input Stack: Double \u21e8 Output Stack: TimeSeriesExpr Generates a line where each datapoint is a constant value. Any double value that is left on the stack will get implicitly converted to a constant line, so this operator is typically not used explicitly. Before After 42 42, :const","title":"const"},{"location":"asl/ref/count/","text":"Count aggregation operator. There are two variants of the :count operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the number of time series that match the query and have a value for a given interval. name,ssCpuUser, :eq , :count When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the count. This leads to a final result of: Name Data ssCpuUser [3.0, 3.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the number of time series from the input expression and have a value for a given interval. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :count","title":"count"},{"location":"asl/ref/count/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the number of time series that match the query and have a value for a given interval. name,ssCpuUser, :eq , :count When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the count. This leads to a final result of: Name Data ssCpuUser [3.0, 3.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/count/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the number of time series from the input expression and have a value for a given interval. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :count","title":"Math"},{"location":"asl/ref/cq/","text":"Input Stack: Query Expr \u21e8 Output Stack: Expr Recursively AND a common query to all queries in an expression. If the first parameter is not an expression, then it will be not be modified. Example: name,ssCpuUser, :eq , name,DiscoveryStatus_UP, :eq , :mul , nf.app,alerttest, :eq , :cq Before After name,ssCpuUser, :eq , name,DiscoveryStatus_UP, :eq , :mul , nf.app,alerttest, :eq name,ssCpuUser, :eq , name,DiscoveryStatus_UP, :eq , :mul , nf.app,alerttest, :eq , :cq Before After 42,nf.app,alerttest, :eq 42,nf.app,alerttest, :eq , :cq","title":"cq"},{"location":"asl/ref/decode/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Note It is recommended to avoid using special symbols or trying to encode structural information into tag values. This feature should be used sparingly and with great care to ensure it will not result in a combinatorial explosion. Perform decoding of the legend strings. Generally data going into Atlas is restricted to simple ascii characters that are easy to use as part of a URI. Most commonly the clients will convert unsupported characters to an _ . In some case it is desirable to be able to reverse that for the purposes of presentation. none : this is the default. It will not modify the legend string. hex : perform a hex decoding of the legend string. This is similar to url encoding except that the _ character is used instead of % to indicate the start of an encoded symbol. The decoding is lenient, if the characters following the _ are not valid hexadecimal digits then it will just copy those characters without modification. Since: 1.5 Example: Hex to ASCII 1,one_21_25_26_3F, :legend , hex, :decode","title":"decode"},{"location":"asl/ref/delay/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Delays the values by the window size. This is similar to the :offset operator except that it can be applied to any input line instead of just changing the time window fetched with a DataExpr. Short delays can be useful for alerting to detect changes in slightly shifted trend lines. Since: 1.6 Before After Combined name,requestsPerSecond, :eq , :sum name,requestsPerSecond, :eq , :sum , 5, :delay name,requestsPerSecond, :eq , :sum , :dup , 5, :delay","title":"delay"},{"location":"asl/ref/depth/","text":"Input Stack: ... \u21e8 Output Stack: Int ... Push the depth of the stack. Since: 1.5.0 Examples: , :depth Pos Input Output 0 0 a, :depth Pos Input Output 0 a 1 1 a a,b, :depth Pos Input Output 0 b 2 1 a b 2 a","title":"depth"},{"location":"asl/ref/derivative/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Opposite of :integral . Computes the rate of change per step of the input time series. Derivative Integral Integral Then Derivative 1, :derivative 1, :integral 1, :integral , :derivative","title":"derivitive"},{"location":"asl/ref/des-epic-signal/","text":"Input Stack: noise: Double minPercent: Double maxPercent: Double beta: Double alpha: Double training: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for configuring DES in a manner compatible with legacy epic alerts. For more information see the epic macros section of the DES page. Before After name,sps, :eq , :sum name,sps, :eq , :sum , 10,0.1,0.5,0.2,0.2,4, :des-epic-signal","title":"des-epic-signal"},{"location":"asl/ref/des-epic-viz/","text":"Input Stack: noise: Double minPercent: Double maxPercent: Double beta: Double alpha: Double training: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for configuring DES in a manner compatible with legacy Epic alerts. For more information see the epic macros section of the DES page. Example name,sps, :eq , :sum , 10,0.1,0.5,0.2,0.2,4, :des-epic-viz","title":"des-epic-viz"},{"location":"asl/ref/des-fast/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing DES using settings to quickly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-fast should be used instead. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :des-fast","title":"des-fast"},{"location":"asl/ref/des-simple/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing DES using default values. Warning The values used by this operation are prone to wild oscillations. See recommended values for better options. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :des-simple","title":"des-simple"},{"location":"asl/ref/des-slow/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing DES using settings to slowly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-slow should be used instead. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :des-slow","title":"des-slow"},{"location":"asl/ref/des-slower/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing DES using settings to slowly adjust to the input line. See recommended values for more information. For most use-cases the sliding DES variant :sdes-slower should be used instead. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :des-slower","title":"des-slower"},{"location":"asl/ref/des/","text":"Input Stack: beta: Double alpha: Double training: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Double exponential smoothing . For most use-cases sliding DES should be used instead to ensure a deterministic prediction. Before After name,requestsPerSecond, :eq , :sum name,requestsPerSecond, :eq , :sum , 5,0.1,0.5, :des","title":"des"},{"location":"asl/ref/dist-avg/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr Compute the average recorded value for timers and distribution summaries . This is calculated by dividing the total amount recorded by the number of recorded values. For [Timer] and Distribution Summary metrics, the totalTime (timers) / totalAmount (distributions) and count are collected each time a measurement is taken. If this technique was applied to a request latency metric, then you would have the average latency per request for an arbitrary grouping. These types of metrics have an explicit count based on activity. To get an average per measurement manually: statistic,totalTime, :eq , :sum , statistic,count, :eq , :sum , :div This expression can be bound to a query using the :cq (common query) operator: statistic,totalTime, :eq , :sum , statistic,count, :eq , :sum , :div , nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :cq Using the :dist-avg function reduces the query to: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-avg To compute the average by group, apply the group after the :dist-avg function: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-avg , (,nf.asg,), :by Before After name,playback.startLatency, :eq name,playback.startLatency, :eq , :dist-avg","title":"dist-avg"},{"location":"asl/ref/dist-max/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr Compute the maximum recorded value for timers and distribution summaries . This is a helper for aggregating by the max of the max statistic for the meter. A manual query would look like: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , statistic,max, :eq , :and , :max Using :dist-max the query is reduced to: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-max Before After name,playback.startLatency, :eq name,playback.startLatency, :eq , :dist-max","title":"dist-max"},{"location":"asl/ref/dist-stddev/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr Compute the standard deviation for timers and distribution summaries . A manual query would look like: statistic,count, :eq , :sum , statistic,totalOfSquares, :eq , :sum , :mul , statistic,totalTime, :eq , :sum , :dup , :mul , :sub , statistic,count, :eq , :sum , :dup , :mul , :div , :sqrt , nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :cq This is much simpler using the :dist-stddev function: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-stddev Before After name,playback.startLatency, :eq name,playback.startLatency, :eq , :dist-stddev","title":"dist-stddev"},{"location":"asl/ref/div/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 / ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a / b) where a and b are the corresponding intervals in the input time series. If a and b are 0, then 0 will be returned for the interval. If only b is 0, then NaN will be returned as the value for the interval. Sample data: :div 0.5 0.0 NaN NaN NaN Input 1 1.0 0.0 1.0 1.0 NaN Input 2 2.0 0.0 0.0 NaN NaN Use the fdiv operator to get strict floating point behavior. Example dividing a constant: Before After name,sps, :eq , 42 name,sps, :eq , 42, :div Example adding two series: Before After name,sps, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,sps, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :div","title":"div"},{"location":"asl/ref/drop/","text":"Input Stack: a \u21e8 Output Stack: Remove the item on the top of the stack. Example: a,b,c, :drop Pos Input Output 0 c b 1 b a 2 a :drop Warning Throws an exception due to an empty stack.","title":"drop"},{"location":"asl/ref/dup/","text":"Input Stack: a: ? \u21e8 Output Stack: a: ? a: ? Duplates the item on the top of the stack. Example: Before After minuteOfDay, :time minuteOfDay, :time , :dup","title":"dup"},{"location":"asl/ref/each/","text":"Input Stack: function: List items: List \u21e8 Output Stack: function(items[N-1]) ... function(items[0]) Pops a list off the stack and executes it as a program. Example: (,a,b,),(, :dup , ), :each Pos Input Output 0 List(:dup) a 1 List(a, b) a 2 b 3 b","title":"each"},{"location":"asl/ref/eq/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k == v): Query Select time series that have a specified value for a key. For example, consider the following query: name,ssCpuUser, :eq When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"eq"},{"location":"asl/ref/eureka-avg/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr A helper to compute an average using the number of instances in the UP state based on the discovery.status metric as the denominator. The common infrastructure tags will be used to restrict the scope for the denominator. This operator should be used if the numerator is based on incoming traffic that is routed via the Eureka service and goal is to compute an average per node receiving traffic. name,sps, :eq , nf.app,nccp, :eq , :and , :eureka-avg","title":"eureka-avg"},{"location":"asl/ref/fadd/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 + ts2): TimeSeriesExpr Floating point addition operator. Compute a new time series where each interval has the value (a + b) where a and b are the corresponding intervals in the input time series. :fadd 3.0 0.0 1.0 NaN NaN Input 1 2.0 0.0 1.0 1.0 NaN Input 2 1.0 0.0 0.0 NaN NaN Note in many cases NaN will appear in data, e.g., if a node was brought up and started reporting in the middle of the time window for the graph. This can lead to confusing behavior if added to a line that does have data as the result will be NaN . Use the add operator to treat NaN values as zero for combining with other time series. Example adding a constant: Before After name,sps, :eq , 30e3 name,sps, :eq , 30e3, :fadd Example adding two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :fadd","title":"fadd"},{"location":"asl/ref/false/","text":"Input Stack: \u21e8 Output Stack: Query Query expression that will not match any input time series. See also :true .","title":"false"},{"location":"asl/ref/fcall/","text":"Input Stack: String ... \u21e8 Output Stack: ? Shorthand equivalent to writing: :get,:call Example: duplicate,(, :dup , ), :set , a,duplicate, :fcall Pos Input Output 0 duplicate a 1 a a","title":"fcall"},{"location":"asl/ref/fdiv/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 / ts2): TimeSeriesExpr Floating point division operator. Compute a new time series where each interval has the value (a / b) where a and b are the corresponding intervals in the input time series. :fdiv 2.0 NaN Inf NaN NaN Input 1 2.0 0.0 1.0 1.0 NaN Input 2 1.0 0.0 0.0 NaN NaN Note in many cases NaN will appear in data, e.g., if a node was brought up and started reporting in the middle of the time window for the graph. Zero divided by zero can also occur due to lack of activity in some windows. Unless you really need strict floating point behavior, use the div operator to get behavior more appropriate for graphs. Example dividing a constant: Before After name,sps, :eq name,sps, :eq , 1024, :fdiv Example dividing two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :fdiv","title":"fdiv"},{"location":"asl/ref/filter/","text":"Input Stack: TimeSeriesExpr TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Filters the results of a grouped expression by another expression. The filter expression is a set of signal time series indicating if the corresponding time series from the original expression should be shown. Simple example that suppresses all lines: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , 0, :filter Filtering is most commonly performed using the summary statistics for the original expression. For example, to show only the lines that have an average value across the query window greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :dup , avg, :stat , 5e3, :gt , :over , avg, :stat , 20e3, :lt , :and , :filter There are helpers, :stat-$(name) , to express this common pattern more easily for filters. They act as place holders for the specified statistic on the input time series. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. See the :stat operator for more details on available statistics. For this example, :stat-avg would be used: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-avg , 5e3, :gt , :stat-avg , 20e3, :lt , :and , :filter","title":"filter"},{"location":"asl/ref/fmul/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 * ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a * b) where a and b are the corresponding intervals in the input time series. Example multiplying a constant: Before After name,sps, :eq name,sps, :eq , 1024, :fmul Example multiplying two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :fmul","title":"fmul"},{"location":"asl/ref/format/","text":"Input Stack: args: List pattern: String \u21e8 Output Stack: str: String Format a string using a printf style pattern . Example: foo%s,(,bar,), :format Pos Input Output 0 List(bar) foobar 1 foo%s","title":"format"},{"location":"asl/ref/freeze/","text":"Input Stack: ... \u21e8 Output Stack: Freeze removes all data from the stack and pushes it to a separate frozen stack that cannot be modified other than to push additional items using the freeze operation. The final stack at the end of the execution will include the frozen contents along with any thing that is on the normal stack. This operation is useful for isolating common parts of the stack while still allowing tooling to manipulate the main stack using concatenative rewrite operations. The most common example of this is the :cq operation used to apply a common query to graph expressions. For a concrete example, suppose you want to have an overlay expression showing network errors on a switch that you want to add in to graphs on a dashboard. The dashboard allows drilling into the graphs by selecting a particular cluster. To make this work the dashboard appends a query rewrite to the expression like: ,:list,(,nf.cluster,{{ selected_cluster }},:eq,:cq,),:each This :list operator will apply to everything on the stack. However, this is problematic because the cluster restriction will break the overlay query. Using the freeze operator the overlay expression can be isolated from the main stack. So the final expression would look something like: # Query that should be used as is and not modified further name,networkErrors,:eq,:sum,50,:gt,:vspan,40,:alpha, :freeze, # Normal contents of the stack name,ssCpuUser,:eq,:avg,1,:axis, name,loadavg1,:eq,:avg,2,:axis, # Rewrite appended by tooling, only applies to main stack :list,(,nf.cluster,{{ selected_cluster }},:eq,:cq,),:each Since: 1.6 Example: a,b,c, :freeze Pos Input Output 0 c c 1 b b 2 a a","title":"freeze"},{"location":"asl/ref/fsub/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 - ts2): TimeSeriesExpr Floating point subtraction operator. Compute a new time series where each interval has the value (a - b) where a and b are the corresponding intervals in the input time series. :fsub 1.0 0.0 1.0 NaN NaN Input 1 2.0 0.0 1.0 1.0 NaN Input 2 1.0 0.0 0.0 NaN NaN Note in many cases NaN will appear in data, e.g., if a node was brought up and started reporting in the middle of the time window for the graph. This can lead to confusing behavior if added to a line that does have data as the result will be NaN . Use the sub operator to treat NaN values as zero for combining with other time series. Example subtracting a constant: Before After name,sps, :eq name,sps, :eq , 30000, :fsub Example subtracting two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :fsub","title":"fsub"},{"location":"asl/ref/ge/","text":"Greater than or equal operator. There are two variants of the :ge operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k >= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :ge When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 >= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a >= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a >= b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Info Note, the data points have floating point values. It is advisable to avoid relying on an exact equality match. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :ge","title":"ge"},{"location":"asl/ref/ge/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k >= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :ge When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/ge/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 >= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a >= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a >= b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Info Note, the data points have floating point values. It is advisable to avoid relying on an exact equality match. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :ge","title":"Math"},{"location":"asl/ref/get/","text":"Input Stack: k \u21e8 Output Stack: vars[k] Get the value of a variable and push it on the stack. Example: k,v, :set , k, :get Pos Input Output 0 k v","title":"get"},{"location":"asl/ref/gt/","text":"Greater than operator. There are two variants of the :gt operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k > v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than a specified value. For example, consider the following query: name,ssCpuSystem, :gt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 > ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a > b) where a and b are the corresponding intervals in the input time series. For example: Time a b a > b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :gt","title":"gt"},{"location":"asl/ref/gt/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k > v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than a specified value. For example, consider the following query: name,ssCpuSystem, :gt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/gt/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 > ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a > b) where a and b are the corresponding intervals in the input time series. For example: Time a b a > b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :gt","title":"Math"},{"location":"asl/ref/has/","text":"Input Stack: k: String \u21e8 Output Stack: Query Select time series that have a specified key. For example, consider the following query: nf.node, :has When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp ssCpuUser api i-0456","title":"has"},{"location":"asl/ref/head/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Shorthand equivalent to writing: :limit Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , 2, :head","title":"head"},{"location":"asl/ref/in/","text":"Input Stack: vs: List[String] k: String \u21e8 Output Stack: (k in vs): Query Select time series where the value for a key is in the specified set. For example, consider the following query: name,(,ssCpuUser,ssCpuSystem,), :in When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"in"},{"location":"asl/ref/integral/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Sum the values across the evaluation context. This is typically used to approximate the distinct number of events that occurred. If the input is non-negative, then each datapoint for the output line will represent the area under the input line from the start of the graph to the time for that datapoint. Missing values, NaN , will be treated as zeroes. For example: Input :integral 0 0 1 1 -1 0 NaN 0 0 0 1 1 2 3 1 4 1 5 0 5 For a counter , each data point represents the average rate per second over the step interval. To compute the total amount incremented, the value first needs to be converted to a rate per step interval. This conversion can be performed using the :per-step operation. Examples: Before After 1 1, :integral Before After name,requestsPerSecond, :eq , :sum , :per-step name,requestsPerSecond, :eq , :sum , :per-step , :integral","title":"integral"},{"location":"asl/ref/le/","text":"Less than or equal operator. There are two variants of the :le operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k <= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :le When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 <= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a <= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a <= b 00:01 0.0 0.0 1.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :le","title":"le"},{"location":"asl/ref/le/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k <= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :le When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/le/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 <= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a <= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a <= b 00:01 0.0 0.0 1.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :le","title":"Math"},{"location":"asl/ref/legend/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Set the legend text. Legends can contain variables based on the exact keys matched in the query clause and keys used in a group by . Variables start with a $ sign and can optionally be enclosed between parentheses. The parentheses are required for cases where the characters immediately following the name could be a part of the name. If a variable is not defined, then the name of the variable will be used as the substitution value. The variable atlas.offset can be used to indicate the time shift used for the underlying data. Examples: Before After name,sps, :eq , (,name,), :by name,sps, :eq , (,name,), :by , $name, :legend Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , cluster+$nf.cluster, :legend","title":"legend"},{"location":"asl/ref/limit/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: StyleExpr Restrict the output to the first N lines from the input expression. The lines will be chosen in order based on the sort and order used. Example: After After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , 3, :limit","title":"limit"},{"location":"asl/ref/line/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: StyleExpr Change the line style to be line. This is the default mode and usually does not need to be set explicitly. See the line style examples page for more information. Example: Default name,sps, :eq , :sum , :line","title":"line"},{"location":"asl/ref/list/","text":"Input Stack: ... \u21e8 Output Stack: List[?] Pop all items off the stack and push them as a list. Example: a,b, :list Pos Input Output 0 b List(b, a) 1 a , :list Pos Input Output 0 List()","title":"list"},{"location":"asl/ref/ls/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Set the line style. The value should be one of: line : this is the default, draws a normal line. area : fill in the space between the line value and 0 on the Y-axis. stack : stack the filled area on to the previous stacked lines on the same axis. vspan : non-zero datapoints will be drawn as a vertical span. See the line style examples page for more information. Example: Line Area name,sps, :eq , :sum , (,name,), :by , line, :ls name,sps, :eq , :sum , (,name,), :by , area, :ls Stack VSpan name,sps, :eq , :sum , (,nf.cluster,), :by , stack, :ls name,sps, :eq , :sum , (,name,), :by , 200e3, :gt , vspan, :ls","title":"ls"},{"location":"asl/ref/lt/","text":"Less than operator. There are two variants of the :lt operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k < v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than a specified value. For example, consider the following query: name,ssCpuSystem, :lt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 < ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a < b) where a and b are the corresponding intervals in the input time series. For example: Time a b a < b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :lt","title":"lt"},{"location":"asl/ref/lt/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k < v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than a specified value. For example, consider the following query: name,ssCpuSystem, :lt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/lt/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 < ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a < b) where a and b are the corresponding intervals in the input time series. For example: Time a b a < b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :lt","title":"Math"},{"location":"asl/ref/lw/","text":"Input Stack: Int TimeSeriesExpr \u21e8 Output Stack: StyleExpr The width of the stroke used when drawing the line. Example: Before After name,sps, :eq , :sum , (,name,), :by name,sps, :eq , :sum , (,name,), :by , 2, :lw","title":"lw"},{"location":"asl/ref/map/","text":"Input Stack: function: List items: List \u21e8 Output Stack: List(function(items[0], ..., items[N-1]) Create a new list by applying a function to all elements of a list. Example: (,a%s,b%s,),(,(,.netflix.com,), :format , ), :map Pos Input Output 0 List((, .netflix.com, ), :format) List(a.netflix.com, b.netflix.com) 1 List(a%s, b%s)","title":"map"},{"location":"asl/ref/max/","text":"Max aggregation operator. There are two variants of the :max operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the maximum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :max When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [8.0, 7.0, 6.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the maximum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :max","title":"max"},{"location":"asl/ref/max/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the maximum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :max When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [8.0, 7.0, 6.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/max/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the maximum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :max","title":"Math"},{"location":"asl/ref/median/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr Shorthand equivalent to writing: (,50,),:percentiles Before After name,requestLatency, :eq name,requestLatency, :eq , :median","title":"median"},{"location":"asl/ref/min/","text":"Min aggregation operator. There are two variants of the :min operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the minimum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :min When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [1.0, 2.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the minimum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :min","title":"min"},{"location":"asl/ref/min/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the minimum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :min When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [1.0, 2.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/min/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the minimum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :min","title":"Math"},{"location":"asl/ref/mul/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 * ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a * b) where a and b are the corresponding intervals in the input time series. NaN s in a series when other series are present are treated as 1 . Example multiplying a constant: Before After name,sps, :eq name,sps, :eq , 1024, :mul Example multiplying two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :mul","title":"mul"},{"location":"asl/ref/named-rewrite/","text":"Input Stack: name: String rewritten: TimeSeriesExpr original: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Internal operation used by some macros to provide a more user friendly display expression. The expanded version will get used for evaluation, but if a new expression is generated from the parsed expression tree it will use the original version along with the named of the macro. Before After name,ssCpuUser, :eq , :dup , :dup , :sum , :swap , :count , :div name,ssCpuUser, :eq , :dup , :dup , :sum , :swap , :count , :div , avg, :named-rewrite","title":"named-rewrite"},{"location":"asl/ref/ndrop/","text":"Input Stack: N a0 ... aN \u21e8 Output Stack: aN Remove the top N items on the stack. Example: a,0, :ndrop Pos Input Output 0 0 a 1 a a,b,c,2, :ndrop Pos Input Output 0 2 a 1 c 2 b 3 a a,b,c,4, :ndrop Pos Input Output 0 4 1 c 2 b 3 a , :ndrop Warning Throws an exception due to missing the N param.","title":"ndrop"},{"location":"asl/ref/neg/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute a new time series where each interval has the negated value of the input time series. Example: 0 64 -64 0, :neg 64, :neg -64, :neg","title":"neg"},{"location":"asl/ref/nip/","text":"Input Stack: ? \u21e8 Output Stack: ? Shorthand equivalent to writing: :swap,:drop Example: a,b, :nip Pos Input Output 0 b b 1 a","title":"nip"},{"location":"asl/ref/nlist/","text":"Input Stack: N a0 ... aN \u21e8 Output Stack: List(aN-1, ..., a0) aN Create a list with the top N items on the stack. Since: 1.5.0 Examples: a,0, :nlist Pos Input Output 0 0 List() 1 a a a,b,c,2, :nlist Pos Input Output 0 2 List(b, c) 1 c a 2 b 3 a a,b,c,4, :nlist Pos Input Output 0 4 List(a, b, c) 1 c 2 b 3 a","title":"nlist"},{"location":"asl/ref/node-avg/","text":"Input Stack: Query \u21e8 Output Stack: TimeSeriesExpr A helper to compute an average using the poller.asg.instance metric as the denominator. The common infrastructure tags will be used to restrict the scope for the denominator. This operator should be used instead of :avg if the goal is to compute an average per node. name,sps, :eq , nf.app,nccp, :eq , :and , :node-avg","title":"node-avg"},{"location":"asl/ref/not/","text":"Input Stack: q: Query \u21e8 Output Stack: (!q): Query Select time series that have a specified key. For example, consider the following query: nf.node, :has , :not When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp ssCpuUser api i-0456","title":"not"},{"location":"asl/ref/offset/","text":"Input Stack: TimeSeriesExpr Duration \u21e8 Output Stack: TimeSeriesExpr Shift the time frame to use when fetching the data. This is used to look at a previous interval as a point of reference, e.g., day-over-day or week-over-week. Examples: Before After Combined name,sps, :eq , (,name,), :by name,sps, :eq , (,name,), :by , 1w, :offset name,sps, :eq , (,name,), :by , :dup , 1w, :offset Before After Combined name,sps, :eq , (,name,), :by name,sps, :eq , (,name,), :by , PT1H, :offset name,sps, :eq , (,name,), :by , :dup , PT1H, :offset","title":"offset"},{"location":"asl/ref/or/","text":"There are two variants of the :or operator. Choosing \u00b6 Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 OR q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if either of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :or When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 OR ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a OR b) where a and b are the corresponding intervals in the input time series. For example: Time a b a OR b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a or b are non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt , :or","title":"or"},{"location":"asl/ref/or/#choosing","text":"Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 OR q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if either of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :or When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/or/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 OR ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a OR b) where a and b are the corresponding intervals in the input time series. For example: Time a b a OR b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a or b are non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt , :or","title":"Math"},{"location":"asl/ref/order/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Order to use for sorting results. Supported values are asc and desc for ascending and descending order respectively. Default is asc . Since: 1.5 Examples: Sorted Default name,sps, :eq , :sum , (,nf.cluster,), :by , max, :sort , asc, :order name,sps, :eq , :sum , (,nf.cluster,), :by , desc, :order","title":"order"},{"location":"asl/ref/over/","text":"Input Stack: b a \u21e8 Output Stack: a b a Copy the item in the second position on the stack to the top. Example: a,b, :over Pos Input Output 0 b a 1 a b 2 a","title":"over"},{"location":"asl/ref/palette/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Set the palette to use for the results of an expression. This operator is allows for scoping a palette to a particular group by instead of to all lines that share the same axis. A common use-case is to have multiple stacked group by expressions using different palettes. For example, suppose I want to create a graph showing overall request per second hitting my services with successful requests shown in shades of green and errors in shades of red . This can make it easy to visually see if a change is due to an increase in errors: Or a spike in successful requests: Examples: Before After name,sps, :eq , :sum name,sps, :eq , :sum , reds, :palette Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , reds, :palette","title":"palette"},{"location":"asl/ref/pct/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Shorthand equivalent to writing: :dup,:dup,:sum,:div,100,:mul,pct,:named-rewrite The percent contribution of an individual time series to a group. Example: name,sps, :eq , (,nf.cluster,), :by , :pct Before After Stack to 100% name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :pct name,sps, :eq , (,nf.cluster,), :by , :pct , :stack","title":"pct"},{"location":"asl/ref/per-step/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Converts a line from a rate per second to a rate based on the step size of the graph. This is useful for getting an estimate of the raw number of events for a given interval. 0 64 -64 0, :per-step 64, :per-step -64, :per-step","title":"per-step"},{"location":"asl/ref/percentiles/","text":"Input Stack: percentiles: List Query \u21e8 Output Stack: TimeSeriesExpr Estimate percentiles for a timer or distribution summary. The data must have been published appropriately to allow the approximation. If using spectator , then see PercentileTimer and PercentileDistributionSummary helper classes. The percentile values can be shown in the legend using $percentile . Since: 1.5.0 (first in 1.5.0-rc.4) Before After name,requestLatency, :eq name,requestLatency, :eq , (,25,50,90,), :percentiles","title":"percentiles"},{"location":"asl/ref/pick/","text":"Input Stack: N a0 ... aN \u21e8 Output Stack: aN-1 a0 ... aN Pick an item in the stack and put a copy on the top. Since: 1.5.0 Example: a,0, :pick Pos Input Output 0 0 a 1 a a a,b,0, :pick Pos Input Output 0 0 b 1 b b 2 a a a,b,1, :pick Pos Input Output 0 1 a 1 b b 2 a a","title":"pick"},{"location":"asl/ref/pow/","text":"Input Stack: TimeSeriesExpr TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute a new time series where each interval has the value (a power b) where a and b are the corresponding intervals in the input time series. Examples: Before After name,sps, :eq name,sps, :eq , 42, :pow Before After name,sps, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,sps, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :pow","title":"pow"},{"location":"asl/ref/random/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Generate a time series that appears to be random noise for the purposes of experimentation and generating sample data. To ensure that the line is deterministic and reproducible it actually is based on a hash of the timestamp. Each datapoint is a value between 0.0 and 1.0. Random :random","title":"random"},{"location":"asl/ref/re/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k=~/^v/): Query Warning Regular expressions can be expensive to check and should be avoided if possible. When designing data to publish ensure that common query patterns would not need the use of regular expressions. Select time series where the value for a key matches the specified regular expression. For example, consider the following query: name,ssCpu, :re When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpu User alerttest i-0123 ssCpu System alerttest i-0123 ssCpu User nccp i-0abc ssCpu System nccp i-0abc numRequests nccp i-0abc ssCpu User api i-0456 The regular expression value will be automatically anchored at the start and the matching is case sensitive. Always try to have a simple prefix on the expression to allow for more efficient matching of the expression. For more information on supported patterns, see the Java regular expressions documentation.","title":"re"},{"location":"asl/ref/reic/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k=~/^v/i): Query Warning Ignoring the case will always result if a full scan for the key. This should be used sparingly and only for tag queries. If a case-insensitive match is not required, use :re intead. Select time series where the value for a key matches the specified regular expression with case insenitive matching. For example, consider the following query: name,ssCPU, :reic When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpu User alerttest i-0123 ssCpu System alerttest i-0123 ssCpu User nccp i-0abc ssCpu System nccp i-0abc numRequests nccp i-0abc ssCpu User api i-0456 Notice that the casing for the query does not match the data. The regular expression value will be automatically anchored at the start. For more information on supported patterns, see the Java regular expressions documentation.","title":"reic"},{"location":"asl/ref/roll/","text":"Input Stack: N a0 ... aN \u21e8 Output Stack: aN-1 a0 ... aN-2 aN Rotate an item in the stack and put it on the top. Since: 1.5.0 Example: a,0, :roll Pos Input Output 0 0 a 1 a a,b,0, :roll Pos Input Output 0 0 b 1 b a 2 a a,b,1, :roll Pos Input Output 0 1 a 1 b b 2 a","title":"roll"},{"location":"asl/ref/rolling-count/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Number of occurrences within a specified window. This operation is frequently used in alerting expressions to reduce noise. For example: # Check to see if average cpu usage is > 80% name,cpuUser,:eq,:avg,80,:gt, # Only alert if that is true for more than 3 of the last 5 # datapoints 5,:rolling-count,3,:gt A value is counted if it is non-zero. Missing values, NaN , will be treated as zeroes. For example: Input 3,:rolling-count 0 0 1 1 -1 2 NaN 2 0 1 1 1 1 2 1 3 1 3 0 2 The window size, n , is the number of datapoints to consider including the current value. Note that it is based on datapoints not a specific amount of time. As a result the number of occurrences will be reduced when transitioning to a larger time frame that causes consolidation. Before After :random , 0.4, :gt :random , 0.4, :gt , 5, :rolling-count","title":"rolling-count"},{"location":"asl/ref/rolling-max/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Maximum value within a specified window. This operation can be used in alerting expressions to find a lower bound for noisy data based on recent samples. For example: name,sps,:eq,:sum, :dup, 5,:rolling-max Missing values, NaN , will be ignored when computing the min. If all values within the window are NaN , then NaN will be emitted. For example: Input 3,:rolling-max 0 0 1 1 -1 1 NaN 1 0 0 1 1 1 1 1 1 1 1 0 1 The window size, n , is the number of datapoints to consider including the current value. Note that it is based on datapoints not a specific amount of time. As a result the number of occurrences will be reduced when transitioning to a larger time frame that causes consolidation. Since: 1.6 Before After :random , 0.4, :gt :random , 0.4, :gt , 5, :rolling-max","title":"rolling-max"},{"location":"asl/ref/rolling-mean/","text":"Input Stack: minNumValues: Int n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Mean of the values within a specified window. The mean will only be emitted if there are at least a minimum number of actual values (not NaN ) within the window. Otherwise NaN will be emitted for that time period. Input 3,2,:rolling-mean 0 NaN 1 0.5 -1 0.0 NaN 0.0 NaN NaN 0 NaN 1 0.5 1 0.667 1 1 0 0.667 The window size, n , is the number of datapoints to consider including the current value. There must be at least minNumValues non-NaN values within that window before it will emit a mean. Note that it is based on datapoints, not a specific amount of time. As a result the number of occurrences will be reduced when transitioning to a larger time frame that causes consolidation. Since: 1.6 Before After name,sps, :eq , :sum name,sps, :eq , :sum , 5,3, :rolling-mean","title":"rolling-mean"},{"location":"asl/ref/rolling-min/","text":"Input Stack: n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Minimum value within a specified window. This operation can be used in alerting expressions to find a lower bound for noisy data based on recent samples. For example: name,sps,:eq,:sum, :dup, 5,:rolling-min Missing values, NaN , will be ignored when computing the min. If all values within the window are NaN , then NaN will be emitted. For example: Input 3,:rolling-min 0 0 1 0 -1 -1 NaN -1 0 -1 1 0 1 0 1 1 1 1 0 0 The window size, n , is the number of datapoints to consider including the current value. Note that it is based on datapoints not a specific amount of time. As a result the number of occurrences will be reduced when transitioning to a larger time frame that causes consolidation. Since: 1.6 Before After name,sps, :eq , :sum name,sps, :eq , :sum , 5, :rolling-min","title":"rolling-min"},{"location":"asl/ref/rolling-sum/","text":"Input Stack: minNumValues: Int n: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Sum of the values within a specified window. Input 3,:rolling-sum 0 0.0 1 1.0 -1 0.0 NaN 0.0 NaN -1.0 NaN NaN 1 1.0 1 2.0 1 3.0 0 2.0 The window size, n , is the number of datapoints to consider including the current value. Note that it is based on datapoints, not a specific amount of time. As a result the number of occurrences will be reduced when transitioning to a larger time frame that causes consolidation. Since: 1.6 Before After name,sps, :eq , :sum name,sps, :eq , :sum , 5, :rolling-sum","title":"rolling-sum"},{"location":"asl/ref/rot/","text":"Input Stack: b ... a \u21e8 Output Stack: a b ... Rotate the stack so that the item at the bottom is now at the top. Example: a,b,c,d, :rot Pos Input Output 0 d a 1 c d 2 b c 3 a b","title":"rot"},{"location":"asl/ref/s/","text":"Input Stack: replacement: String searchPattern: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Perform a search and replace on the legend strings. This command is similar to the global search and replace ( s/regexp/replace/g ) operation from tools like vim or sed . The replacement string can use variables to refer to the capture groups of the input expression. The syntax is that same as for legends . Since: 1.6 Examples: Before After name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend , ^nccp-(.*)$,$1, :s Before After name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend , ^nccp-(? .*)$,$stack, :s Before After name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend , nccp-,_, :s Before After name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend name,sps, :eq , (,nf.cluster,), :by , $nf.cluster, :legend , ([a-z]),_$1, :s","title":"s"},{"location":"asl/ref/sdes-fast/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing sliding DES using settings to quickly adjust to the input line. See recommended values for more information. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :sdes-fast","title":"sdes-fast"},{"location":"asl/ref/sdes-simple/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Shorthand equivalent to writing: :dup,10,0.1,0.5,:sdes,sdes-simple,:named-rewrite Before After name,sps, :eq , :sum name,sps, :eq , :sum , :sdes-simple","title":"sdes-simple"},{"location":"asl/ref/sdes-slow/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing sliding DES using settings to slowly adjust to the input line. See recommended values for more information. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :sdes-slow","title":"sdes-slow"},{"location":"asl/ref/sdes-slower/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Helper for computing sliding DES using settings to slowly adjust to the input line. See recommended values for more information. Before After name,sps, :eq , :sum name,sps, :eq , :sum , :sdes-slower","title":"sdes-slower"},{"location":"asl/ref/sdes/","text":"Input Stack: beta: Double alpha: Double training: Int TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Variant of :des that is deterministic as long as the step size does not change. One of the common complaints with DES is that to get the same value for a given time you must start feeding in data at exactly the same time. So for normal graphs where it is computed using the window of the chart it will have slightly different predictions for a given time. As it is often used for alerting this makes it cumbersome to try and determine: Why an alarm fired When alarms would have fired for tuning Sliding DES uses two DES functions and alternates between them. One will get trained while the other is getting used, and then the one that was getting used will get reset and the roles swapped. F1 | A |-- T1 --|-- P1 --|-- T1 --|-- P1 --|-- T1 --| F2 | A | |-- T2 --|-- P2 --|-- T2 --|-- P2 --| Result: R |-- NaN -----|-- P1 --|-- P2 --|-- P1 --|-- P2 --| Both functions will ignore any data until it reaches a boundary, even multiple, of the training window. That is shown as A in the diagram above. The first function will then start training, T1 , and after the training window the first predicted values, P1 , will get generated. The ouput line will alternate between the predictions from both DES functions. The alternation between functions can cause the prediction line to look choppier than DES, e.g., on a gradual drop: Further, since each prediction only considers data for a narrow window it will adjust to sharp changes faster. For example: Since: 1.5.0 Before After name,requestsPerSecond, :eq , :sum , :per-step name,requestsPerSecond, :eq , :sum , 5,0.1,0.5, :sdes","title":"sdes"},{"location":"asl/ref/set/","text":"Input Stack: v k \u21e8 Output Stack: Set the value of a variable. Example: k,v, :set Pos Input Output 0 v 1 k","title":"set"},{"location":"asl/ref/sort/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: StyleExpr Sort the results of an expression in the legend by one of the summary statistics or by the legend text. The default behavior is to sort by the legend text. This will sort in ascending order by default, for descending order use order . Since: 1.5 Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , max, :sort","title":"sort"},{"location":"asl/ref/sqrt/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute a new time series where each interval has the square root of the value from the input time series. 0 64 -64 0, :sqrt 64, :sqrt -64, :sqrt","title":"sqrt"},{"location":"asl/ref/srandom/","text":"Input Stack: seed: Int \u21e8 Output Stack: TimeSeriesExpr Generate a time series that appears to be random noise for the purposes of experimentation and generating sample data. To ensure that the line is deterministic and reproducible it actually is based on a hash of the timestamp. The seed value is used to vary the values for the purposes of creating multiple different sample lines. Each datapoint is a value between 0.0 and 1.0. Example: Seeded Random: /api/v1/graph?w=200&h=125&s=e-3h&e=2012-01-01T07:00&tz=UTC&q=42,:srandom @@@","title":"srandom"},{"location":"asl/ref/sset/","text":"Input Stack: k v \u21e8 Output Stack: Shorthand equivalent to writing: :swap,:set Example: a,b, :sset Pos Input Output 0 b 1 a","title":"sset"},{"location":"asl/ref/stack/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: StyleExpr Change the line style to be stack. In this mode the line will be filled to the previous stacked line on the same axis. See the line style examples page for more information. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stack","title":"stack"},{"location":"asl/ref/stat-avg-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to avg,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-avg-mf","title":"stat-avg-mf"},{"location":"asl/ref/stat-avg/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the avg,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have an average value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-avg , 5e3, :gt , :stat-avg , 20e3, :lt , :and , :filter","title":"stat-avg"},{"location":"asl/ref/stat-count/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the count,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the count value is greater than 50: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-count , 50, :gt , :filter","title":"stat-count"},{"location":"asl/ref/stat-last/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the last,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the last value is greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-last , 5e3, :gt , :stat-last , 20e3, :lt , :and , :filter","title":"stat-last"},{"location":"asl/ref/stat-max-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to max,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-max-mf","title":"stat-max-mf"},{"location":"asl/ref/stat-max/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the max,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have a maximum value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-max , 5e3, :gt , :stat-max , 20e3, :lt , :and , :filter","title":"stat-max"},{"location":"asl/ref/stat-min-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to min,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-min-mf","title":"stat-min-mf"},{"location":"asl/ref/stat-min/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the min,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have a minimum value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-min , 5e3, :gt , :stat-min , 20e3, :lt , :and , :filter","title":"stat-min"},{"location":"asl/ref/stat-total/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the total,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the sum of all data points for the line is greater than 1M and less than 4M: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-total , 1e6, :gt , :stat-total , 4e6, :lt , :and , :filter","title":"stat-total"},{"location":"asl/ref/stat/","text":"Input Stack: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Create a summary time series showing the value of the specified summary statistic for the data points of the input time series. Valid statistic values are avg , count , max , min , last , and total . The graph below shows avg , max , min , and last for a simple input time series: The count is the number of data points for the time series. In the example above, that is five since the last value is NaN . The total is the sum of the data points for the time series. The most common usage of stats is in conjunction with :filter to restrict the set of results for grouped expression. When filtering, helper macros, :stat-$(name) , can be used to represent applying the statistic to the input time series being filtered without explicitly repeating the input expression. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , avg, :stat","title":"stat"},{"location":"asl/ref/stddev/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the standard deviation for the results of a group by . If the underlying data is for a timer or distribution summary, then dist-stddev is likely a better choice. Since: 1.6 Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stddev","title":"stddev"},{"location":"asl/ref/sub/","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 - ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a subtractNaN b) where a and b are the corresponding intervals in the input time series. :sub 1.0 0.0 1.0 1.0 NaN Input 1 2.0 0.0 1.0 1.0 NaN Input 2 1.0 0.0 0.0 NaN NaN Use the fsub operator to get strict floating point behavior. Example subtracting a constant: Before After name,sps, :eq name,sps, :eq , 30e3, :sub Example subtracting two series: Before After name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by name,requestLatency, :eq , :sum , name,requestsPerSecond, :eq , :max , (,name,), :by , :sub","title":"sub"},{"location":"asl/ref/sum/","text":"Sum aggregation operator. There are two variants of the :sum operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the sum of all the time series that match the query. Sum is the default aggregate used if a query is specified with no explicit aggregate function. Example with implicit sum: name,ssCpuUser, :eq Equivalent example with explicit sum: name,ssCpuUser, :eq , :sum When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the sum. This leads to a final result of: Name Data ssCpuUser [10.0, 11.0, 8.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the sum of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :max , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :sum","title":"sum"},{"location":"asl/ref/sum/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the sum of all the time series that match the query. Sum is the default aggregate used if a query is specified with no explicit aggregate function. Example with implicit sum: name,ssCpuUser, :eq Equivalent example with explicit sum: name,ssCpuUser, :eq , :sum When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the sum. This leads to a final result of: Name Data ssCpuUser [10.0, 11.0, 8.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/sum/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the sum of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :max , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :sum","title":"Math"},{"location":"asl/ref/swap/","text":"Input Stack: b a \u21e8 Output Stack: a b Swap the top two items on the stack. Example: a,b, :swap Pos Input Output 0 b a 1 a b","title":"swap"},{"location":"asl/ref/time-span/","text":"Input Stack: e: String s: String \u21e8 Output Stack: TimeSeriesExpr Generates a signal line based on the specified time range. The line will be 1 within the range and 0 for all other times. The format of the start and end times is the same as the start and end time parameters on the Graph API. If the time zone is not explicitly specified, then the value from the tz variable will get used. The default value for the tz variable is the primary time zone used for the graph. The following named times are supported for time spans: Name Description gs Graph start time. ge Graph end time. s Start time for the span, can only be used for the end time. e End time for the span, can only be used for the start time. now Current time. epoch January 1, 1970 UTC. Since: 1.6 Example: Relative Absolute e-30m,ge, :time-span 2014-02-20T13:00,s%2B30m, :time-span","title":"time-span"},{"location":"asl/ref/time/","text":"Input Stack: String \u21e8 Output Stack: TimeSeriesExpr Generates a line based on the current time. Supported modes are: secondOfMinute secondOfDay minuteOfHour minuteOfDay hourOfDay dayOfWeek dayOfMonth dayOfYear monthOfYear yearOfCentury yearOfEra seconds (since epoch) days (since epoch) The mode can also be a value of the enum ChronoField . Examples: Hour of Day Enum hourOfDay, :time HOUR_OF_DAY, :time","title":"time"},{"location":"asl/ref/topk-others-avg/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes an average aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-avg","title":"topk-others-avg"},{"location":"asl/ref/topk-others-max/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a max aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-max","title":"topk-others-max"},{"location":"asl/ref/topk-others-min/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a min aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-min","title":"topk-others-min"},{"location":"asl/ref/topk-others-sum/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a sum aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-sum","title":"topk-others-sum"},{"location":"asl/ref/topk/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic . Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk In some cases it can be useful to see an aggregate summary of the other time series that were not part of the top set. This can be accomplished using the :topk-others-$(aggr) operators. For more details see: :topk-others-avg :topk-others-max :topk-others-min :topk-others-sum","title":"topk"},{"location":"asl/ref/trend/","text":"Input Stack: window: Duration TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Warning Deprecated: Use :rolling-mean instead. Computes a moving average over the input window. Until there is at least one sample for the whole window it will emit NaN . If the input line has NaN values, then they will be treated as zeros. Example: Input 2m,:trend 5m,:trend 0 NaN NaN 1 0.5 NaN -1 0.0 NaN NaN -0.5 NaN 0 0.0 0.0 1 0.5 0.2 2 1.5 0.4 1 1.5 0.8 1 1.0 1.0 0 0.5 1.0 The window size is specified as a range of time. If the window size is not evenly divisible by the step size , then the window size will be rounded down. So a 5m window with a 2m step would result in a 4m window with two datapoints per average. A step size larger than the window will result in the trend being a no-op. Examples: 5 Minutes 20 Minutes :random , PT5M, :trend :random , 20m, :trend","title":"trend"},{"location":"asl/ref/true/","text":"Input Stack: \u21e8 Output Stack: Query Query expression that will match any input time series. See also :false .","title":"true"},{"location":"asl/ref/tuck/","text":"Input Stack: b a \u21e8 Output Stack: b a b Shorthand equivalent to writing: :swap,:over Example: a,b, :tuck Pos Input Output 0 b b 1 a a 2 b","title":"tuck"},{"location":"asl/ref/vspan/","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: StyleExpr Change the line style to be a vertical span. In this mode any non-zero datapoints on the line will be shown as a span. This is frequently used to visualize when an alert would have fired. See the line style examples page for more information. Example: Before After name,sps, :eq , :sum , :dup , 20e3, :gt name,sps, :eq , :sum , :dup , 20e3, :gt , :vspan","title":"vspan"},{"location":"concepts/consolidation/","text":"TODO","title":"Consolidation"},{"location":"concepts/naming/","text":"Summary \u00b6 Names Describe the measurement being collected Use camelCase Static Succinct Tags Should be used for dimensional filtering Be careful about combinatorial explosion Tag keys should be static Use id to distinguish between instances Use Base Units Names \u00b6 Describe the Measurement \u00b6 Use camelCase \u00b6 The main goal here is to promote consistency, which makes it easier for users. The choice of style is somewhat arbitrary, but camelCase was chosen because: Used by SNMP Used by Java It was commonly used at Netflix when the guideline was written The exception to this rule is where there is an established common case. For example, with Amazon regions, it is preferred to use us-east-1 rather than usEast1 as it is the more common form. Static \u00b6 There should not be any dynamic content in a metric name, such as requests.$APP_NAME . Metric names and tag keys are how users interact with the data, and dynamic values make them difficult to use. Dynamic information is better suited for tag values, such as nf.app or status . Succinct \u00b6 Long names should be avoided. In many cases, long names are the result of combining many pieces of information together into a single string. In this case, consider either discarding information that is not useful or encoding the information in tag values. Tags \u00b6 Historically, tags have been used to play one of two roles: Dimensions. This is the primary use of tags and this feature allows the data to be filtered into subsets by values of interest. Namespace. Similar to packages in Java, this allows grouping related data. This type of usage is discouraged. As a general rule, it should be possible to use the name as a pivot. If only the name is selected, then the user should be able to use other dimensions to filter the data and successfully reason about the value being shown. As a concrete example, suppose we have two metrics: The number of threads currently in a thread pool. The number of rows in a database table. Discouraged Approach \u00b6 Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"ThreadPool\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"Database\" ) . withTag ( \"table\" , \"users\" ); In this approach, if you select the name size , then it will match both the ThreadPool and Database classes. This results in a value that is the an aggregate of the number of threads and the number of items in a database, which has no meaning. Recommended Approach \u00b6 Id poolSize = registry . createId ( \"threadpool.size\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"db.size\" ) . withTag ( \"table\" , \"users\" ); This variation provides enough context, so that if just the name is selected, the value can be reasoned about and is at least potentially meaningful. This variation provides enough context in the name so that the meaning is more apparent and you can successfully reason about the values. For example, if you select threadpool.size , then you can see the total number of threads in all pools. You can then group by or select an id to further filter the data to a subset in which you have an interest. Use Base Units \u00b6 Keep measurements in base units where possible. It is better to have all timers in seconds, disk sizes in bytes, and network rates in bytes/second. This allows any SI unit prefixes applied to tick labels on a graph to have an obvious meaning, such as 1k meaning 1 kilobyte, as opposed to 1 kilo-megabyte.","title":"Naming"},{"location":"concepts/naming/#summary","text":"Names Describe the measurement being collected Use camelCase Static Succinct Tags Should be used for dimensional filtering Be careful about combinatorial explosion Tag keys should be static Use id to distinguish between instances Use Base Units","title":"Summary"},{"location":"concepts/naming/#names","text":"","title":"Names"},{"location":"concepts/naming/#describe-the-measurement","text":"","title":"Describe the Measurement"},{"location":"concepts/naming/#use-camelcase","text":"The main goal here is to promote consistency, which makes it easier for users. The choice of style is somewhat arbitrary, but camelCase was chosen because: Used by SNMP Used by Java It was commonly used at Netflix when the guideline was written The exception to this rule is where there is an established common case. For example, with Amazon regions, it is preferred to use us-east-1 rather than usEast1 as it is the more common form.","title":"Use camelCase"},{"location":"concepts/naming/#static","text":"There should not be any dynamic content in a metric name, such as requests.$APP_NAME . Metric names and tag keys are how users interact with the data, and dynamic values make them difficult to use. Dynamic information is better suited for tag values, such as nf.app or status .","title":"Static"},{"location":"concepts/naming/#succinct","text":"Long names should be avoided. In many cases, long names are the result of combining many pieces of information together into a single string. In this case, consider either discarding information that is not useful or encoding the information in tag values.","title":"Succinct"},{"location":"concepts/naming/#tags","text":"Historically, tags have been used to play one of two roles: Dimensions. This is the primary use of tags and this feature allows the data to be filtered into subsets by values of interest. Namespace. Similar to packages in Java, this allows grouping related data. This type of usage is discouraged. As a general rule, it should be possible to use the name as a pivot. If only the name is selected, then the user should be able to use other dimensions to filter the data and successfully reason about the value being shown. As a concrete example, suppose we have two metrics: The number of threads currently in a thread pool. The number of rows in a database table.","title":"Tags"},{"location":"concepts/naming/#discouraged-approach","text":"Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"ThreadPool\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"Database\" ) . withTag ( \"table\" , \"users\" ); In this approach, if you select the name size , then it will match both the ThreadPool and Database classes. This results in a value that is the an aggregate of the number of threads and the number of items in a database, which has no meaning.","title":"Discouraged Approach"},{"location":"concepts/naming/#recommended-approach","text":"Id poolSize = registry . createId ( \"threadpool.size\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"db.size\" ) . withTag ( \"table\" , \"users\" ); This variation provides enough context, so that if just the name is selected, the value can be reasoned about and is at least potentially meaningful. This variation provides enough context in the name so that the meaning is more apparent and you can successfully reason about the values. For example, if you select threadpool.size , then you can see the total number of threads in all pools. You can then group by or select an id to further filter the data to a subset in which you have an interest.","title":"Recommended Approach"},{"location":"concepts/naming/#use-base-units","text":"Keep measurements in base units where possible. It is better to have all timers in seconds, disk sizes in bytes, and network rates in bytes/second. This allows any SI unit prefixes applied to tick labels on a graph to have an obvious meaning, such as 1k meaning 1 kilobyte, as opposed to 1 kilo-megabyte.","title":"Use Base Units"},{"location":"concepts/normalization/","text":"Normalization \u00b6 In Atlas, this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute, instead of exactly on the minute. The values will get normalized to the minute boundary, so that all time series in the system are consistent. How a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like Spectator will automatically handle tagging based on the data source type. It is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data. Gauge \u00b6 A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 8 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 6 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Rate \u00b6 A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 7 \u2502 \u2502 \u2502 6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 3 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Here, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute. Time Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7 If many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, the value will be treated as the average rate per second over the previous step. This behavior is important to avoid under-counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to 1 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Why perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms: Avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually getting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with Gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part. Counter \u00b6 Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate . Note, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval.","title":"Normalization"},{"location":"concepts/normalization/#normalization","text":"In Atlas, this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute, instead of exactly on the minute. The values will get normalized to the minute boundary, so that all time series in the system are consistent. How a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like Spectator will automatically handle tagging based on the data source type. It is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data.","title":"Normalization"},{"location":"concepts/normalization/#gauge","text":"A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 8 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 6 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03","title":"Gauge"},{"location":"concepts/normalization/#rate","text":"A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 7 \u2502 \u2502 \u2502 6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 3 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Here, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute. Time Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7 If many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, the value will be treated as the average rate per second over the previous step. This behavior is important to avoid under-counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to 1 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Why perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms: Avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually getting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with Gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part.","title":"Rate"},{"location":"concepts/normalization/#counter","text":"Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate . Note, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval.","title":"Counter"},{"location":"concepts/time-series/","text":"Time Series \u00b6 A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size . In Atlas, each time series is paired with metadata called tags that allow us to query and group the data. Tags \u00b6 A set of key value pairs associated with a time series . Each time series must have at least one tag with a key of name . To make it more concrete, here is an example of a tag set represented as a JSON object: { \"name\" : \"server.requestCount\" , \"status\" : \"200\" , \"endpoint\" : \"api\" , \"nf.app\" : \"fooserver\" , \"nf.cluster\" : \"fooserver-main\" , \"nf.stack\" : \"main\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1c\" , \"nf.node\" : \"i-12345678\" } Usage of tags typically falls into two categories: Namespace. These are tags necessary to qualify a name, so that it can be meaningfully aggregated. Using the sample above, consider computing the sum of all metrics for application fooserver . That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name . The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions. These are tags used to filter the data to a meaningful subset. They can be used to see the number of successful requests across the cluster by querying for status = 200 or the number of requests for a single node by querying for nf.node = i-12345678 . Most tags should fall into this category. When creating metrics, it is important to carefully think about how the data should be tagged. See the naming docs for more information. Metric \u00b6 A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series . A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric. Data Point \u00b6 A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter . Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size . The client library will sample the counter once for each interval and report a single value. Suppose that each circle in the diagram below represents a request: 1:00 1:01 1:02 1:03 \u251c\u2500\u25cf\u2500\u2500\u2500\u2500\u25cf\u25cf\u25cf\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 There are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be: Time Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167 Step Size \u00b6 The amount of time between two successive data points in a time series . For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary.","title":"Time Series"},{"location":"concepts/time-series/#time-series","text":"A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size . In Atlas, each time series is paired with metadata called tags that allow us to query and group the data.","title":"Time Series"},{"location":"concepts/time-series/#tags","text":"A set of key value pairs associated with a time series . Each time series must have at least one tag with a key of name . To make it more concrete, here is an example of a tag set represented as a JSON object: { \"name\" : \"server.requestCount\" , \"status\" : \"200\" , \"endpoint\" : \"api\" , \"nf.app\" : \"fooserver\" , \"nf.cluster\" : \"fooserver-main\" , \"nf.stack\" : \"main\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1c\" , \"nf.node\" : \"i-12345678\" } Usage of tags typically falls into two categories: Namespace. These are tags necessary to qualify a name, so that it can be meaningfully aggregated. Using the sample above, consider computing the sum of all metrics for application fooserver . That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name . The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions. These are tags used to filter the data to a meaningful subset. They can be used to see the number of successful requests across the cluster by querying for status = 200 or the number of requests for a single node by querying for nf.node = i-12345678 . Most tags should fall into this category. When creating metrics, it is important to carefully think about how the data should be tagged. See the naming docs for more information.","title":"Tags"},{"location":"concepts/time-series/#metric","text":"A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series . A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric.","title":"Metric"},{"location":"concepts/time-series/#data-point","text":"A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter . Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size . The client library will sample the counter once for each interval and report a single value. Suppose that each circle in the diagram below represents a request: 1:00 1:01 1:02 1:03 \u251c\u2500\u25cf\u2500\u2500\u2500\u2500\u25cf\u25cf\u25cf\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 There are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be: Time Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167","title":"Data Point"},{"location":"concepts/time-series/#step-size","text":"The amount of time between two successive data points in a time series . For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary.","title":"Step Size"},{"location":"spectator/","text":"Simple library for instrumenting code to record dimensional time series data. At a minimum, you need to: Understand core concepts. Time Series Normalization Naming Clock Install the language-specific library and configuration bindings, where available. Support Class Descriptions Language Overview First-Class Support Java Node.js Best-Effort Support C++ Go Python Ruby Instrument some code, referring to the core usage guides on the following meter types: Counters Distribution Summaries Gauges Percentile Timers Timers After you are more familiar with the library and need assistance with more advanced topics, see the Patterns section on the left.","title":"Overview"},{"location":"spectator/core/clock/","text":"Clock \u00b6 When taking measurements or working with timers it is recommended to use the Clock interface. It provides two methods for measuring time: Wall Time \u00b6 This is what most users think of for time. It can be used to get the current time like what you would see on a wall clock. In most cases when not running in tests this will call System.currentTimeMillis() . Note that the values returned by this method may not be monotonically increasing. Just like a clock on your wall, this value can go back in time or jump forward at unpredictable intervals, if someone sets the time. On many systems, ntpd or similar daemons will be constantly keeping the time synced up with an authoritative source. With Spectator, the Clock is typically accessed via the Registry . Java usage example: // Current time in milliseconds since the epoch long currentTime = registry . clock (). wallTime (); Monotonic Time \u00b6 While it is good in general for the wall clock to show the correct time, the unpredictable changes mean it is not a good choice for measuring how long an operation took. Consider a simple example of measuring request latency on a server: long start = registry . clock (). wallTime (); handleRequest ( request , response ); long end = registry . clock (). wallTime (); reqLatencyTimer . record ( end - start , TimeUnit . MILLISECONDS ); If ntp fixes the server time between start and end , then the recorded latency will be wrong. Spectator will protect against obviously wrong measurements like negative latencies by dropping those values when they are recorded. However, the change could incorrectly shorten or lengthen the measured latency. The clock interface also provides access to a monotonic source that is only useful for measuring elapsed time, for example: long start = registry . clock (). monotonicTime (); handleRequest ( request , response ); long end = registry . clock (). monotonicTime (); reqLatencyTimer . record ( end - start , TimeUnit . NANOSECONDS ); In most cases this will map to System.nanoTime() . Note the actual value returned is not meaningful unless compared with another sample to get a delta. Manual Clock \u00b6 If timing code is written to the Clock interface, then alternative implementations can be plugged-in. For test cases, it is common to use ManualClock so that tests can be reliable and fast without having to rely on hacks like sleep or assuming something will run in less than a certain amount of time. ManualClock clock = new ManualClock (); Registry registry = new DefaultRegistry ( clock ); Timer timer = registry . timer ( \"test\" ); timer . record (() -> { doSomething (); clock . setMonotonicTime ( 42L ); }); Assert . assertEquals ( timer . totalTime (), 42L );","title":"Clock"},{"location":"spectator/core/clock/#clock","text":"When taking measurements or working with timers it is recommended to use the Clock interface. It provides two methods for measuring time:","title":"Clock"},{"location":"spectator/core/clock/#wall-time","text":"This is what most users think of for time. It can be used to get the current time like what you would see on a wall clock. In most cases when not running in tests this will call System.currentTimeMillis() . Note that the values returned by this method may not be monotonically increasing. Just like a clock on your wall, this value can go back in time or jump forward at unpredictable intervals, if someone sets the time. On many systems, ntpd or similar daemons will be constantly keeping the time synced up with an authoritative source. With Spectator, the Clock is typically accessed via the Registry . Java usage example: // Current time in milliseconds since the epoch long currentTime = registry . clock (). wallTime ();","title":"Wall Time"},{"location":"spectator/core/clock/#monotonic-time","text":"While it is good in general for the wall clock to show the correct time, the unpredictable changes mean it is not a good choice for measuring how long an operation took. Consider a simple example of measuring request latency on a server: long start = registry . clock (). wallTime (); handleRequest ( request , response ); long end = registry . clock (). wallTime (); reqLatencyTimer . record ( end - start , TimeUnit . MILLISECONDS ); If ntp fixes the server time between start and end , then the recorded latency will be wrong. Spectator will protect against obviously wrong measurements like negative latencies by dropping those values when they are recorded. However, the change could incorrectly shorten or lengthen the measured latency. The clock interface also provides access to a monotonic source that is only useful for measuring elapsed time, for example: long start = registry . clock (). monotonicTime (); handleRequest ( request , response ); long end = registry . clock (). monotonicTime (); reqLatencyTimer . record ( end - start , TimeUnit . NANOSECONDS ); In most cases this will map to System.nanoTime() . Note the actual value returned is not meaningful unless compared with another sample to get a delta.","title":"Monotonic Time"},{"location":"spectator/core/clock/#manual-clock","text":"If timing code is written to the Clock interface, then alternative implementations can be plugged-in. For test cases, it is common to use ManualClock so that tests can be reliable and fast without having to rely on hacks like sleep or assuming something will run in less than a certain amount of time. ManualClock clock = new ManualClock (); Registry registry = new DefaultRegistry ( clock ); Timer timer = registry . timer ( \"test\" ); timer . record (() -> { doSomething (); clock . setMonotonicTime ( 42L ); }); Assert . assertEquals ( timer . totalTime (), 42L );","title":"Manual Clock"},{"location":"spectator/core/meters/counter/","text":"Counter \u00b6 A Counter is used to measure the rate at which some event is occurring. Considering a simple queue, Counters could be used to measure things like the rate at which items are being inserted and removed. Counters are reported to the backend as a rate-per-second. This makes it much easier to reason about the measurement and allows for aggregating the counter across instances. In Atlas, the :per-step operator can be used to convert them back into a count-per-step on a graph. Note For high performance code, such as incrementing in a tight loop that lasts less than a reporting interval, increment a local variable and add the final value to the counter after the loop has completed. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Counters"},{"location":"spectator/core/meters/counter/#counter","text":"A Counter is used to measure the rate at which some event is occurring. Considering a simple queue, Counters could be used to measure things like the rate at which items are being inserted and removed. Counters are reported to the backend as a rate-per-second. This makes it much easier to reason about the measurement and allows for aggregating the counter across instances. In Atlas, the :per-step operator can be used to convert them back into a count-per-step on a graph. Note For high performance code, such as incrementing in a tight loop that lasts less than a reporting interval, increment a local variable and add the final value to the counter after the loop has completed.","title":"Counter"},{"location":"spectator/core/meters/counter/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/counter/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/counter/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/dist-summary/","text":"Distribution Summary \u00b6 A Distribution Summary is used to track the distribution of events. It is similar to a [Timer], but more general, in that the size does not have to be a period of time. For example, a distribution summary could be used to measure the payload sizes of requests hitting a server or the number of records returned from a query. It is recommended to always use base units when recording the data. So, if measuring the payload size use bytes, not kilobytes or some other unit. This allows the presentation layer for graphing to use either SI or IEC prefixes in a natural manner, and you do not need to consider the meaning of something like \"milli-milliseconds\". Querying \u00b6 Note Distribution summaries report summarized statistics about the measurements for a time window including the totalAmount , count , max and totalOfSquares . If you were to simply query for the name of your timer via nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and you would get a nonsense value that is the sum of the reported statistics. When querying the results of a distribution summary, either select one of the statistics above via a filter, or use one of the operators below to generate a useful response. Average Measurement (:dist-avg) \u00b6 To compute the average latency across an arbitrary group, use the :dist-avg function: nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and , Maximum Measurement (:dist-max) \u00b6 To compute the maximum latency across a group, use :dist-max : nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and , Standard Deviation of Measurement (:dist-stddev) \u00b6 To compute the standard deviation of measurements across all instances for a time interval: nnf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and , :dist-stddev Raw Statistics \u00b6 Note that it is possible to plot the individual statics by filtering on the statistic tag. If you choose to do so, note that the count , totalAmount and totalOfSquares are counters thus reported as rates per second, while the max is reported as a gauge. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Distribution Summaries"},{"location":"spectator/core/meters/dist-summary/#distribution-summary","text":"A Distribution Summary is used to track the distribution of events. It is similar to a [Timer], but more general, in that the size does not have to be a period of time. For example, a distribution summary could be used to measure the payload sizes of requests hitting a server or the number of records returned from a query. It is recommended to always use base units when recording the data. So, if measuring the payload size use bytes, not kilobytes or some other unit. This allows the presentation layer for graphing to use either SI or IEC prefixes in a natural manner, and you do not need to consider the meaning of something like \"milli-milliseconds\".","title":"Distribution Summary"},{"location":"spectator/core/meters/dist-summary/#querying","text":"Note Distribution summaries report summarized statistics about the measurements for a time window including the totalAmount , count , max and totalOfSquares . If you were to simply query for the name of your timer via nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and you would get a nonsense value that is the sum of the reported statistics. When querying the results of a distribution summary, either select one of the statistics above via a filter, or use one of the operators below to generate a useful response.","title":"Querying"},{"location":"spectator/core/meters/dist-summary/#average-measurement-dist-avg","text":"To compute the average latency across an arbitrary group, use the :dist-avg function: nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and ,","title":"Average Measurement (:dist-avg)"},{"location":"spectator/core/meters/dist-summary/#maximum-measurement-dist-max","text":"To compute the maximum latency across a group, use :dist-max : nf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and ,","title":"Maximum Measurement (:dist-max)"},{"location":"spectator/core/meters/dist-summary/#standard-deviation-of-measurement-dist-stddev","text":"To compute the standard deviation of measurements across all instances for a time interval: nnf.cluster,foo, :eq , name,http.req.payload.size, :eq , :and , :dist-stddev","title":"Standard Deviation of Measurement (:dist-stddev)"},{"location":"spectator/core/meters/dist-summary/#raw-statistics","text":"Note that it is possible to plot the individual statics by filtering on the statistic tag. If you choose to do so, note that the count , totalAmount and totalOfSquares are counters thus reported as rates per second, while the max is reported as a gauge.","title":"Raw Statistics"},{"location":"spectator/core/meters/dist-summary/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/dist-summary/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/dist-summary/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/gauge/","text":"Gauge \u00b6 A Gauge is a value that is sampled at some point in time. Typical examples for Gauges would be the size of a queue, or the number of threads in a running state. Since Gauges are not updated inline when a state change occurs, there is no information about what might have occurred between samples. Consider monitoring the behavior of a queue of tasks. If the data is being collected once a minute, then a Gauge for the size will show the size when it was sampled (a.k.a. last-write-wins). The size may have been much higher or lower at some point during interval, but that is not known. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Gauges"},{"location":"spectator/core/meters/gauge/#gauge","text":"A Gauge is a value that is sampled at some point in time. Typical examples for Gauges would be the size of a queue, or the number of threads in a running state. Since Gauges are not updated inline when a state change occurs, there is no information about what might have occurred between samples. Consider monitoring the behavior of a queue of tasks. If the data is being collected once a minute, then a Gauge for the size will show the size when it was sampled (a.k.a. last-write-wins). The size may have been much higher or lower at some point during interval, but that is not known.","title":"Gauge"},{"location":"spectator/core/meters/gauge/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/gauge/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/gauge/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/timer/","text":"Timer \u00b6 A Timer is used to measure how long (in seconds) some event is taking. Timer measurements are typically short, less than 1 minute. A selection of specialized timers include: LongTaskTimer - Periodically reports the time taken for a long running task (> 1 minute). See the Long Task Timer pattern for details. PercentileTimer - Useful if percentile approximations are needed in addition to basic stats. See the Percentile Timer pattern for details. Querying \u00b6 Note Timers report summarized statistics about the measurements for a time window including the totalTime , count , max and totalOfSquares . If you were to simply query for the name of your timer via nnf.cluster,foo, :eq , name,http.req.latency, :eq , :and you would get a nonsense value that is the sum of the reported statistics. When querying the results of a timer, use one of the operators below to generate a useful response. Average Measurement (:dist-avg) \u00b6 To compute the average latency across an arbitrary group, use the :dist-avg function: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , Maximum Measurement (:dist-max) \u00b6 To compute the maximum latency across a group, use :dist-max : nf.cluster,foo, :eq , name,http.req.latency, :eq , :and , Standard Deviation of Measurement (:dist-stddev) \u00b6 To compute the standard deviation of measurements across all instances for a time interval: nnf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-stddev Raw Statistics \u00b6 Note that it is possible to plot the individual statics by filtering on the statistic tag. If you choose to do so, note that the count , totalAmount and totalOfSquares are counters thus reported as rates per second, while the max is reported as a gauge. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Timers"},{"location":"spectator/core/meters/timer/#timer","text":"A Timer is used to measure how long (in seconds) some event is taking. Timer measurements are typically short, less than 1 minute. A selection of specialized timers include: LongTaskTimer - Periodically reports the time taken for a long running task (> 1 minute). See the Long Task Timer pattern for details. PercentileTimer - Useful if percentile approximations are needed in addition to basic stats. See the Percentile Timer pattern for details.","title":"Timer"},{"location":"spectator/core/meters/timer/#querying","text":"Note Timers report summarized statistics about the measurements for a time window including the totalTime , count , max and totalOfSquares . If you were to simply query for the name of your timer via nnf.cluster,foo, :eq , name,http.req.latency, :eq , :and you would get a nonsense value that is the sum of the reported statistics. When querying the results of a timer, use one of the operators below to generate a useful response.","title":"Querying"},{"location":"spectator/core/meters/timer/#average-measurement-dist-avg","text":"To compute the average latency across an arbitrary group, use the :dist-avg function: nf.cluster,foo, :eq , name,http.req.latency, :eq , :and ,","title":"Average Measurement (:dist-avg)"},{"location":"spectator/core/meters/timer/#maximum-measurement-dist-max","text":"To compute the maximum latency across a group, use :dist-max : nf.cluster,foo, :eq , name,http.req.latency, :eq , :and ,","title":"Maximum Measurement (:dist-max)"},{"location":"spectator/core/meters/timer/#standard-deviation-of-measurement-dist-stddev","text":"To compute the standard deviation of measurements across all instances for a time interval: nnf.cluster,foo, :eq , name,http.req.latency, :eq , :and , :dist-stddev","title":"Standard Deviation of Measurement (:dist-stddev)"},{"location":"spectator/core/meters/timer/#raw-statistics","text":"Note that it is possible to plot the individual statics by filtering on the statistic tag. If you choose to do so, note that the count , totalAmount and totalOfSquares are counters thus reported as rates per second, while the max is reported as a gauge.","title":"Raw Statistics"},{"location":"spectator/core/meters/timer/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/timer/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/timer/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/lang/overview/","text":"The original Spectator library was written in Java , with the first stable version ( 0.35.0 ) released on Jan 18, 2016. Since then, there has been a proliferation of languages at Netflix which seek first-class observability support. After some thought and experimentation, we have settled on a strategy of developing minimal Spectator implementations in many languages, which function as thin clients that send data to Atlas. Our goal is to have partners invested in each experimental language who will provide the necessary expertise to develop idiomatic solutions, deliver real-world feedback on library usage, and shoulder some of the support and maintenance burden. We think this is a more sustainable path over the long-term than expanding our team to support N different languages for this singular polyglot use case. First-Class Support \u00b6 These libraries are fully-supported by the team and see wide use across Netflix. Issues are fixed in a timely manner and updates are published regularly. Java (GA) Node.js (GA) Best-Effort Support \u00b6 Warning Support for these languages is experimental and subject to change. Do not use these libraries unless you understand and accept that risk. C++ (Alpha) Go (Beta) Python (Beta) Ruby (Alpha)","title":"Overview"},{"location":"spectator/lang/overview/#first-class-support","text":"These libraries are fully-supported by the team and see wide use across Netflix. Issues are fixed in a timely manner and updates are published regularly. Java (GA) Node.js (GA)","title":"First-Class Support"},{"location":"spectator/lang/overview/#best-effort-support","text":"Warning Support for these languages is experimental and subject to change. Do not use these libraries unless you understand and accept that risk. C++ (Alpha) Go (Beta) Python (Beta) Ruby (Alpha)","title":"Best-Effort Support"},{"location":"spectator/lang/cpp/usage/","text":"Project \u00b6 Source Product Lifecycle: Alpha This implements a basic Spectator library for instrumenting C++ applications and sending metrics to an Atlas aggregator service. Install Library \u00b6 TBD If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 #include <spectator/registry.h> // use default values static constexpr auto kDefault = 0 ; struct Request { std :: string country ; }; struct Response { int status ; int size ; }; class Server { public : explicit Server ( spectator :: Registry * registry ) : registry_ { registry }, request_count_id_ { registry -> CreateId ( \"server.requestCount\" , spectator :: Tags {})}, request_latency_ { registry -> GetTimer ( \"server.requestLatency\" )}, response_size_ { registry -> GetDistributionSummary ( \"server.responseSizes\" )} {} Response Handle ( const Request & request ) { using spectator :: Registry ; auto start = Registry :: clock :: now (); // do some work and obtain a response... Response res { 200 , 64 }; // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. auto cnt_id = request_count_id_ -> WithTag ( \"country\" , request . country ) -> WithTag ( \"status\" , std :: to_string ( res . status )); registry_ -> GetCounter ( std :: move ( cnt_id )) -> Increment (); request_latency_ -> Record ( Registry :: clock :: now () - start ); response_size_ -> Record ( res . size ); return res ; } private : spectator :: Registry * registry_ ; std :: shared_ptr < spectator :: Id > request_count_id_ ; std :: shared_ptr < spectator :: Timer > request_latency_ ; std :: shared_ptr < spectator :: DistributionSummary > response_size_ ; }; Request get_next_request () { //... return Request { \"US\" }; } int main () { spectator :: Registry registry { spectator :: GetConfiguration ()}; registry . Start (); Server server { & registry }; for ( auto i = 1 ; i <= 3 ; ++ i ) { // get a request auto req = get_next_request (); server . Handle ( req ); } registry . Stop (); } Netflix Integration \u00b6 Copy the netflix_config.cc file from the netflix-spectator-cppconf repository in Stash to a directory where your sources reside.","title":"Usage"},{"location":"spectator/lang/cpp/usage/#project","text":"Source Product Lifecycle: Alpha This implements a basic Spectator library for instrumenting C++ applications and sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/cpp/usage/#install-library","text":"TBD If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/cpp/usage/#instrumenting-code","text":"#include <spectator/registry.h> // use default values static constexpr auto kDefault = 0 ; struct Request { std :: string country ; }; struct Response { int status ; int size ; }; class Server { public : explicit Server ( spectator :: Registry * registry ) : registry_ { registry }, request_count_id_ { registry -> CreateId ( \"server.requestCount\" , spectator :: Tags {})}, request_latency_ { registry -> GetTimer ( \"server.requestLatency\" )}, response_size_ { registry -> GetDistributionSummary ( \"server.responseSizes\" )} {} Response Handle ( const Request & request ) { using spectator :: Registry ; auto start = Registry :: clock :: now (); // do some work and obtain a response... Response res { 200 , 64 }; // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. auto cnt_id = request_count_id_ -> WithTag ( \"country\" , request . country ) -> WithTag ( \"status\" , std :: to_string ( res . status )); registry_ -> GetCounter ( std :: move ( cnt_id )) -> Increment (); request_latency_ -> Record ( Registry :: clock :: now () - start ); response_size_ -> Record ( res . size ); return res ; } private : spectator :: Registry * registry_ ; std :: shared_ptr < spectator :: Id > request_count_id_ ; std :: shared_ptr < spectator :: Timer > request_latency_ ; std :: shared_ptr < spectator :: DistributionSummary > response_size_ ; }; Request get_next_request () { //... return Request { \"US\" }; } int main () { spectator :: Registry registry { spectator :: GetConfiguration ()}; registry . Start (); Server server { & registry }; for ( auto i = 1 ; i <= 3 ; ++ i ) { // get a request auto req = get_next_request (); server . Handle ( req ); } registry . Stop (); }","title":"Instrumenting Code"},{"location":"spectator/lang/cpp/usage/#netflix-integration","text":"Copy the netflix_config.cc file from the netflix-spectator-cppconf repository in Stash to a directory where your sources reside.","title":"Netflix Integration"},{"location":"spectator/lang/cpp/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/cpp/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/cpp/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/cpp/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/cpp/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/go/usage/","text":"Project \u00b6 Source Product Lifecycle: Beta This implements a basic Spectator library for instrumenting golang applications and sending metrics to an Atlas aggregator service. Install Library \u00b6 Add a github.com/Netflix/spectator-go remote import to your code. If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 package main import ( \"github.com/Netflix/spectator-go\" \"strconv\" \"time\" ) type Server struct { registry * spectator . Registry requestCountId * spectator . Id requestLatency * spectator . Timer responseSizes * spectator . DistributionSummary } type Request struct { country string } type Response struct { status int size int64 } func ( s * Server ) Handle ( request * Request ) ( res * Response ) { clock := s . registry . Clock () start := clock . Now () // initialize res res = & Response { 200 , 64 } // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. cntId := s . requestCountId . WithTag ( \"country\" , request . country ). WithTag ( \"status\" , strconv . Itoa ( res . status )) s . registry . CounterWithId ( cntId ). Increment () // ... s . requestLatency . Record ( clock . Now (). Sub ( start )) s . responseSizes . Record ( res . size ) return } func newServer ( registry * spectator . Registry ) * Server { return & Server { registry , registry . NewId ( \"server.requestCount\" , nil ), registry . Timer ( \"server.requestLatency\" , nil ), registry . DistributionSummary ( \"server.responseSizes\" , nil ), } } func getNextRequest () * Request { // ... return & Request { \"US\" } } func main () { commonTags := map [ string ] string { \"nf.app\" : \"example\" , \"nf.region\" : \"us-west-1\" } config := & spectator . Config { Frequency : 5 * time . Second , Timeout : 1 * time . Second , Uri : \"http://example.org/api/v1/publish\" , CommonTags : commonTags } registry := spectator . NewRegistry ( config ) // optionally set custom logger (needs to implement Debugf, Infof, Errorf) // registry.SetLogger(logger) registry . Start () defer registry . Stop () // collect memory and file descriptor metrics spectator . CollectRuntimeMetrics ( registry ) server := newServer ( registry ) for i := 1 ; i < 3 ; i ++ { // get a request req := getNextRequest () server . Handle ( req ) } } Netflix Integration \u00b6 Create a Netflix Spectator Config to be used by spectator-go , replacing STASH_HOSTNAME with the hostname of the internal Stash server. import ( nfspectator \"STASH_HOSTNAME/cldmta/netflix-spectator-goconf\" spectator \"github.com/Netflix/spectator-go\" ) func main () { config := nfspectator . Config () registry := spectator . NewRegistry ( config ) registry . Start () defer registry . Stop () # ... }","title":"Usage"},{"location":"spectator/lang/go/usage/#project","text":"Source Product Lifecycle: Beta This implements a basic Spectator library for instrumenting golang applications and sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/go/usage/#install-library","text":"Add a github.com/Netflix/spectator-go remote import to your code. If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/go/usage/#instrumenting-code","text":"package main import ( \"github.com/Netflix/spectator-go\" \"strconv\" \"time\" ) type Server struct { registry * spectator . Registry requestCountId * spectator . Id requestLatency * spectator . Timer responseSizes * spectator . DistributionSummary } type Request struct { country string } type Response struct { status int size int64 } func ( s * Server ) Handle ( request * Request ) ( res * Response ) { clock := s . registry . Clock () start := clock . Now () // initialize res res = & Response { 200 , 64 } // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. cntId := s . requestCountId . WithTag ( \"country\" , request . country ). WithTag ( \"status\" , strconv . Itoa ( res . status )) s . registry . CounterWithId ( cntId ). Increment () // ... s . requestLatency . Record ( clock . Now (). Sub ( start )) s . responseSizes . Record ( res . size ) return } func newServer ( registry * spectator . Registry ) * Server { return & Server { registry , registry . NewId ( \"server.requestCount\" , nil ), registry . Timer ( \"server.requestLatency\" , nil ), registry . DistributionSummary ( \"server.responseSizes\" , nil ), } } func getNextRequest () * Request { // ... return & Request { \"US\" } } func main () { commonTags := map [ string ] string { \"nf.app\" : \"example\" , \"nf.region\" : \"us-west-1\" } config := & spectator . Config { Frequency : 5 * time . Second , Timeout : 1 * time . Second , Uri : \"http://example.org/api/v1/publish\" , CommonTags : commonTags } registry := spectator . NewRegistry ( config ) // optionally set custom logger (needs to implement Debugf, Infof, Errorf) // registry.SetLogger(logger) registry . Start () defer registry . Stop () // collect memory and file descriptor metrics spectator . CollectRuntimeMetrics ( registry ) server := newServer ( registry ) for i := 1 ; i < 3 ; i ++ { // get a request req := getNextRequest () server . Handle ( req ) } }","title":"Instrumenting Code"},{"location":"spectator/lang/go/usage/#netflix-integration","text":"Create a Netflix Spectator Config to be used by spectator-go , replacing STASH_HOSTNAME with the hostname of the internal Stash server. import ( nfspectator \"STASH_HOSTNAME/cldmta/netflix-spectator-goconf\" spectator \"github.com/Netflix/spectator-go\" ) func main () { config := nfspectator . Config () registry := spectator . NewRegistry ( config ) registry . Start () defer registry . Stop () # ... }","title":"Netflix Integration"},{"location":"spectator/lang/go/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/go/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/go/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/go/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/go/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/java/servo-migration/","text":"Servo Comparison \u00b6 Servo is an alternative client monitoring library that is also developed by Netflix. Originally, Spectator was an experiment for a simpler API that wrapped Servo. It was done as a separate project to avoid breaking backwards compatibility for Servo. From a user perspective, both will be supported for a long time, but most of our efforts for future improvement will go to Spectator. For new code, it is recommended to use the spectator API. If running at Netflix , the correct bindings will be in place for both Servo and Spectator. Differences \u00b6 This section provides a quick summary of the differences between Spectator and Servo. Simpler API \u00b6 Servo gives the user a lot of control, but this makes it hard to use correctly. For example, to create a Counter, the user needs to understand the trade-offs and choose between: BasicCounter DynamicCounter ContextualCounter StepCounter Further, each of these can impact how data is reported to observers. The Spectator API focuses on the constructs a user needs to instrument the code. In Spectator, the user would always use the Registry to create a Counter . The implementation details are left up to the Registry. The registration is simpler as well to avoid common pitfalls when using Servo like overwriting a registered object. More Focused \u00b6 The goal of Spectator is instrumenting code to send to a dimensional time-series system like Atlas . Servo has goals of staying compatible with a number of legacy libraries and naming formats, exposing data to JMX, etc. Examples of how this influences decisions: No support for non-numeric data. Servo supported this feature, so that it can expose data to JMX. Exposing the numeric data registered in Spectator to JMX can be done using a registry that supports it, but there is no goal to be a general interface for exposing arbitrary data in JMX. No support for custom time units when reporting timer data. Base units should always be used for reporting and conversions can be performed in the presentation layer, if needed. It also avoids a lot of the confusion around the timer unit for the data and issues like creating aggregates that are meaningless due to mixed units. It is better to have a simple way to send correct and easy-to-understand data to the backend than many options. If you want more knobs, then you can use Servo. DI Friendly \u00b6 When Servo was originally written, dependency injection (DI) was not heavily used at Netflix. Further, Servo needed to stay compatible with a number of use-cases that were heavily static. While Spectator does have a static registry that can be used, the recommended way is to create a registry and inject it either manually or via a framework into the classes that need it. This also makes it much easier to test in isolation . Migration \u00b6 If you want to migrate from the Servo API to the Spectator API, then this section provides some guides on how Servo constructs can be ported over. The sub-sections are the class names of monitor types supported by Servo. For users at Netflix, we are not actively pushing teams to migrate or do any additional work. Servo is still supported and if it works for your use-case, then feel free to continue using it. Registration \u00b6 First read through the Servo docs on registration . With Servo, say you have a class like the following: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } public void doSomething () { ... } } The state of the class is in the member variables of an instance of Foo . If multiple instances of class Foo are created with the same value for id , then the last one will overwrite the others for the registration. So the values getting reported will only be from the last instance registered. Also the registry has a reference to the instance of Foo , so it will never go away. For Counters and Timers, one way to get around this is to use DynamicCounter and DynamicTimer , respectively. Those classes will automatically handle the registration and expire if there is no activity. They also get used for cases where the set of dimensions is not known up front. Gauges need to sample the state of something, so they need to have a reference to an object that contains the state. So the user would need to ensure that only a single copy was registered leading to patterns like: class Foo { private static class FooStats { private AtomicInteger gauge ; private Counter counter ; public FooStats ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } } private static ConcurrentHashMap < String , FooStats > STATS = new ConcurrentHashMap <> (); private final FooStats stats ; public Foo ( String id ) { stats = STATS . computeIfAbsent ( id , ( i ) -> new FooStats ( i )); } public void doSomething () { ... stats . update (); } } This ensures that there is a single copy for a given id. In spectator this example would look like: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"gauge\" ). withTag ( \"id\" , id ); gauge = registry . gauge ( gaugeId , new AtomicInteger ()); counter = registry . counter ( \"counter\" , \"id\" , id ); } public void doSomething () { ... } } Everything using the same Registry will get the same Counter instance, if the same id is used. For the Gauge, the Registry will keep a weak reference and will sum the values if multiple instances are present. Since it is a weak reference, nothing will prevent an instance of Foo from getting garbage collected. Annotations \u00b6 Annotations are not supported, use the appropriate meter type: DataSourceType Spectator Alternative COUNTER Counter Usage GAUGE Gauge Usage INFORMATIONAL Not supported BasicCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new BasicCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } } BasicGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final BasicGauge g = new BasicGauge ( MonitorConfig . builder ( \"name\" ). build (), this :: getCurrentValue ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); registry . gauge ( gaugeId , this , Foo :: getCurrentValue ); } } BasicTimer \u00b6 See the general overview of registration differences and summary of Timer usage . In Spectator, the reported unit for Timers is always seconds and cannot be changed. Seconds is the base unit and other units should only be used as a presentation detail. Servo allows the unit to be customized and defaults to milliseconds. Servo: public class Foo { private final Timer t = new BasicTimer ( MonitorConfig . builder ( \"name\" ). build (), TimeUnit . SECONDS ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . timer ( \"name\" , \"id\" , id ); } public void doSomething () { t . record (() -> { ... }); } } BasicDistributionSummary \u00b6 See the general overview of registration differences and summary of Distribution Summary usage . Servo: public class Foo { private final BasicDistributionSummary s = new BasicDistributionSummary ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { ... s . record ( getValue ()); } } Spectator: public class Foo { private final DistributionSummary s ; @Inject public Foo ( Registry registry , String id ) { s = registry . distributionSummary ( \"name\" , \"id\" , id ); } public void doSomething () { ... s . record ( getValue ()); } } BasicInformational \u00b6 Not supported, see the overview of differences . BasicStopwatch \u00b6 There isn't an explicit stopwatch class in Spectator. Use a timing call directly. Servo: public void doSomething () { Stopwatch s = timer . start (); try { ... } finally { s . stop (); } } Spectator: public void doSomething () { final long s = System . nanoTime (); try { ... } finally { timer . record ( System . nanoTime () - s , TimeUnit . NANOSECONDS ); } } BucketTimer \u00b6 TODO: find sandbox documentation or remove reference See the general overview of registration differences and the summary of sandbox documentation . In Spectator, BucketTimer is provided in the sandbox extension library and may change in future as we gain more experience using it. Servo: public class Foo { private final Timer t = new BucketTimer ( MonitorConfig . builder ( \"name\" ). build (), new BucketConfig . Builder () . withTimeUnit ( TimeUnit . MILLISECONDS ) . withBuckets ( new long [] { 500 , 2500 , 5000 , 10000 }) . build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { Id timerId = registry . createId ( \"name\" , \"id\" , id ); BucketFunction f = BucketFunctions . latency ( 10 , TimeUnit . SECONDS ); t = BucketTimer . get ( registry , timerId , f ); } public void doSomething () { t . record (() -> { ... }); } } ContextualCounter \u00b6 Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 . ContextualTimer \u00b6 Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 . DoubleGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final DoubleGauge g = new DoubleGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: import com.google.common.util.concurrent.AtomicDouble ; public class Foo { private final AtomicDouble v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicDouble ()); } } DurationTimer \u00b6 See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final DurationTimer t = new DurationTimer ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final LongTaskTimer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . longTaskTimer ( \"name\" , \"id\" , id ); } } DynamicCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final String id ; public Foo ( String id ) { this . id = id ; } public void doSomething ( Context ctxt ) { DynamicCounter . increment ( \"staticId\" , \"id\" , id ); DynamicCounter . increment ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Counter staticCounter ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticCounter = registry . counter ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { // Keeping the reference to the counter avoids additional allocations // to create the id object and the lookup cost staticCounter . increment (); // If the id is dynamic it must be looked up registry . counter ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()). increment (); // This will update the same counter as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . counter ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())). increment (); } } DynamicTimer \u00b6 See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final String id ; private final MonitorConfig staticId ; public Foo ( String id ) { this . id = id ; staticId = MonitorConfig . builder ( \"staticId\" ). withTag ( \"id\" , id ). build (); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); DynamicTimer . record ( staticId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); MonitorConfig dynamicId = MonitorConfig . builder ( \"dynamicId\" ) . withTag ( \"id\" , id ) . withTag ( \"foo\" , ctxt . getFoo ()) . build (); DynamicTimer . record ( dynamicId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Timer staticTimer ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticTimer = registry . timer ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); // Keeping the reference to the timer avoids additional allocations // to create the id object and the lookup cost staticTimer . record ( d , TimeUnit . MILLISECONDS ); // If the id is dynamic it must be looked up registry . timer ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()) . record ( d , TimeUnit . MILLISECONDS ); // This will update the same timer as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . timer ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())) . record ( d , TimeUnit . MILLISECONDS ); } } LongGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final LongGauge g = new LongGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } } MonitorConfig \u00b6 See the documentation on naming . Servo: MonitorConfig id = MonitorConfig . builder ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ) . build (); Spectator: Id id = registry . createId ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ); // or Id id = registry . createId ( \"name\" , \"country\" , \"US\" , \"device\" , \"xbox\" ); MonitoredCache \u00b6 Not supported because Spectator does not have a direct dependency on Guava. If there is enough demand, an extension can be created. NumberGauge \u00b6 See the general overview of registration differences and summary of gauge usage . Servo: public class Foo { private final NumberGauge g = new NumberGauge ( MonitorConfig . builder ( \"name\" ). build (), new AtomicLong ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } } StatsTimer \u00b6 Not supported, see overview of differences . StepCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new StepCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"Servo Migration"},{"location":"spectator/lang/java/servo-migration/#servo-comparison","text":"Servo is an alternative client monitoring library that is also developed by Netflix. Originally, Spectator was an experiment for a simpler API that wrapped Servo. It was done as a separate project to avoid breaking backwards compatibility for Servo. From a user perspective, both will be supported for a long time, but most of our efforts for future improvement will go to Spectator. For new code, it is recommended to use the spectator API. If running at Netflix , the correct bindings will be in place for both Servo and Spectator.","title":"Servo Comparison"},{"location":"spectator/lang/java/servo-migration/#differences","text":"This section provides a quick summary of the differences between Spectator and Servo.","title":"Differences"},{"location":"spectator/lang/java/servo-migration/#simpler-api","text":"Servo gives the user a lot of control, but this makes it hard to use correctly. For example, to create a Counter, the user needs to understand the trade-offs and choose between: BasicCounter DynamicCounter ContextualCounter StepCounter Further, each of these can impact how data is reported to observers. The Spectator API focuses on the constructs a user needs to instrument the code. In Spectator, the user would always use the Registry to create a Counter . The implementation details are left up to the Registry. The registration is simpler as well to avoid common pitfalls when using Servo like overwriting a registered object.","title":"Simpler API"},{"location":"spectator/lang/java/servo-migration/#more-focused","text":"The goal of Spectator is instrumenting code to send to a dimensional time-series system like Atlas . Servo has goals of staying compatible with a number of legacy libraries and naming formats, exposing data to JMX, etc. Examples of how this influences decisions: No support for non-numeric data. Servo supported this feature, so that it can expose data to JMX. Exposing the numeric data registered in Spectator to JMX can be done using a registry that supports it, but there is no goal to be a general interface for exposing arbitrary data in JMX. No support for custom time units when reporting timer data. Base units should always be used for reporting and conversions can be performed in the presentation layer, if needed. It also avoids a lot of the confusion around the timer unit for the data and issues like creating aggregates that are meaningless due to mixed units. It is better to have a simple way to send correct and easy-to-understand data to the backend than many options. If you want more knobs, then you can use Servo.","title":"More Focused"},{"location":"spectator/lang/java/servo-migration/#di-friendly","text":"When Servo was originally written, dependency injection (DI) was not heavily used at Netflix. Further, Servo needed to stay compatible with a number of use-cases that were heavily static. While Spectator does have a static registry that can be used, the recommended way is to create a registry and inject it either manually or via a framework into the classes that need it. This also makes it much easier to test in isolation .","title":"DI Friendly"},{"location":"spectator/lang/java/servo-migration/#migration","text":"If you want to migrate from the Servo API to the Spectator API, then this section provides some guides on how Servo constructs can be ported over. The sub-sections are the class names of monitor types supported by Servo. For users at Netflix, we are not actively pushing teams to migrate or do any additional work. Servo is still supported and if it works for your use-case, then feel free to continue using it.","title":"Migration"},{"location":"spectator/lang/java/servo-migration/#registration","text":"First read through the Servo docs on registration . With Servo, say you have a class like the following: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } public void doSomething () { ... } } The state of the class is in the member variables of an instance of Foo . If multiple instances of class Foo are created with the same value for id , then the last one will overwrite the others for the registration. So the values getting reported will only be from the last instance registered. Also the registry has a reference to the instance of Foo , so it will never go away. For Counters and Timers, one way to get around this is to use DynamicCounter and DynamicTimer , respectively. Those classes will automatically handle the registration and expire if there is no activity. They also get used for cases where the set of dimensions is not known up front. Gauges need to sample the state of something, so they need to have a reference to an object that contains the state. So the user would need to ensure that only a single copy was registered leading to patterns like: class Foo { private static class FooStats { private AtomicInteger gauge ; private Counter counter ; public FooStats ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } } private static ConcurrentHashMap < String , FooStats > STATS = new ConcurrentHashMap <> (); private final FooStats stats ; public Foo ( String id ) { stats = STATS . computeIfAbsent ( id , ( i ) -> new FooStats ( i )); } public void doSomething () { ... stats . update (); } } This ensures that there is a single copy for a given id. In spectator this example would look like: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"gauge\" ). withTag ( \"id\" , id ); gauge = registry . gauge ( gaugeId , new AtomicInteger ()); counter = registry . counter ( \"counter\" , \"id\" , id ); } public void doSomething () { ... } } Everything using the same Registry will get the same Counter instance, if the same id is used. For the Gauge, the Registry will keep a weak reference and will sum the values if multiple instances are present. Since it is a weak reference, nothing will prevent an instance of Foo from getting garbage collected.","title":"Registration"},{"location":"spectator/lang/java/servo-migration/#annotations","text":"Annotations are not supported, use the appropriate meter type: DataSourceType Spectator Alternative COUNTER Counter Usage GAUGE Gauge Usage INFORMATIONAL Not supported","title":"Annotations"},{"location":"spectator/lang/java/servo-migration/#basiccounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new BasicCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"BasicCounter"},{"location":"spectator/lang/java/servo-migration/#basicgauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final BasicGauge g = new BasicGauge ( MonitorConfig . builder ( \"name\" ). build (), this :: getCurrentValue ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); registry . gauge ( gaugeId , this , Foo :: getCurrentValue ); } }","title":"BasicGauge"},{"location":"spectator/lang/java/servo-migration/#basictimer","text":"See the general overview of registration differences and summary of Timer usage . In Spectator, the reported unit for Timers is always seconds and cannot be changed. Seconds is the base unit and other units should only be used as a presentation detail. Servo allows the unit to be customized and defaults to milliseconds. Servo: public class Foo { private final Timer t = new BasicTimer ( MonitorConfig . builder ( \"name\" ). build (), TimeUnit . SECONDS ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . timer ( \"name\" , \"id\" , id ); } public void doSomething () { t . record (() -> { ... }); } }","title":"BasicTimer"},{"location":"spectator/lang/java/servo-migration/#basicdistributionsummary","text":"See the general overview of registration differences and summary of Distribution Summary usage . Servo: public class Foo { private final BasicDistributionSummary s = new BasicDistributionSummary ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { ... s . record ( getValue ()); } } Spectator: public class Foo { private final DistributionSummary s ; @Inject public Foo ( Registry registry , String id ) { s = registry . distributionSummary ( \"name\" , \"id\" , id ); } public void doSomething () { ... s . record ( getValue ()); } }","title":"BasicDistributionSummary"},{"location":"spectator/lang/java/servo-migration/#basicinformational","text":"Not supported, see the overview of differences .","title":"BasicInformational"},{"location":"spectator/lang/java/servo-migration/#basicstopwatch","text":"There isn't an explicit stopwatch class in Spectator. Use a timing call directly. Servo: public void doSomething () { Stopwatch s = timer . start (); try { ... } finally { s . stop (); } } Spectator: public void doSomething () { final long s = System . nanoTime (); try { ... } finally { timer . record ( System . nanoTime () - s , TimeUnit . NANOSECONDS ); } }","title":"BasicStopwatch"},{"location":"spectator/lang/java/servo-migration/#buckettimer","text":"TODO: find sandbox documentation or remove reference See the general overview of registration differences and the summary of sandbox documentation . In Spectator, BucketTimer is provided in the sandbox extension library and may change in future as we gain more experience using it. Servo: public class Foo { private final Timer t = new BucketTimer ( MonitorConfig . builder ( \"name\" ). build (), new BucketConfig . Builder () . withTimeUnit ( TimeUnit . MILLISECONDS ) . withBuckets ( new long [] { 500 , 2500 , 5000 , 10000 }) . build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { Id timerId = registry . createId ( \"name\" , \"id\" , id ); BucketFunction f = BucketFunctions . latency ( 10 , TimeUnit . SECONDS ); t = BucketTimer . get ( registry , timerId , f ); } public void doSomething () { t . record (() -> { ... }); } }","title":"BucketTimer"},{"location":"spectator/lang/java/servo-migration/#contextualcounter","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 .","title":"ContextualCounter"},{"location":"spectator/lang/java/servo-migration/#contextualtimer","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 .","title":"ContextualTimer"},{"location":"spectator/lang/java/servo-migration/#doublegauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final DoubleGauge g = new DoubleGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: import com.google.common.util.concurrent.AtomicDouble ; public class Foo { private final AtomicDouble v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicDouble ()); } }","title":"DoubleGauge"},{"location":"spectator/lang/java/servo-migration/#durationtimer","text":"See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final DurationTimer t = new DurationTimer ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final LongTaskTimer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . longTaskTimer ( \"name\" , \"id\" , id ); } }","title":"DurationTimer"},{"location":"spectator/lang/java/servo-migration/#dynamiccounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final String id ; public Foo ( String id ) { this . id = id ; } public void doSomething ( Context ctxt ) { DynamicCounter . increment ( \"staticId\" , \"id\" , id ); DynamicCounter . increment ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Counter staticCounter ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticCounter = registry . counter ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { // Keeping the reference to the counter avoids additional allocations // to create the id object and the lookup cost staticCounter . increment (); // If the id is dynamic it must be looked up registry . counter ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()). increment (); // This will update the same counter as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . counter ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())). increment (); } }","title":"DynamicCounter"},{"location":"spectator/lang/java/servo-migration/#dynamictimer","text":"See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final String id ; private final MonitorConfig staticId ; public Foo ( String id ) { this . id = id ; staticId = MonitorConfig . builder ( \"staticId\" ). withTag ( \"id\" , id ). build (); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); DynamicTimer . record ( staticId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); MonitorConfig dynamicId = MonitorConfig . builder ( \"dynamicId\" ) . withTag ( \"id\" , id ) . withTag ( \"foo\" , ctxt . getFoo ()) . build (); DynamicTimer . record ( dynamicId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Timer staticTimer ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticTimer = registry . timer ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); // Keeping the reference to the timer avoids additional allocations // to create the id object and the lookup cost staticTimer . record ( d , TimeUnit . MILLISECONDS ); // If the id is dynamic it must be looked up registry . timer ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()) . record ( d , TimeUnit . MILLISECONDS ); // This will update the same timer as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . timer ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())) . record ( d , TimeUnit . MILLISECONDS ); } }","title":"DynamicTimer"},{"location":"spectator/lang/java/servo-migration/#longgauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final LongGauge g = new LongGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } }","title":"LongGauge"},{"location":"spectator/lang/java/servo-migration/#monitorconfig","text":"See the documentation on naming . Servo: MonitorConfig id = MonitorConfig . builder ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ) . build (); Spectator: Id id = registry . createId ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ); // or Id id = registry . createId ( \"name\" , \"country\" , \"US\" , \"device\" , \"xbox\" );","title":"MonitorConfig"},{"location":"spectator/lang/java/servo-migration/#monitoredcache","text":"Not supported because Spectator does not have a direct dependency on Guava. If there is enough demand, an extension can be created.","title":"MonitoredCache"},{"location":"spectator/lang/java/servo-migration/#numbergauge","text":"See the general overview of registration differences and summary of gauge usage . Servo: public class Foo { private final NumberGauge g = new NumberGauge ( MonitorConfig . builder ( \"name\" ). build (), new AtomicLong ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } }","title":"NumberGauge"},{"location":"spectator/lang/java/servo-migration/#statstimer","text":"Not supported, see overview of differences .","title":"StatsTimer"},{"location":"spectator/lang/java/servo-migration/#stepcounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new StepCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"StepCounter"},{"location":"spectator/lang/java/testing/","text":"Testing \u00b6 Testing should be relatively straightforward if you are using injection for the Registry . Consider a sample class: public class Foo { private final Counter counter ; @Inject public Foo ( Registry registry ) { counter = registry . counter ( \"foo\" ); } public void doSomething () { counter . increment (); } } Tests will typically want to use an isolated instance of the DefaultRegistry . Simple Test \u00b6 A basic standalone test class would look something like: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { registry = new DefaultRegistry (); foo = new Foo ( registry ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } } Guice Test \u00b6 If using guice, then the TestModule can be used: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { Injector injector = Guice . createInjector ( new TestModule ()); registry = injector . getInstance ( Registry . class ); foo = injector . getInstance ( Foo . class ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } } Exceptions \u00b6 By default, for most user errors Spectator will log a warning rather than throw an exception. The rationale is that users do not often think about instrumentation and logging code causing an exception and interrupting the control flow of a program. However, for test cases it is recommended to be more aggressive and learn about problems as early as possible. This can be done by setting a system property: spectator.api.propagateWarnings=true Consider an example: private static final Id RARE_EXCEPTION_ID = null ; public void doSomethingImportant () { try { ... do work ... } catch ( RareException e ) { // There is a bug in the program, an Id is not allowed to be null. In production we do // not want it to throw and interrupt the control flow. Instrumentation should gracefully // degrade. registry . counter ( RARE_EXCEPTION_ID ). increment (); // These statements are important to provide context for operating the system // and to ensure the app continues to function properly. LOGGER . error ( \"important context for user\" , e ); properlyHandleException ( e ); } }","title":"Testing"},{"location":"spectator/lang/java/testing/#testing","text":"Testing should be relatively straightforward if you are using injection for the Registry . Consider a sample class: public class Foo { private final Counter counter ; @Inject public Foo ( Registry registry ) { counter = registry . counter ( \"foo\" ); } public void doSomething () { counter . increment (); } } Tests will typically want to use an isolated instance of the DefaultRegistry .","title":"Testing"},{"location":"spectator/lang/java/testing/#simple-test","text":"A basic standalone test class would look something like: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { registry = new DefaultRegistry (); foo = new Foo ( registry ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } }","title":"Simple Test"},{"location":"spectator/lang/java/testing/#guice-test","text":"If using guice, then the TestModule can be used: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { Injector injector = Guice . createInjector ( new TestModule ()); registry = injector . getInstance ( Registry . class ); foo = injector . getInstance ( Foo . class ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } }","title":"Guice Test"},{"location":"spectator/lang/java/testing/#exceptions","text":"By default, for most user errors Spectator will log a warning rather than throw an exception. The rationale is that users do not often think about instrumentation and logging code causing an exception and interrupting the control flow of a program. However, for test cases it is recommended to be more aggressive and learn about problems as early as possible. This can be done by setting a system property: spectator.api.propagateWarnings=true Consider an example: private static final Id RARE_EXCEPTION_ID = null ; public void doSomethingImportant () { try { ... do work ... } catch ( RareException e ) { // There is a bug in the program, an Id is not allowed to be null. In production we do // not want it to throw and interrupt the control flow. Instrumentation should gracefully // degrade. registry . counter ( RARE_EXCEPTION_ID ). increment (); // These statements are important to provide context for operating the system // and to ensure the app continues to function properly. LOGGER . error ( \"important context for user\" , e ); properlyHandleException ( e ); } }","title":"Exceptions"},{"location":"spectator/lang/java/usage/","text":"Project \u00b6 Source Javadoc Product Lifecycle: GA Requirements: Java >= 8 Install Library \u00b6 Depend on the API library, which is available in Maven Central . The only transitive dependency is slf4j . For Gradle, the dependency is specified as follows: dependencies { compile \"com.netflix.spectator:spectator-api:0.101.0\" } Pick a Registry to bind, when initializing the application. If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 Suppose we have a server and we want to keep track of: Number of requests received with dimensions for breaking down by status code, country, and the exception type if the request fails in an unexpected way. Latency for handling requests. Summary of the response sizes. Current number of active connections on the server. Here is some sample code that does that: // In the application initialization setup a registry Registry registry = new DefaultRegistry (); Server s = new Server ( registry ); public class Server { private final Registry registry ; private final Id requestCountId ; private final Timer requestLatency ; private final DistributionSummary responseSizes ; @Inject public Server ( Registry registry ) { this . registry = registry ; // Create a base id for the request count. The id will get refined with // additional dimensions when we receive a request. requestCountId = registry . createId ( \"server.requestCount\" ); // Create a timer for tracking the latency. The reference can be held onto // to avoid additional lookup cost in critical paths. requestLatency = registry . timer ( \"server.requestLatency\" ); // Create a distribution summary meter for tracking the response sizes. responseSizes = registry . distributionSummary ( \"server.responseSizes\" ); // Gauge type that can be sampled. In this case it will invoke the // specified method via reflection to get the value. The registry will // keep a weak reference to the object passed in so that registration will // not prevent garbage collection of the server object. registry . methodValue ( \"server.numConnections\" , this , \"getNumConnections\" ); } public Response handle ( Request req ) { final long s = System . nanoTime (); requestLatency . record (() -> { try { Response res = doSomething ( req ); // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a ConcurrentHashMap. // However, it is more expensive than having a local variable seti // to the counter. final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , res . status ()); registry . counter ( cntId ). increment (); responseSizes . record ( res . body (). size ()); return res ; } catch ( Exception e ) { final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , \"exception\" ) . withTag ( \"error\" , e . getClass (). getSimpleName ()); registry . counter ( cntId ). increment (); throw e ; } }); } public int getNumConnections () { // however we determine the current number of connections on the server } } Netflix Integration \u00b6 When running at Netflix, use the atlas-client library to enable transferring the instrumented data to Atlas . See the appropriate section for the type of project you are working on: Libraries Applications , specifically standalone apps using Guice or Governator directly. Base Server Libraries \u00b6 For libraries, the only dependency that should be needed is: com.netflix.spectator:spectator-api:0.101.0 The bindings to integrate internally should be included with the application. In your code, just inject a Registry , e.g.: public class Foo { @Inject public Foo ( Registry registry ) { ... } ... } See the testing docs for more information about creating a binding to use with tests. Libraries should not install SpectatorModule . The bindings to use for the Registry should be determined by the application that is using the library. Think of it as being like slf4j where logging configuration is up to the end-user, not the library owner. When creating a Guice module for your library, you may want to avoid binding errors if the end-user has not provided a binding for the Spectator registry. This can be done by using optional injections inside of the module, for example: // Sample library class public class MyLib { Registry registry ; @Inject public MyLib ( Registry registry ) { this . registry = registry ; } } // Guice module to configure the library and setup the bindings public class MyLibModule extends AbstractModule { private static final Logger LOGGER = LoggerFactory . getLogger ( MyLibModule . class ); @Override protected void configure () { } @Provides private MyLib provideMyLib ( OptionalInjections opts ) { return new MyLib ( opts . registry ()); } private static class OptionalInjections { @Inject ( optional = true ) private Registry registry ; Registry registry () { if ( registry == null ) { LOGGER . warn ( \"no spectator registry has been bound, so using noop implementation\" ); registry = new NoopRegistry (); } return registry ; } } } Applications \u00b6 Applications should include a dependency on the atlas-client plugin: netflix:atlas-client:latest.release Note this is an internal-only library with configs specific to the Netflix environments. It is assumed you are using Nebula so that internal Maven repositories are available for your build. When configuring with Governator, specify the AtlasModule : Injector injector = LifecycleInjector . builder () . withModules ( new AtlasModule ()) . build () . createInjector (); The Registry binding will then be available for injection as shown in the libraries section . The Insight libraries do not use any Governator or Guice specific features. It is possible to use Guice or other dependency injection frameworks directly with the following caveats: Some of the libraries use the @PostConstruct and @PreDestroy annotations for managing lifecycle. Governator adds lifecycle management and many other features on top of Guice and is the recommended way. For more minimalist support of just the lifecycle annotations on top of Guice, see iep-guice . The bindings and configuration necessary to run correctly with the internal setup are only supported as Guice modules. If you are trying to use some other dependency injection framework, then you will be responsible for either finding a way to leverage the Guice module in that framework or recreating those bindings and maintaining them as things change. It is not a paved path. Base Server \u00b6 If using base-server , then you will get the Spectator and Atlas bindings automatically.","title":"Usage"},{"location":"spectator/lang/java/usage/#project","text":"Source Javadoc Product Lifecycle: GA Requirements: Java >= 8","title":"Project"},{"location":"spectator/lang/java/usage/#install-library","text":"Depend on the API library, which is available in Maven Central . The only transitive dependency is slf4j . For Gradle, the dependency is specified as follows: dependencies { compile \"com.netflix.spectator:spectator-api:0.101.0\" } Pick a Registry to bind, when initializing the application. If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/java/usage/#instrumenting-code","text":"Suppose we have a server and we want to keep track of: Number of requests received with dimensions for breaking down by status code, country, and the exception type if the request fails in an unexpected way. Latency for handling requests. Summary of the response sizes. Current number of active connections on the server. Here is some sample code that does that: // In the application initialization setup a registry Registry registry = new DefaultRegistry (); Server s = new Server ( registry ); public class Server { private final Registry registry ; private final Id requestCountId ; private final Timer requestLatency ; private final DistributionSummary responseSizes ; @Inject public Server ( Registry registry ) { this . registry = registry ; // Create a base id for the request count. The id will get refined with // additional dimensions when we receive a request. requestCountId = registry . createId ( \"server.requestCount\" ); // Create a timer for tracking the latency. The reference can be held onto // to avoid additional lookup cost in critical paths. requestLatency = registry . timer ( \"server.requestLatency\" ); // Create a distribution summary meter for tracking the response sizes. responseSizes = registry . distributionSummary ( \"server.responseSizes\" ); // Gauge type that can be sampled. In this case it will invoke the // specified method via reflection to get the value. The registry will // keep a weak reference to the object passed in so that registration will // not prevent garbage collection of the server object. registry . methodValue ( \"server.numConnections\" , this , \"getNumConnections\" ); } public Response handle ( Request req ) { final long s = System . nanoTime (); requestLatency . record (() -> { try { Response res = doSomething ( req ); // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a ConcurrentHashMap. // However, it is more expensive than having a local variable seti // to the counter. final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , res . status ()); registry . counter ( cntId ). increment (); responseSizes . record ( res . body (). size ()); return res ; } catch ( Exception e ) { final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , \"exception\" ) . withTag ( \"error\" , e . getClass (). getSimpleName ()); registry . counter ( cntId ). increment (); throw e ; } }); } public int getNumConnections () { // however we determine the current number of connections on the server } }","title":"Instrumenting Code"},{"location":"spectator/lang/java/usage/#netflix-integration","text":"When running at Netflix, use the atlas-client library to enable transferring the instrumented data to Atlas . See the appropriate section for the type of project you are working on: Libraries Applications , specifically standalone apps using Guice or Governator directly. Base Server","title":"Netflix Integration"},{"location":"spectator/lang/java/usage/#libraries","text":"For libraries, the only dependency that should be needed is: com.netflix.spectator:spectator-api:0.101.0 The bindings to integrate internally should be included with the application. In your code, just inject a Registry , e.g.: public class Foo { @Inject public Foo ( Registry registry ) { ... } ... } See the testing docs for more information about creating a binding to use with tests. Libraries should not install SpectatorModule . The bindings to use for the Registry should be determined by the application that is using the library. Think of it as being like slf4j where logging configuration is up to the end-user, not the library owner. When creating a Guice module for your library, you may want to avoid binding errors if the end-user has not provided a binding for the Spectator registry. This can be done by using optional injections inside of the module, for example: // Sample library class public class MyLib { Registry registry ; @Inject public MyLib ( Registry registry ) { this . registry = registry ; } } // Guice module to configure the library and setup the bindings public class MyLibModule extends AbstractModule { private static final Logger LOGGER = LoggerFactory . getLogger ( MyLibModule . class ); @Override protected void configure () { } @Provides private MyLib provideMyLib ( OptionalInjections opts ) { return new MyLib ( opts . registry ()); } private static class OptionalInjections { @Inject ( optional = true ) private Registry registry ; Registry registry () { if ( registry == null ) { LOGGER . warn ( \"no spectator registry has been bound, so using noop implementation\" ); registry = new NoopRegistry (); } return registry ; } } }","title":"Libraries"},{"location":"spectator/lang/java/usage/#applications","text":"Applications should include a dependency on the atlas-client plugin: netflix:atlas-client:latest.release Note this is an internal-only library with configs specific to the Netflix environments. It is assumed you are using Nebula so that internal Maven repositories are available for your build. When configuring with Governator, specify the AtlasModule : Injector injector = LifecycleInjector . builder () . withModules ( new AtlasModule ()) . build () . createInjector (); The Registry binding will then be available for injection as shown in the libraries section . The Insight libraries do not use any Governator or Guice specific features. It is possible to use Guice or other dependency injection frameworks directly with the following caveats: Some of the libraries use the @PostConstruct and @PreDestroy annotations for managing lifecycle. Governator adds lifecycle management and many other features on top of Guice and is the recommended way. For more minimalist support of just the lifecycle annotations on top of Guice, see iep-guice . The bindings and configuration necessary to run correctly with the internal setup are only supported as Guice modules. If you are trying to use some other dependency injection framework, then you will be responsible for either finding a way to leverage the Guice module in that framework or recreating those bindings and maintaining them as things change. It is not a paved path.","title":"Applications"},{"location":"spectator/lang/java/usage/#base-server","text":"If using base-server , then you will get the Spectator and Atlas bindings automatically.","title":"Base Server"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/","text":"Buffer Pools \u00b6 Buffer pools, such as direct byte buffers, can be monitored at a high level using the BufferPoolMXBean provided by the JDK. Getting Started \u00b6 To get information about buffer pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ()); Metrics \u00b6 jvm.buffer.count \u00b6 Gauge showing the current number of distinct buffers. Unit: count Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files. jvm.buffer.memoryUsed \u00b6 Gauge showing the current number of bytes used by all buffers. Unit: bytes Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"Buffer Pools"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#buffer-pools","text":"Buffer pools, such as direct byte buffers, can be monitored at a high level using the BufferPoolMXBean provided by the JDK.","title":"Buffer Pools"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#getting-started","text":"To get information about buffer pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ());","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#jvmbuffercount","text":"Gauge showing the current number of distinct buffers. Unit: count Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"jvm.buffer.count"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#jvmbuffermemoryused","text":"Gauge showing the current number of bytes used by all buffers. Unit: bytes Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"jvm.buffer.memoryUsed"},{"location":"spectator/lang/java/ext/jvm-gc-causes/","text":"GC Causes \u00b6 The various GC causes aren't well documented. The list provided here comes from the gcCause.cpp file in the jdk and we include some information on what these mean for the application. System.gc__ \u00b6 Something called System.gc() . If you are seeing this once an hour it is likely related to the RMI GC interval. For more details see: Unexplained System.gc() calls due to Remote Method Invocation (RMI) or explict garbage collections sun.rmi.dgc.client.gcInterval FullGCAlot \u00b6 Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+FullGCALot , that will trigger a full GC at a regular interval for testing purposes. ScavengeAlot \u00b6 Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+ScavengeALot , that will trigger a minor GC at a regular interval for testing purposes. Allocation_Profiler \u00b6 Prior to java 8 you would see this if running with the -Xaprof setting. It would be triggered just before the jvm exits. The -Xaprof option was removed in java 8. JvmtiEnv_ForceGarbageCollection \u00b6 Something called the JVM tool interface function ForceGarbageCollection . Look at the -agentlib param to java to see what agents are configured. GCLocker_Initiated_GC \u00b6 The GC locker prevents GC from occurring when JNI code is in a critical region . If GC is needed while a thread is in a critical region, then it will allow them to complete, i.e. call the corresponding release function. Other threads will not be permitted to enter a critical region. Once all threads are out of critical regions a GC event will be triggered. Heap_Inspection_Initiated_GC \u00b6 GC was initiated by an inspection operation on the heap. For example you can trigger this with jmap : $ jmap -histo:live <pid> Heap_Dump_Initiated_GC \u00b6 GC was initiated before dumping the heap. For example you can trigger this with jmap : $ jmap -dump:live,format=b,file=heap.out <pid> Another common example would be clicking the Heap Dump button on the Monitor tab in jvisualvm . WhiteBox_Initiated_Young_GC \u00b6 Most likely you'll never see this value. Used for testing hotspot, it indicates something called sun.hotspot.WhiteBox.youngGC() . No_GC \u00b6 Used for CMS to indicate concurrent phases. Allocation_Failure \u00b6 Usually this means that there is an allocation request that is bigger than the available space in young generation and will typically be associated with a minor GC. For G1 this will likely be a major GC and it is more common to see G1_Evacuation_Pause for routine minor collections. On linux the jvm will trigger a GC if the kernel indicates there isn't much memory left via mem_notify . Tenured_Generation_Full \u00b6 Not used? Permanent_Generation_Full \u00b6 Triggered as a result of an allocation failure in PermGen . Pre java 8. Metadata_GC_Threshold \u00b6 Triggered as a result of an allocation failure in Metaspace . Metaspace replaced PermGen was added in java 8. CMS_Generation_Full \u00b6 Not used? CMS_Initial_Mark \u00b6 Initial mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . CMS_Final_Remark \u00b6 Remark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . CMS_Concurrent_Mark \u00b6 Concurrent mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . Old_Generation_Expanded_On_Last_Scavenge \u00b6 Not used? Old_Generation_Too_Full_To_Scavenge \u00b6 Not used? Ergonomics \u00b6 This indicates you are using the adaptive size policy, -XX:+UseAdaptiveSizePolicy and is on by default for recent versions, with the parallel collector ( -XX:+UseParallelGC ). For more details see The Why of GC Ergonomics . G1_Evacuation_Pause \u00b6 An evacuation pause is the most common young gen cause for G1 and indicates that it is copying live objects from one set of regions, young and sometimes young + old, to another set of regions. For more details see Understanding G1 GC Logs . G1_Humongous_Allocation \u00b6 A humongous allocation is one where the size is greater than 50% of the G1 region size. Before a humongous allocation the jvm checks if it should do a routine evacuation pause without regard to the actual allocation size, but if triggered due to this check the cause will be listed as humongous allocation. This cause is also used for any collections used to free up enough space for the allocation. Last_ditch_collection \u00b6 For perm gen (java 7 or earlier) and metaspace (java 8+) a last ditch collection will be triggered if an allocation fails and the memory pool cannot be expanded. ILLEGAL_VALUE_- last_gc_cause -_ILLEGAL_VALUE \u00b6 Included for completeness, but you should never see this value. unknown_GCCause \u00b6 Included for completeness, but you should never see this value.","title":"GC Causes"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#gc-causes","text":"The various GC causes aren't well documented. The list provided here comes from the gcCause.cpp file in the jdk and we include some information on what these mean for the application.","title":"GC Causes"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#systemgc__","text":"Something called System.gc() . If you are seeing this once an hour it is likely related to the RMI GC interval. For more details see: Unexplained System.gc() calls due to Remote Method Invocation (RMI) or explict garbage collections sun.rmi.dgc.client.gcInterval","title":"System.gc__"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#fullgcalot","text":"Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+FullGCALot , that will trigger a full GC at a regular interval for testing purposes.","title":"FullGCAlot"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#scavengealot","text":"Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+ScavengeALot , that will trigger a minor GC at a regular interval for testing purposes.","title":"ScavengeAlot"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#allocation_profiler","text":"Prior to java 8 you would see this if running with the -Xaprof setting. It would be triggered just before the jvm exits. The -Xaprof option was removed in java 8.","title":"Allocation_Profiler"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#jvmtienv_forcegarbagecollection","text":"Something called the JVM tool interface function ForceGarbageCollection . Look at the -agentlib param to java to see what agents are configured.","title":"JvmtiEnv_ForceGarbageCollection"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#gclocker_initiated_gc","text":"The GC locker prevents GC from occurring when JNI code is in a critical region . If GC is needed while a thread is in a critical region, then it will allow them to complete, i.e. call the corresponding release function. Other threads will not be permitted to enter a critical region. Once all threads are out of critical regions a GC event will be triggered.","title":"GCLocker_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#heap_inspection_initiated_gc","text":"GC was initiated by an inspection operation on the heap. For example you can trigger this with jmap : $ jmap -histo:live <pid>","title":"Heap_Inspection_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#heap_dump_initiated_gc","text":"GC was initiated before dumping the heap. For example you can trigger this with jmap : $ jmap -dump:live,format=b,file=heap.out <pid> Another common example would be clicking the Heap Dump button on the Monitor tab in jvisualvm .","title":"Heap_Dump_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#whitebox_initiated_young_gc","text":"Most likely you'll never see this value. Used for testing hotspot, it indicates something called sun.hotspot.WhiteBox.youngGC() .","title":"WhiteBox_Initiated_Young_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#no_gc","text":"Used for CMS to indicate concurrent phases.","title":"No_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#allocation_failure","text":"Usually this means that there is an allocation request that is bigger than the available space in young generation and will typically be associated with a minor GC. For G1 this will likely be a major GC and it is more common to see G1_Evacuation_Pause for routine minor collections. On linux the jvm will trigger a GC if the kernel indicates there isn't much memory left via mem_notify .","title":"Allocation_Failure"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#tenured_generation_full","text":"Not used?","title":"Tenured_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#permanent_generation_full","text":"Triggered as a result of an allocation failure in PermGen . Pre java 8.","title":"Permanent_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#metadata_gc_threshold","text":"Triggered as a result of an allocation failure in Metaspace . Metaspace replaced PermGen was added in java 8.","title":"Metadata_GC_Threshold"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_generation_full","text":"Not used?","title":"CMS_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_initial_mark","text":"Initial mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Initial_Mark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_final_remark","text":"Remark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Final_Remark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_concurrent_mark","text":"Concurrent mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Concurrent_Mark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#old_generation_expanded_on_last_scavenge","text":"Not used?","title":"Old_Generation_Expanded_On_Last_Scavenge"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#old_generation_too_full_to_scavenge","text":"Not used?","title":"Old_Generation_Too_Full_To_Scavenge"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#ergonomics","text":"This indicates you are using the adaptive size policy, -XX:+UseAdaptiveSizePolicy and is on by default for recent versions, with the parallel collector ( -XX:+UseParallelGC ). For more details see The Why of GC Ergonomics .","title":"Ergonomics"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#g1_evacuation_pause","text":"An evacuation pause is the most common young gen cause for G1 and indicates that it is copying live objects from one set of regions, young and sometimes young + old, to another set of regions. For more details see Understanding G1 GC Logs .","title":"G1_Evacuation_Pause"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#g1_humongous_allocation","text":"A humongous allocation is one where the size is greater than 50% of the G1 region size. Before a humongous allocation the jvm checks if it should do a routine evacuation pause without regard to the actual allocation size, but if triggered due to this check the cause will be listed as humongous allocation. This cause is also used for any collections used to free up enough space for the allocation.","title":"G1_Humongous_Allocation"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#last_ditch_collection","text":"For perm gen (java 7 or earlier) and metaspace (java 8+) a last ditch collection will be triggered if an allocation fails and the memory pool cannot be expanded.","title":"Last_ditch_collection"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#illegal_value_-last_gc_cause-_illegal_value","text":"Included for completeness, but you should never see this value.","title":"ILLEGAL_VALUE_-last_gc_cause-_ILLEGAL_VALUE"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#unknown_gccause","text":"Included for completeness, but you should never see this value.","title":"unknown_GCCause"},{"location":"spectator/lang/java/ext/jvm-gc/","text":"Garbage Collection \u00b6 The GC module registers with the notification emitter of the GarbageCollectorMXBean to provide some basic GC logging and metrics. Getting started Logging Metrics Alerting Getting Started \u00b6 For using it internally at Netflix, see the Java Usage guide, otherwise keep reading this section. Requirements \u00b6 This library relies on the notification emitter added in 7u4, but there are known issues prior to 7u40. There is also a regression impacting Java 9 and higher, see #502 and JDK-8196325 for more information. For G1, it is recommended to be on the latest version available. Dependencies \u00b6 com.netflix.spectator:spectator-ext-gc:0.101.0 Start Reporting \u00b6 Then in the initialization for the application: import com.netflix.spectator.gc.GcLogger ; ... // Keep a single instance of the logger GcLogger gc = new GcLogger (); gc . start ( null ); Logging \u00b6 After GC events, an INFO level log message will get reported using slf4j. This makes it easy to see GC events in the context of other log messages for the application. The logger name is com.netflix.spectator.gc.GcLogger and the message will look like: ${GC_TYPE}: ${COLLECTOR_NAME}, id=${N}, at=${START_TIME}, duration=${T}ms, cause=[${CAUSE}], ${TOTAL_USAGE_BEFORE} => ${TOTAL_USAGE_AFTER} / ${MAX_SIZE} (${PERCENT_USAGE_BEFORE} => ${PERCENT_USAGE_AFTER}) The id can be used to verify events were not skipped or correlate with other sources like detailed GC logs. See GC causes for more details on the possible causes. Sample: 2014-08-31 02:02:24,724 INFO [com.netflix.spectator.gc.GcLogger] YOUNG: ParNew, id=5281, at=Sun Aug 31 02:02:24 UTC 2014, duration=2ms, cause=[Allocation Failure], 0.4G => 0.3G / 1.8G (24.3% => 16.6%) Metrics \u00b6 jvm.gc.allocationRate \u00b6 The allocation rate measures how fast the application is allocating memory. It is a counter that is incremented after a GC event by the amount youngGen.sizeBeforeGC . Technically, right now it is: youngGen.sizeBeforeGC - youngGen.sizeAfterGC However, youngGen.sizeAfterGC should be 0 and thus the size of young gen before the GC is the amount allocated since the previous GC event. Unit: bytes/second Dimensions: n/a jvm.gc.promotionRate \u00b6 The promotion rate measures how fast data is being moved from young generation into the old generation. It is a counter that is incremented after a GC event by the amount: abs(oldGen.sizeAfterGC - oldGen.sizeBeforeGC) Unit: bytes/second Dimensions: n/a jvm.gc.liveDataSize \u00b6 The live data size is the size of the old generation after a major GC. The image below shows how the live data size view compares to a metric showing the current size of the memory pool: Unit: bytes Dimensions: n/a jvm.gc.maxDataSize \u00b6 Maximum size for the old generation. Primary use-case is for gaining perspective on the the live data size. Unit: bytes Dimensions: n/a jvm.gc.pause \u00b6 Pause time for a GC event. All of the values reported are stop the world pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page. jvm.gc.concurrentPhaseTime \u00b6 Time spent in concurrent phases of CMS pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page. Alerting \u00b6 This section assumes the data is available in Atlas , but users of other systems should be able to take the idea and make it work. For all of these alerts it is recommended to check them on instance. At Netflix that can be done by selecting the option in alert ui: Max Pause Time \u00b6 Example to trigger an alert if the pause time exceeds 500 milliseconds: name,jvm.gc.pause,:eq, statistic,max,:eq, :and, :max,(,cause,),:by, 0.5,:gt, $cause,:legend Heap Pressure \u00b6 Example to trigger an alert if the live data size is over 70% of the heap: name,jvm.gc.liveDataSize,:eq,:max, name,jvm.gc.maxDataSize,:eq,:max, :div,100,:mul, 70,:gt, percentUsed,:legend","title":"Garbage Collection"},{"location":"spectator/lang/java/ext/jvm-gc/#garbage-collection","text":"The GC module registers with the notification emitter of the GarbageCollectorMXBean to provide some basic GC logging and metrics. Getting started Logging Metrics Alerting","title":"Garbage Collection"},{"location":"spectator/lang/java/ext/jvm-gc/#getting-started","text":"For using it internally at Netflix, see the Java Usage guide, otherwise keep reading this section.","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-gc/#requirements","text":"This library relies on the notification emitter added in 7u4, but there are known issues prior to 7u40. There is also a regression impacting Java 9 and higher, see #502 and JDK-8196325 for more information. For G1, it is recommended to be on the latest version available.","title":"Requirements"},{"location":"spectator/lang/java/ext/jvm-gc/#dependencies","text":"com.netflix.spectator:spectator-ext-gc:0.101.0","title":"Dependencies"},{"location":"spectator/lang/java/ext/jvm-gc/#start-reporting","text":"Then in the initialization for the application: import com.netflix.spectator.gc.GcLogger ; ... // Keep a single instance of the logger GcLogger gc = new GcLogger (); gc . start ( null );","title":"Start Reporting"},{"location":"spectator/lang/java/ext/jvm-gc/#logging","text":"After GC events, an INFO level log message will get reported using slf4j. This makes it easy to see GC events in the context of other log messages for the application. The logger name is com.netflix.spectator.gc.GcLogger and the message will look like: ${GC_TYPE}: ${COLLECTOR_NAME}, id=${N}, at=${START_TIME}, duration=${T}ms, cause=[${CAUSE}], ${TOTAL_USAGE_BEFORE} => ${TOTAL_USAGE_AFTER} / ${MAX_SIZE} (${PERCENT_USAGE_BEFORE} => ${PERCENT_USAGE_AFTER}) The id can be used to verify events were not skipped or correlate with other sources like detailed GC logs. See GC causes for more details on the possible causes. Sample: 2014-08-31 02:02:24,724 INFO [com.netflix.spectator.gc.GcLogger] YOUNG: ParNew, id=5281, at=Sun Aug 31 02:02:24 UTC 2014, duration=2ms, cause=[Allocation Failure], 0.4G => 0.3G / 1.8G (24.3% => 16.6%)","title":"Logging"},{"location":"spectator/lang/java/ext/jvm-gc/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcallocationrate","text":"The allocation rate measures how fast the application is allocating memory. It is a counter that is incremented after a GC event by the amount youngGen.sizeBeforeGC . Technically, right now it is: youngGen.sizeBeforeGC - youngGen.sizeAfterGC However, youngGen.sizeAfterGC should be 0 and thus the size of young gen before the GC is the amount allocated since the previous GC event. Unit: bytes/second Dimensions: n/a","title":"jvm.gc.allocationRate"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcpromotionrate","text":"The promotion rate measures how fast data is being moved from young generation into the old generation. It is a counter that is incremented after a GC event by the amount: abs(oldGen.sizeAfterGC - oldGen.sizeBeforeGC) Unit: bytes/second Dimensions: n/a","title":"jvm.gc.promotionRate"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgclivedatasize","text":"The live data size is the size of the old generation after a major GC. The image below shows how the live data size view compares to a metric showing the current size of the memory pool: Unit: bytes Dimensions: n/a","title":"jvm.gc.liveDataSize"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcmaxdatasize","text":"Maximum size for the old generation. Primary use-case is for gaining perspective on the the live data size. Unit: bytes Dimensions: n/a","title":"jvm.gc.maxDataSize"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcpause","text":"Pause time for a GC event. All of the values reported are stop the world pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page.","title":"jvm.gc.pause"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcconcurrentphasetime","text":"Time spent in concurrent phases of CMS pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page.","title":"jvm.gc.concurrentPhaseTime"},{"location":"spectator/lang/java/ext/jvm-gc/#alerting","text":"This section assumes the data is available in Atlas , but users of other systems should be able to take the idea and make it work. For all of these alerts it is recommended to check them on instance. At Netflix that can be done by selecting the option in alert ui:","title":"Alerting"},{"location":"spectator/lang/java/ext/jvm-gc/#max-pause-time","text":"Example to trigger an alert if the pause time exceeds 500 milliseconds: name,jvm.gc.pause,:eq, statistic,max,:eq, :and, :max,(,cause,),:by, 0.5,:gt, $cause,:legend","title":"Max Pause Time"},{"location":"spectator/lang/java/ext/jvm-gc/#heap-pressure","text":"Example to trigger an alert if the live data size is over 70% of the heap: name,jvm.gc.liveDataSize,:eq,:max, name,jvm.gc.maxDataSize,:eq,:max, :div,100,:mul, 70,:gt, percentUsed,:legend","title":"Heap Pressure"},{"location":"spectator/lang/java/ext/jvm-memory-pools/","text":"Memory Pools \u00b6 Uses the MemoryPoolMXBean provided by the JDK to monitor the sizes of java memory spaces such as perm gen, eden, old gen, etc. Getting Started \u00b6 To get information about memory pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ()); Metrics \u00b6 jvm.memory.used \u00b6 Gauge reporting the current amount of memory used. For the young and old gen pools this metric will typically have a sawtooth pattern. For alerting or detecting memory pressure the live data size is probably a better option. Unit: bytes Dimensions: see metric dimensions jvm.memory.committed \u00b6 Gauge reporting the current amount of memory committed. From the javadocs , committed is: The amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. Unit: bytes Dimensions: see metric dimensions jvm.memory.max \u00b6 Gauge reporting the max amount of memory that can be used. From the javadocs , max is: The maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used > committed even if used <= max would still be true (for example, when the system is low on virtual memory). Unit: bytes Dimensions: see metric dimensions Metric Dimensions \u00b6 All memory metrics have the following dimensions: id : name of the memory pool being reported. The names of the pools vary depending on the garbage collector algorithm being used. memtype : type of memory. It has two possible values: HEAP and NON_HEAP . For more information see the javadocs for MemoryType .","title":"Memory Pools"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#memory-pools","text":"Uses the MemoryPoolMXBean provided by the JDK to monitor the sizes of java memory spaces such as perm gen, eden, old gen, etc.","title":"Memory Pools"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#getting-started","text":"To get information about memory pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ());","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemoryused","text":"Gauge reporting the current amount of memory used. For the young and old gen pools this metric will typically have a sawtooth pattern. For alerting or detecting memory pressure the live data size is probably a better option. Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.used"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemorycommitted","text":"Gauge reporting the current amount of memory committed. From the javadocs , committed is: The amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.committed"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemorymax","text":"Gauge reporting the max amount of memory that can be used. From the javadocs , max is: The maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used > committed even if used <= max would still be true (for example, when the system is low on virtual memory). Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.max"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#metric-dimensions","text":"All memory metrics have the following dimensions: id : name of the memory pool being reported. The names of the pools vary depending on the garbage collector algorithm being used. memtype : type of memory. It has two possible values: HEAP and NON_HEAP . For more information see the javadocs for MemoryType .","title":"Metric Dimensions"},{"location":"spectator/lang/java/ext/log4j1/","text":"Log4j1 Appender \u00b6 Custom appender for log4j1 to track the number of log messages reported. Note Log4j 1.x has reached end of life and is no longer supported by Apache. This extension is provided for some users that have difficulty moving to a supported version of log4j. Getting Started \u00b6 To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j1:0.101.0 Then in your log4j configuration specify the com.netflix.spectator.log4j.SpectatorAppender . In a properties file it would look something like: log4j.rootLogger=ALL, A1 log4j.appender.A1=com.netflix.spectator.log4j.SpectatorAppender Metrics \u00b6 log4j.numMessages \u00b6 Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: loglevel : standard log level of the events. log4j.numStackTraces \u00b6 Counter for the number of messages with stack traces written to the logs. Unit: messages/second Dimensions: loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"Log4j1 Appender"},{"location":"spectator/lang/java/ext/log4j1/#log4j1-appender","text":"Custom appender for log4j1 to track the number of log messages reported. Note Log4j 1.x has reached end of life and is no longer supported by Apache. This extension is provided for some users that have difficulty moving to a supported version of log4j.","title":"Log4j1 Appender"},{"location":"spectator/lang/java/ext/log4j1/#getting-started","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j1:0.101.0 Then in your log4j configuration specify the com.netflix.spectator.log4j.SpectatorAppender . In a properties file it would look something like: log4j.rootLogger=ALL, A1 log4j.appender.A1=com.netflix.spectator.log4j.SpectatorAppender","title":"Getting Started"},{"location":"spectator/lang/java/ext/log4j1/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/log4j1/#log4jnummessages","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: loglevel : standard log level of the events.","title":"log4j.numMessages"},{"location":"spectator/lang/java/ext/log4j1/#log4jnumstacktraces","text":"Counter for the number of messages with stack traces written to the logs. Unit: messages/second Dimensions: loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"log4j.numStackTraces"},{"location":"spectator/lang/java/ext/log4j2/","text":"Log4j2 Appender \u00b6 Custom appender for log4j2 to track the number of log messages reported. Getting Started \u00b6 To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j2:0.101.0 Then in your application initialization: Registry registry = ... SpectatorAppender . addToRootLogger ( registry , // Registry to use \"spectator\" , // Name for the appender false ); // Should stack traces be ignored? This will add the appender to the root logger and register a listener so it will get re-added if the configuration changes. You can also use the appender by specifying it in the log4j2 configuration, but this will cause some of the loggers in Spectator to get created before log4j is properly initialized and result in some lost log messages. With that caveat in mind, if you need the additional flexibility of using the configuration then specify the Spectator appender: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration monitorInterval= \"5\" status= \"warn\" > <Appenders> <Spectator name= \"root\" /> </Appenders> <Loggers> <Root level= \"debug\" > <AppenderRef ref= \"root\" /> </Root> </Loggers> </Configuration> Metrics \u00b6 log4j.numMessages \u00b6 Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. log4j.numStackTraces \u00b6 Counter for the number of messages with stack traces written to the logs. This will only be collected if the ignoreExceptions flag is set to false for the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"Log4j2 Appender"},{"location":"spectator/lang/java/ext/log4j2/#log4j2-appender","text":"Custom appender for log4j2 to track the number of log messages reported.","title":"Log4j2 Appender"},{"location":"spectator/lang/java/ext/log4j2/#getting-started","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j2:0.101.0 Then in your application initialization: Registry registry = ... SpectatorAppender . addToRootLogger ( registry , // Registry to use \"spectator\" , // Name for the appender false ); // Should stack traces be ignored? This will add the appender to the root logger and register a listener so it will get re-added if the configuration changes. You can also use the appender by specifying it in the log4j2 configuration, but this will cause some of the loggers in Spectator to get created before log4j is properly initialized and result in some lost log messages. With that caveat in mind, if you need the additional flexibility of using the configuration then specify the Spectator appender: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration monitorInterval= \"5\" status= \"warn\" > <Appenders> <Spectator name= \"root\" /> </Appenders> <Loggers> <Root level= \"debug\" > <AppenderRef ref= \"root\" /> </Root> </Loggers> </Configuration>","title":"Getting Started"},{"location":"spectator/lang/java/ext/log4j2/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/log4j2/#log4jnummessages","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events.","title":"log4j.numMessages"},{"location":"spectator/lang/java/ext/log4j2/#log4jnumstacktraces","text":"Counter for the number of messages with stack traces written to the logs. This will only be collected if the ignoreExceptions flag is set to false for the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"log4j.numStackTraces"},{"location":"spectator/lang/java/ext/placeholders/","text":"Placeholders \u00b6 The placeholders extension allows for identifiers to be created with dimensions that will get filled in based on the context when an activity occurs. The primary use-cases are to support: Optional dimensions that can be conditionally enabled. Pulling dimensions from another context such as a thread local store. This can make it is easier to share the across various parts of the code. Dependencies \u00b6 To use the placeholders support add a dependency on: com.netflix.spectator:spectator-ext-placeholders:0.101.0 Usage \u00b6 Placeholder support is available for activity based types including counters , timers , and distribution summaries . To get started create a PlaceholderFactory from the registry: PlaceholderFactory factory = PlaceholderFactory . from ( registry ); Then use the factory to create an identifier using a TagFactory to dynamically fetch the value for a given dimension when some activity occurs. Suppose we want to use a dynamic configuration library such as Archaius to conditionally enable a dimension with high cardinality: public class Server { private final Context context ; private final Counter rps ; public Server ( Context context , PropertyFactory props , Registry registry ) { this . context = context ; // Property that can be dynamically updated to indicate whether or not // detailed dimensions should be added to metrics. Property < Boolean > enabled = props . getProperty ( \"server.detailedMetricsEnabled\" ) . asBoolean ( false ); // Factory for creating instances of the counter using placeholders PlaceholderFactory factory = PlaceholderFactory . from ( registry ); // Create the underlying id with 4 possible dimensions: // * method and status - low cardinality and always added if available // in the context. // * geo and device - high cardinality and only available if the property // to enable detailed metrics is set to true. PlaceholderId rpsId = factory . createId ( \"server.requests\" ) . withTagFactory ( TagFactory . from ( \"method\" , context :: getMethod )) . withTagFactory ( TagFactory . from ( \"status\" , context :: getStatus )) . withTagFactory ( new DetailedDimension ( \"geo\" , enabled , context :: getGeo )) . withTagFactory ( new DetailedDimension ( \"device\" , enabled , context :: getDevice )); rps = factory . counter ( rpsId ); } public Response handle ( Request request ) { fillInContext ( request ); Response response = process ( request ); fillInContext ( response ); // Update the counter, the placeholders will be resolved when the activity, in // this case the increment is called. rps . increment (); return response ; } // Tag factory that can be controlled with an enabled property. private static class DetailedDimension implements TagFactory { private final String name ; private final Supplier < String > valueFunc ; DetailedDimension ( String name , Property < Boolean > enabled , Supplier < String > valueFunc ) { this . name = name ; this . enabled = enabled ; this . valueFunc = valueFunc ; } @Override public String name () { return name ; } @Override public Tag createTag () { return enabled . get () ? new BasicTag ( name , valueFunc . get ()) : null ; } } }","title":"Placeholders"},{"location":"spectator/lang/java/ext/placeholders/#placeholders","text":"The placeholders extension allows for identifiers to be created with dimensions that will get filled in based on the context when an activity occurs. The primary use-cases are to support: Optional dimensions that can be conditionally enabled. Pulling dimensions from another context such as a thread local store. This can make it is easier to share the across various parts of the code.","title":"Placeholders"},{"location":"spectator/lang/java/ext/placeholders/#dependencies","text":"To use the placeholders support add a dependency on: com.netflix.spectator:spectator-ext-placeholders:0.101.0","title":"Dependencies"},{"location":"spectator/lang/java/ext/placeholders/#usage","text":"Placeholder support is available for activity based types including counters , timers , and distribution summaries . To get started create a PlaceholderFactory from the registry: PlaceholderFactory factory = PlaceholderFactory . from ( registry ); Then use the factory to create an identifier using a TagFactory to dynamically fetch the value for a given dimension when some activity occurs. Suppose we want to use a dynamic configuration library such as Archaius to conditionally enable a dimension with high cardinality: public class Server { private final Context context ; private final Counter rps ; public Server ( Context context , PropertyFactory props , Registry registry ) { this . context = context ; // Property that can be dynamically updated to indicate whether or not // detailed dimensions should be added to metrics. Property < Boolean > enabled = props . getProperty ( \"server.detailedMetricsEnabled\" ) . asBoolean ( false ); // Factory for creating instances of the counter using placeholders PlaceholderFactory factory = PlaceholderFactory . from ( registry ); // Create the underlying id with 4 possible dimensions: // * method and status - low cardinality and always added if available // in the context. // * geo and device - high cardinality and only available if the property // to enable detailed metrics is set to true. PlaceholderId rpsId = factory . createId ( \"server.requests\" ) . withTagFactory ( TagFactory . from ( \"method\" , context :: getMethod )) . withTagFactory ( TagFactory . from ( \"status\" , context :: getStatus )) . withTagFactory ( new DetailedDimension ( \"geo\" , enabled , context :: getGeo )) . withTagFactory ( new DetailedDimension ( \"device\" , enabled , context :: getDevice )); rps = factory . counter ( rpsId ); } public Response handle ( Request request ) { fillInContext ( request ); Response response = process ( request ); fillInContext ( response ); // Update the counter, the placeholders will be resolved when the activity, in // this case the increment is called. rps . increment (); return response ; } // Tag factory that can be controlled with an enabled property. private static class DetailedDimension implements TagFactory { private final String name ; private final Supplier < String > valueFunc ; DetailedDimension ( String name , Property < Boolean > enabled , Supplier < String > valueFunc ) { this . name = name ; this . enabled = enabled ; this . valueFunc = valueFunc ; } @Override public String name () { return name ; } @Override public Tag createTag () { return enabled . get () ? new BasicTag ( name , valueFunc . get ()) : null ; } } }","title":"Usage"},{"location":"spectator/lang/java/ext/thread-pools/","text":"Thread Pools \u00b6 Java's ThreadPoolExecutor exposes several properties that are useful to monitor to assess the health, performance, and configuration of the pool. Getting Started \u00b6 To report thread pool metrics, one can attach a ThreadPoolMonitor in the following manner: import com.netflix.spectator.api.patterns.ThreadPoolMonitor ; ThreadPoolMonitor . attach ( registry , myThreadPoolExecutor , \"my-thread-pool\" ); The thread pool's properties will be polled regularly in the background and will report metrics to the provided registry. The third parameter will be added to each metric as an id dimension, if provided. However, if the value is null or an empty string, then a default will be used as the id . Metrics \u00b6 threadpool.taskCount \u00b6 Counter of the total number of tasks that have been scheduled. Unit: tasks/second Data Source: ThreadPoolExecutor#getTaskCount() threadpool.completedTaskCount \u00b6 Counter of the total number of tasks that have completed. Unit: tasks/second Data Source: ThreadPoolExecutor#getCompletedTaskCount() threadpool.currentThreadsBusy \u00b6 Gauge showing the current number of threads actively doing work. Unit: count Data Source: ThreadPoolExecutor#getActiveCount() threadpool.maxThreads \u00b6 Gauge showing the current maximum number of threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getMaximumPoolSize() threadpool.poolSize \u00b6 Gauge showing the current size of the pool. Unit: count Data Source: ThreadPoolExecutor#getPoolSize() threadpool.corePoolSize \u00b6 Gauge showing the current maximum number of core threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getCorePoolSize() threadpool.queueSize \u00b6 Gauge showing the current number of threads queued for execution. Unit: count Data Source: ThreadPoolExecutor#getQueue().size()","title":"Thread Pools"},{"location":"spectator/lang/java/ext/thread-pools/#thread-pools","text":"Java's ThreadPoolExecutor exposes several properties that are useful to monitor to assess the health, performance, and configuration of the pool.","title":"Thread Pools"},{"location":"spectator/lang/java/ext/thread-pools/#getting-started","text":"To report thread pool metrics, one can attach a ThreadPoolMonitor in the following manner: import com.netflix.spectator.api.patterns.ThreadPoolMonitor ; ThreadPoolMonitor . attach ( registry , myThreadPoolExecutor , \"my-thread-pool\" ); The thread pool's properties will be polled regularly in the background and will report metrics to the provided registry. The third parameter will be added to each metric as an id dimension, if provided. However, if the value is null or an empty string, then a default will be used as the id .","title":"Getting Started"},{"location":"spectator/lang/java/ext/thread-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/thread-pools/#threadpooltaskcount","text":"Counter of the total number of tasks that have been scheduled. Unit: tasks/second Data Source: ThreadPoolExecutor#getTaskCount()","title":"threadpool.taskCount"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcompletedtaskcount","text":"Counter of the total number of tasks that have completed. Unit: tasks/second Data Source: ThreadPoolExecutor#getCompletedTaskCount()","title":"threadpool.completedTaskCount"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcurrentthreadsbusy","text":"Gauge showing the current number of threads actively doing work. Unit: count Data Source: ThreadPoolExecutor#getActiveCount()","title":"threadpool.currentThreadsBusy"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolmaxthreads","text":"Gauge showing the current maximum number of threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getMaximumPoolSize()","title":"threadpool.maxThreads"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolpoolsize","text":"Gauge showing the current size of the pool. Unit: count Data Source: ThreadPoolExecutor#getPoolSize()","title":"threadpool.poolSize"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcorepoolsize","text":"Gauge showing the current maximum number of core threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getCorePoolSize()","title":"threadpool.corePoolSize"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolqueuesize","text":"Gauge showing the current number of threads queued for execution. Unit: count Data Source: ThreadPoolExecutor#getQueue().size()","title":"threadpool.queueSize"},{"location":"spectator/lang/java/meters/counter/","text":"Java Counters \u00b6 Counters are created using the Registry , which is be setup as part of application initialization. For example: public class Queue { private final Counter insertCounter ; private final Counter removeCounter ; private final QueueImpl impl ; @Inject public Queue ( Registry registry ) { insertCounter = registry . counter ( \"queue.insert\" ); removeCounter = registry . counter ( \"queue.remove\" ); impl = new QueueImpl (); } Then call increment when an event occurs: public void insert ( Object obj ) { insertCounter . increment (); impl . insert ( obj ); } public Object remove () { if ( impl . nonEmpty ()) { removeCounter . increment (); return impl . remove (); } else { return null ; } } Optionally, an amount can be passed in when calling increment. This is useful when a collection of events happen together. public void insertAll ( Collection < Object > objs ) { insertCounter . increment ( objs . size ()); impl . insertAll ( objs ); } }","title":"Counters"},{"location":"spectator/lang/java/meters/counter/#java-counters","text":"Counters are created using the Registry , which is be setup as part of application initialization. For example: public class Queue { private final Counter insertCounter ; private final Counter removeCounter ; private final QueueImpl impl ; @Inject public Queue ( Registry registry ) { insertCounter = registry . counter ( \"queue.insert\" ); removeCounter = registry . counter ( \"queue.remove\" ); impl = new QueueImpl (); } Then call increment when an event occurs: public void insert ( Object obj ) { insertCounter . increment (); impl . insert ( obj ); } public Object remove () { if ( impl . nonEmpty ()) { removeCounter . increment (); return impl . remove (); } else { return null ; } } Optionally, an amount can be passed in when calling increment. This is useful when a collection of events happen together. public void insertAll ( Collection < Object > objs ) { insertCounter . increment ( objs . size ()); impl . insertAll ( objs ); } }","title":"Java Counters"},{"location":"spectator/lang/java/meters/dist-summary/","text":"Java Distribution Summaries \u00b6 Distribution Summaries are created using the Registry , which will be setup as part of application initialization. For example: public class Server { private final DistributionSummary requestSize ; @Inject public Server ( Registry registry ) { requestSize = registry . distributionSummary ( \"server.requestSize\" ); } Then call record when an event occurs: public Response handle ( Request request ) { requestSize . record ( request . sizeInBytes ()); } }","title":"Distribution Summaries"},{"location":"spectator/lang/java/meters/dist-summary/#java-distribution-summaries","text":"Distribution Summaries are created using the Registry , which will be setup as part of application initialization. For example: public class Server { private final DistributionSummary requestSize ; @Inject public Server ( Registry registry ) { requestSize = registry . distributionSummary ( \"server.requestSize\" ); } Then call record when an event occurs: public Response handle ( Request request ) { requestSize . record ( request . sizeInBytes ()); } }","title":"Java Distribution Summaries"},{"location":"spectator/lang/java/meters/gauge/","text":"Java Gauges \u00b6 Polled Gauges \u00b6 The most common use of Gauges is by registering a hook with Spectator, so that it will poll the values in the background. This is done by using the PolledMeter helper class. A Polled Gauge is registered by passing in an id, a reference to the object, and a function to get or compute a numeric value based on the object. Note that a Gauge should only be registered once, not on each update. Consider this example of a web server tracking the number of connections: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; public HttpServer ( Registry registry ) { numConnections = PolledMeter . using ( registry ) . withName ( \"server.numConnections\" ) . monitorValue ( new AtomicInteger ( 0 )); } public void onConnectionCreated () { numConnections . incrementAndGet (); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); ... } ... } The Spectator Registry will keep a weak reference to the object. If the object is garbage collected, then it will automatically drop the registration. In the example above, the Registry will have a weak reference to numConnections and the server instance will have a strong reference to numConnections . If the server instance goes away, then the Gauge will as well. When multiple Gauges are registered with the same id, the reported value will be the sum of the matches. For example, if multiple instances of the HttpServer class were created on different ports, then the value server.numConnections would be the total number of connections across all server instances. If a different behavior is desired, then ensure your usage does not perform multiple registrations. There are several different ways to register a Gauge: Using Number \u00b6 A Gauge can also be created based on an implementation of Number. Note the Number implementation should be thread-safe. For example: AtomicInteger size = new AtomicInteger (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( size ); The call will return the Number so the registration can be inline on the assignment: AtomicInteger size = PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( new AtomicInteger ()); Updates to the value are performed by updating the Number instance directly. Using Lambda \u00b6 Specify a lambda that takes the object as parameter. public class Queue { @Inject public Queue ( Registry registry ) { PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( this , Queue :: size ); } ... } Warning Be careful to avoid creating a reference to the object in the lambda. It will prevent garbage collection and can lead to a memory leak in the application. For example, by calling size without using the passed in object there will be a reference to this : PolledMeter.using(registry) .withName(\"queue.size\") .monitorValue(this, obj -> size()); Collection Sizes \u00b6 For classes that implement Collection or Map , there are helpers: Queue queue = new LinkedBlockingQueue (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorSize ( queue ); Map < String , String > cache = new ConcurrentMap <> (); PolledMeter . using ( registry ) . withName ( \"cache.size\" ) . monitorSize ( cache ); Monotonic Counters \u00b6 A common technique used by some libraries is to expose a monotonically increasing counter that represents the number of events since the system was initialized. An example of that in the JDK is ThreadPoolExecutor.getCompletedTaskCount , which returns the number of completed tasks on the thread pool. For sources like this, the monitorMonotonicCounter method can be used: // For an implementation of Number LongAdder tasks = new LongAdder (); PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( tasks ); // Or using a lambda ThreadPoolExecutor executor = ... PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( executor , ThreadPoolExecutor :: getCompletedTaskCount ); For thread pools specifically, there are better options for getting standard metrics. See the docs for the Thread Pools extension for more information. Active Gauges \u00b6 Gauges can also be set directly by the user. In this mode, the user is responsible for regularly updating the value of the Gauge by calling set . Looking at the HttpServer example, with an active gauge, it would look like: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; private Gauge gauge ; public HttpServer ( Registry registry ) { numConnections = new AtomicInteger (); gauge = registry . gauge ( \"server.numConnections\" ); gauge . set ( numConnections . get ()); } public void onConnectionCreated () { numConnections . incrementAndGet (); gauge . set ( numConnections . get ()); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); gauge . set ( numConnections . get ()); ... } ... }","title":"Gauges"},{"location":"spectator/lang/java/meters/gauge/#java-gauges","text":"","title":"Java Gauges"},{"location":"spectator/lang/java/meters/gauge/#polled-gauges","text":"The most common use of Gauges is by registering a hook with Spectator, so that it will poll the values in the background. This is done by using the PolledMeter helper class. A Polled Gauge is registered by passing in an id, a reference to the object, and a function to get or compute a numeric value based on the object. Note that a Gauge should only be registered once, not on each update. Consider this example of a web server tracking the number of connections: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; public HttpServer ( Registry registry ) { numConnections = PolledMeter . using ( registry ) . withName ( \"server.numConnections\" ) . monitorValue ( new AtomicInteger ( 0 )); } public void onConnectionCreated () { numConnections . incrementAndGet (); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); ... } ... } The Spectator Registry will keep a weak reference to the object. If the object is garbage collected, then it will automatically drop the registration. In the example above, the Registry will have a weak reference to numConnections and the server instance will have a strong reference to numConnections . If the server instance goes away, then the Gauge will as well. When multiple Gauges are registered with the same id, the reported value will be the sum of the matches. For example, if multiple instances of the HttpServer class were created on different ports, then the value server.numConnections would be the total number of connections across all server instances. If a different behavior is desired, then ensure your usage does not perform multiple registrations. There are several different ways to register a Gauge:","title":"Polled Gauges"},{"location":"spectator/lang/java/meters/gauge/#using-number","text":"A Gauge can also be created based on an implementation of Number. Note the Number implementation should be thread-safe. For example: AtomicInteger size = new AtomicInteger (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( size ); The call will return the Number so the registration can be inline on the assignment: AtomicInteger size = PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( new AtomicInteger ()); Updates to the value are performed by updating the Number instance directly.","title":"Using Number"},{"location":"spectator/lang/java/meters/gauge/#using-lambda","text":"Specify a lambda that takes the object as parameter. public class Queue { @Inject public Queue ( Registry registry ) { PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( this , Queue :: size ); } ... } Warning Be careful to avoid creating a reference to the object in the lambda. It will prevent garbage collection and can lead to a memory leak in the application. For example, by calling size without using the passed in object there will be a reference to this : PolledMeter.using(registry) .withName(\"queue.size\") .monitorValue(this, obj -> size());","title":"Using Lambda"},{"location":"spectator/lang/java/meters/gauge/#collection-sizes","text":"For classes that implement Collection or Map , there are helpers: Queue queue = new LinkedBlockingQueue (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorSize ( queue ); Map < String , String > cache = new ConcurrentMap <> (); PolledMeter . using ( registry ) . withName ( \"cache.size\" ) . monitorSize ( cache );","title":"Collection Sizes"},{"location":"spectator/lang/java/meters/gauge/#monotonic-counters","text":"A common technique used by some libraries is to expose a monotonically increasing counter that represents the number of events since the system was initialized. An example of that in the JDK is ThreadPoolExecutor.getCompletedTaskCount , which returns the number of completed tasks on the thread pool. For sources like this, the monitorMonotonicCounter method can be used: // For an implementation of Number LongAdder tasks = new LongAdder (); PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( tasks ); // Or using a lambda ThreadPoolExecutor executor = ... PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( executor , ThreadPoolExecutor :: getCompletedTaskCount ); For thread pools specifically, there are better options for getting standard metrics. See the docs for the Thread Pools extension for more information.","title":"Monotonic Counters"},{"location":"spectator/lang/java/meters/gauge/#active-gauges","text":"Gauges can also be set directly by the user. In this mode, the user is responsible for regularly updating the value of the Gauge by calling set . Looking at the HttpServer example, with an active gauge, it would look like: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; private Gauge gauge ; public HttpServer ( Registry registry ) { numConnections = new AtomicInteger (); gauge = registry . gauge ( \"server.numConnections\" ); gauge . set ( numConnections . get ()); } public void onConnectionCreated () { numConnections . incrementAndGet (); gauge . set ( numConnections . get ()); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); gauge . set ( numConnections . get ()); ... } ... }","title":"Active Gauges"},{"location":"spectator/lang/java/meters/percentile-timer/","text":"Java Percentile Timers \u00b6 TBD","title":"Percentile Timers"},{"location":"spectator/lang/java/meters/percentile-timer/#java-percentile-timers","text":"TBD","title":"Java Percentile Timers"},{"location":"spectator/lang/java/meters/timer/","text":"Java Timers \u00b6 Timer \u00b6 To get started, create an instance using the Registry : public class Server { private final Registry registry ; private final Timer requestLatency ; @Inject public Server ( Registry registry ) { this . registry = registry ; requestLatency = registry . timer ( \"server.requestLatency\" ); } Then wrap the call you need to measure, preferably using a lambda: public Response handle ( Request request ) { return requestLatency . record (() -> handleImpl ( request )); } The lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like: public Response handle ( Request request ) { final long start = registry . clock (). monotonicTime (); try { return handleImpl ( request ); } finally { final long end = registry . clock (). monotonicTime (); requestLatency . record ( end - start , TimeUnit . NANOSECONDS ); } } This example uses the Clock from the Registry, which can be useful for testing, if you need to control the timing. In actual usage, it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times, to avoid occasionally having bogus measurements due to time adjustments. For more information, see the Clock documentation . LongTaskTimer \u00b6 To get started, create an instance using the Registry : import com.netflix.spectator.api.patterns.LongTaskTimer ; public class MetadataService { private final LongTaskTimer metadataRefresh ; @Inject public MetadataService ( Registry registry ) { metadataRefresh = LongTaskTimer . get ( registry , registry . createId ( \"metadata.refreshDuration\" )); // setup background thread to call refresh() } private void refresh () { final int id = metadataRefresh . start (); try { refreshImpl (); } finally { metadataRefresh . stop ( id ); } } The id value returned by the start method is used to keep track of a particular task being measured by the Timer. It must be stopped using the provided id. Note that unlike a regular Timer that does not do anything until the final duration is recorded, a long duration Timer will report as two Gauges: duration : total duration spent within all currently running tasks. activeTasks : number of currently running tasks. This means that you can see what is happening while the task is running, but you need to keep in mind: The meter id is fixed before the task begins. There is no way to change tags based on the run, e.g., update a different Timer, if an exception is thrown. Being a Gauge, it is inappropriate for short tasks. In particular, Gauges are sampled and if it is not sampled during the execution, or the sampling period is a significant subset of the expected duration, then the duration value will not be meaningful. Like a regular Timer, the duration Timer also supports using a lambda to simplify the common case: private void refresh () { metadataRefresh . record ( this :: refreshImpl ); }","title":"Timers"},{"location":"spectator/lang/java/meters/timer/#java-timers","text":"","title":"Java Timers"},{"location":"spectator/lang/java/meters/timer/#timer","text":"To get started, create an instance using the Registry : public class Server { private final Registry registry ; private final Timer requestLatency ; @Inject public Server ( Registry registry ) { this . registry = registry ; requestLatency = registry . timer ( \"server.requestLatency\" ); } Then wrap the call you need to measure, preferably using a lambda: public Response handle ( Request request ) { return requestLatency . record (() -> handleImpl ( request )); } The lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like: public Response handle ( Request request ) { final long start = registry . clock (). monotonicTime (); try { return handleImpl ( request ); } finally { final long end = registry . clock (). monotonicTime (); requestLatency . record ( end - start , TimeUnit . NANOSECONDS ); } } This example uses the Clock from the Registry, which can be useful for testing, if you need to control the timing. In actual usage, it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times, to avoid occasionally having bogus measurements due to time adjustments. For more information, see the Clock documentation .","title":"Timer"},{"location":"spectator/lang/java/meters/timer/#longtasktimer","text":"To get started, create an instance using the Registry : import com.netflix.spectator.api.patterns.LongTaskTimer ; public class MetadataService { private final LongTaskTimer metadataRefresh ; @Inject public MetadataService ( Registry registry ) { metadataRefresh = LongTaskTimer . get ( registry , registry . createId ( \"metadata.refreshDuration\" )); // setup background thread to call refresh() } private void refresh () { final int id = metadataRefresh . start (); try { refreshImpl (); } finally { metadataRefresh . stop ( id ); } } The id value returned by the start method is used to keep track of a particular task being measured by the Timer. It must be stopped using the provided id. Note that unlike a regular Timer that does not do anything until the final duration is recorded, a long duration Timer will report as two Gauges: duration : total duration spent within all currently running tasks. activeTasks : number of currently running tasks. This means that you can see what is happening while the task is running, but you need to keep in mind: The meter id is fixed before the task begins. There is no way to change tags based on the run, e.g., update a different Timer, if an exception is thrown. Being a Gauge, it is inappropriate for short tasks. In particular, Gauges are sampled and if it is not sampled during the execution, or the sampling period is a significant subset of the expected duration, then the duration value will not be meaningful. Like a regular Timer, the duration Timer also supports using a lambda to simplify the common case: private void refresh () { metadataRefresh . record ( this :: refreshImpl ); }","title":"LongTaskTimer"},{"location":"spectator/lang/java/registry/metrics3/","text":"Metrics3 Registry \u00b6 Registry that uses metrics3 as the underlying implementation. To use the metrics registry, add a dependency on the spectator-reg-metrics3 library. For gradle: com.netflix.spectator:spectator-reg-metrics3:0.101.0 Then when initializing the application, use the MetricsRegistry . For more information see the metrics3 example .","title":"Metrics3"},{"location":"spectator/lang/java/registry/metrics3/#metrics3-registry","text":"Registry that uses metrics3 as the underlying implementation. To use the metrics registry, add a dependency on the spectator-reg-metrics3 library. For gradle: com.netflix.spectator:spectator-reg-metrics3:0.101.0 Then when initializing the application, use the MetricsRegistry . For more information see the metrics3 example .","title":"Metrics3 Registry"},{"location":"spectator/lang/java/registry/overview/","text":"Registry \u00b6 The Registry is the main class for managing a set of meters. A Meter is a class for collecting a set of measurements about your application. Choose Implementation \u00b6 The core Spectator library, spectator-api , comes with the following Registry implementations: Class Dependency Description DefaultRegistry spectator-api Updates local counters, frequently used with unit tests . NoopRegistry spectator-api Does nothing, tries to make operations as cheap as possible. This implementation is typically used to help understand the overhead being created due to instrumentation. It can also be useful in testing to help ensure that no side effects were introduced where the instrumentation is now needed in order for the application for function properly. MetricsRegistry spectator-reg-metrics3 Map to metrics3 library . This implementation is typically used for reporting to local files, JMX, or other backends like Graphite. Note that it uses a hierarchical naming scheme rather than the dimensional naming used by Spectator, so the names will get flattened when mapped to this Registry. It is recommended for libraries to write code against the Registry interface and allow the implementation to get injected by the user of the library. The simplest way is to accept the Registry via the constructor, for example: public class HttpServer { public HttpServer ( Registry registry ) { // use registry to collect measurements } } The user of the class can then provide the implementation: Registry registry = new DefaultRegistry (); HttpServer server = new HttpServer ( registry ); More complete examples can be found on the testing page or in the spectator-examples repo . Working with Ids \u00b6 Spectator is primarily intended for collecting data for dimensional time series backends like Atlas . The ids used for looking up a Meter in the Registry consist of a name and set of tags. Ids will be consumed many times by users after the data has been reported, so they should be chosen with some care and thought about how they will get used. See the conventions page for some general guidelines. Ids are created via the Registry, for example: Id id = registry . createId ( \"server.requestCount\" ); The ids are immutable, so they can be freely passed around and used in a concurrent context. Tags can be added when an id is created: Id id = registry . createId ( \"server.requestCount\" , \"status\" , \"2xx\" , \"method\" , \"GET\" ); Or by using withTag and withTags on an existing id: public class HttpServer { private final Id baseId ; public HttpServer ( Registry registry ) { baseId = registry . createId ( \"server.requestCount\" ); } private void handleRequestComplete ( HttpRequest req , HttpResponse res ) { // Remember Id is immutable, withTags will return a copy with the // the additional metadata Id reqId = baseId . withTags ( \"status\" , res . getStatus (), \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } private void handleRequestError ( HttpRequest req , Throwable t ) { // Can also be added individually using `withTag`. However, it is better // for performance to batch modifications using `withTags`. Id reqId = baseId . withTag ( \"error\" , t . getClass (). getSimpleName ()) . withTag ( \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } } Collecting Measurements \u00b6 Once you have an id, the Registry can be used to get an instance of a Meter to record a measurement. Meters can roughly be categorized in two groups: Active \u00b6 Active Meters are ones that are called directly when some event occurs. There are three basic types supported: Counters measure how often something is occurring. This will be reported to backend systems as a rate-per-second. For example, the number of requests processed by a web server. Timers measure how long something took. For example, the latency of requests processed by a web server. Distribution Summaries measure the size of something. For example, the entity sizes for requests processed by a web server. Passive \u00b6 Passive Meters are ones where the Registry has a reference to get the value when needed. For example, the number of current connections on a web server or the number threads that are currently in use. These will be Gauges . Global Registry \u00b6 There are some use-cases where injecting the Registry is not possible or is too cumbersome. The main example from the core Spectator libraries is the log4j appender . The Global Registry is useful there because logging is often initialized before any other systems and Spectator itself uses logging via the slf4j api which is quite likely being bound to log4j when that the appender is being used. By using the Global Registry, the logging initialization can proceed before the Spectator initialization in the application. Though any measurements taken before a Registry instance has been added will be lost. The Global Registry is accessed using: Registry registry = Spectator . globalRegistry (); By default, it will not record anything. For a specific registry instance you can choose to configure it to work with the Global Registry by calling add : public void init () { Registry registry = // Choose an appropriate implementation // Add it to the global registry so it will receive // any activity on the global registry Spectator . globalRegistry (). add ( registry ); } Any measurements taken while no Registries are added to the global instance will be lost. If multiple Registries are added, all will receive updates made to the Global Registry.","title":"Overview"},{"location":"spectator/lang/java/registry/overview/#registry","text":"The Registry is the main class for managing a set of meters. A Meter is a class for collecting a set of measurements about your application.","title":"Registry"},{"location":"spectator/lang/java/registry/overview/#choose-implementation","text":"The core Spectator library, spectator-api , comes with the following Registry implementations: Class Dependency Description DefaultRegistry spectator-api Updates local counters, frequently used with unit tests . NoopRegistry spectator-api Does nothing, tries to make operations as cheap as possible. This implementation is typically used to help understand the overhead being created due to instrumentation. It can also be useful in testing to help ensure that no side effects were introduced where the instrumentation is now needed in order for the application for function properly. MetricsRegistry spectator-reg-metrics3 Map to metrics3 library . This implementation is typically used for reporting to local files, JMX, or other backends like Graphite. Note that it uses a hierarchical naming scheme rather than the dimensional naming used by Spectator, so the names will get flattened when mapped to this Registry. It is recommended for libraries to write code against the Registry interface and allow the implementation to get injected by the user of the library. The simplest way is to accept the Registry via the constructor, for example: public class HttpServer { public HttpServer ( Registry registry ) { // use registry to collect measurements } } The user of the class can then provide the implementation: Registry registry = new DefaultRegistry (); HttpServer server = new HttpServer ( registry ); More complete examples can be found on the testing page or in the spectator-examples repo .","title":"Choose Implementation"},{"location":"spectator/lang/java/registry/overview/#working-with-ids","text":"Spectator is primarily intended for collecting data for dimensional time series backends like Atlas . The ids used for looking up a Meter in the Registry consist of a name and set of tags. Ids will be consumed many times by users after the data has been reported, so they should be chosen with some care and thought about how they will get used. See the conventions page for some general guidelines. Ids are created via the Registry, for example: Id id = registry . createId ( \"server.requestCount\" ); The ids are immutable, so they can be freely passed around and used in a concurrent context. Tags can be added when an id is created: Id id = registry . createId ( \"server.requestCount\" , \"status\" , \"2xx\" , \"method\" , \"GET\" ); Or by using withTag and withTags on an existing id: public class HttpServer { private final Id baseId ; public HttpServer ( Registry registry ) { baseId = registry . createId ( \"server.requestCount\" ); } private void handleRequestComplete ( HttpRequest req , HttpResponse res ) { // Remember Id is immutable, withTags will return a copy with the // the additional metadata Id reqId = baseId . withTags ( \"status\" , res . getStatus (), \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } private void handleRequestError ( HttpRequest req , Throwable t ) { // Can also be added individually using `withTag`. However, it is better // for performance to batch modifications using `withTags`. Id reqId = baseId . withTag ( \"error\" , t . getClass (). getSimpleName ()) . withTag ( \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } }","title":"Working with Ids"},{"location":"spectator/lang/java/registry/overview/#collecting-measurements","text":"Once you have an id, the Registry can be used to get an instance of a Meter to record a measurement. Meters can roughly be categorized in two groups:","title":"Collecting Measurements"},{"location":"spectator/lang/java/registry/overview/#active","text":"Active Meters are ones that are called directly when some event occurs. There are three basic types supported: Counters measure how often something is occurring. This will be reported to backend systems as a rate-per-second. For example, the number of requests processed by a web server. Timers measure how long something took. For example, the latency of requests processed by a web server. Distribution Summaries measure the size of something. For example, the entity sizes for requests processed by a web server.","title":"Active"},{"location":"spectator/lang/java/registry/overview/#passive","text":"Passive Meters are ones where the Registry has a reference to get the value when needed. For example, the number of current connections on a web server or the number threads that are currently in use. These will be Gauges .","title":"Passive"},{"location":"spectator/lang/java/registry/overview/#global-registry","text":"There are some use-cases where injecting the Registry is not possible or is too cumbersome. The main example from the core Spectator libraries is the log4j appender . The Global Registry is useful there because logging is often initialized before any other systems and Spectator itself uses logging via the slf4j api which is quite likely being bound to log4j when that the appender is being used. By using the Global Registry, the logging initialization can proceed before the Spectator initialization in the application. Though any measurements taken before a Registry instance has been added will be lost. The Global Registry is accessed using: Registry registry = Spectator . globalRegistry (); By default, it will not record anything. For a specific registry instance you can choose to configure it to work with the Global Registry by calling add : public void init () { Registry registry = // Choose an appropriate implementation // Add it to the global registry so it will receive // any activity on the global registry Spectator . globalRegistry (). add ( registry ); } Any measurements taken while no Registries are added to the global instance will be lost. If multiple Registries are added, all will receive updates made to the Global Registry.","title":"Global Registry"},{"location":"spectator/lang/nodejs/usage/","text":"Project \u00b6 spectator-js \u00b6 Source NPM Product Lifecycle: GA Module Name: nflx-spectator This module can be used to instrument an application using counters, distribution summaries, gauges, long task timers, timers, and more complex meter types (like Bucket or Percentile Timers) using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator. spectator-js-nodejsmetrics \u00b6 Source NPM Product Lifecycle: GA Module Name: nflx-spectator-nodejsmetrics Generate Node.js runtime metrics using the spectator-js Node module. Install Libraries \u00b6 Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" } } Instrumenting Code \u00b6 'use strict' ; const spectator = require ( 'nflx-spectator' ); // Netflix applications can use the nflx-spectator-config node module available // internally through artifactory to generate the config required by nflx-spectator function getConfig () { return { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' , timeout : 1000 // milliseconds } } class Response { constructor ( status , size ) { this . status = status ; this . size = size ; } } class Server { constructor ( registry ) { this . registry = registry ; // create a base Id, to which we'll add some dynamic tags later this . requestCountId = registry . createId ( 'server.requestCount' , { version : 'v1' }); this . requestLatency = registry . timer ( 'server.requestLatency' ); this . responseSize = registry . distributionSummary ( 'server.responseSizes' ); } handle ( request ) { const start = this . registry . hrtime (); // do some work based on request and obtain a response const res = new Response ( 200 , 64 ); // update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as a lookup of an id object in a map // However, it is more expensive than having a local variable set // to the counter const counterId = this . requestCountId . withTags ({ country : request . country , status : res . status }); this . registry . counter ( counterId ). increment (); this . requestLatency . record ( this . registry . hrtime ( start )); this . responseSize . record ( res . size ); return res ; } } const config = getConfig (); const registry = new spectator . Registry ( config ); class Request { constructor ( country ) { this . country = country ; } } // somehow get a request from the user... function getNextRequest () { return new Request ( 'AR' ); } function handleTermination () { registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); registry . start (); const server = new Server ( registry ); for ( let i = 0 ; i < 3 ; ++ i ) { const req = getNextRequest (); server . handle ( req ) } registry . stop (); Enable Runtime Metrics \u00b6 'use strict' ; function getConfig () { } const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const config = { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' }; const registry = new spectator . Registry ( config ); registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); // start collecting nodejs metrics // ... metrics . stop (); registry . stop (); Netflix Integration \u00b6 Create a Netflix Spectator Config to be used by spectator-js . Only applications should depend on the @netflix-internal/spectator-conf package. Libraries should get the Registry passed by the application, and therefore should only need to depend on spectator-js . Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" , \"@netflix-internal/spectator-conf\" : \"*\" } } This configuration also brings in spectator-js-nodejsmetrics to provide Node.js runtime metrics. You can override the logger used by the Spectator registry by setting the logger property. The specified logger should provide debug , info , and error methods. By default, spectator-js logs to stdout. const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const getSpectatorConfig = require ( '@netflix-internal/spectator-conf' ); const logger = require ( 'pino' )(); //... const registry = new spectator . Registry ( getSpectatorConfig ()); registry . logger = logger ; registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); function handleTermination () { metrics . stop (); registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); //... your app handleTermination ();","title":"Usage"},{"location":"spectator/lang/nodejs/usage/#project","text":"","title":"Project"},{"location":"spectator/lang/nodejs/usage/#spectator-js","text":"Source NPM Product Lifecycle: GA Module Name: nflx-spectator This module can be used to instrument an application using counters, distribution summaries, gauges, long task timers, timers, and more complex meter types (like Bucket or Percentile Timers) using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator.","title":"spectator-js"},{"location":"spectator/lang/nodejs/usage/#spectator-js-nodejsmetrics","text":"Source NPM Product Lifecycle: GA Module Name: nflx-spectator-nodejsmetrics Generate Node.js runtime metrics using the spectator-js Node module.","title":"spectator-js-nodejsmetrics"},{"location":"spectator/lang/nodejs/usage/#install-libraries","text":"Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" } }","title":"Install Libraries"},{"location":"spectator/lang/nodejs/usage/#instrumenting-code","text":"'use strict' ; const spectator = require ( 'nflx-spectator' ); // Netflix applications can use the nflx-spectator-config node module available // internally through artifactory to generate the config required by nflx-spectator function getConfig () { return { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' , timeout : 1000 // milliseconds } } class Response { constructor ( status , size ) { this . status = status ; this . size = size ; } } class Server { constructor ( registry ) { this . registry = registry ; // create a base Id, to which we'll add some dynamic tags later this . requestCountId = registry . createId ( 'server.requestCount' , { version : 'v1' }); this . requestLatency = registry . timer ( 'server.requestLatency' ); this . responseSize = registry . distributionSummary ( 'server.responseSizes' ); } handle ( request ) { const start = this . registry . hrtime (); // do some work based on request and obtain a response const res = new Response ( 200 , 64 ); // update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as a lookup of an id object in a map // However, it is more expensive than having a local variable set // to the counter const counterId = this . requestCountId . withTags ({ country : request . country , status : res . status }); this . registry . counter ( counterId ). increment (); this . requestLatency . record ( this . registry . hrtime ( start )); this . responseSize . record ( res . size ); return res ; } } const config = getConfig (); const registry = new spectator . Registry ( config ); class Request { constructor ( country ) { this . country = country ; } } // somehow get a request from the user... function getNextRequest () { return new Request ( 'AR' ); } function handleTermination () { registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); registry . start (); const server = new Server ( registry ); for ( let i = 0 ; i < 3 ; ++ i ) { const req = getNextRequest (); server . handle ( req ) } registry . stop ();","title":"Instrumenting Code"},{"location":"spectator/lang/nodejs/usage/#enable-runtime-metrics","text":"'use strict' ; function getConfig () { } const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const config = { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' }; const registry = new spectator . Registry ( config ); registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); // start collecting nodejs metrics // ... metrics . stop (); registry . stop ();","title":"Enable Runtime Metrics"},{"location":"spectator/lang/nodejs/usage/#netflix-integration","text":"Create a Netflix Spectator Config to be used by spectator-js . Only applications should depend on the @netflix-internal/spectator-conf package. Libraries should get the Registry passed by the application, and therefore should only need to depend on spectator-js . Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" , \"@netflix-internal/spectator-conf\" : \"*\" } } This configuration also brings in spectator-js-nodejsmetrics to provide Node.js runtime metrics. You can override the logger used by the Spectator registry by setting the logger property. The specified logger should provide debug , info , and error methods. By default, spectator-js logs to stdout. const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const getSpectatorConfig = require ( '@netflix-internal/spectator-conf' ); const logger = require ( 'pino' )(); //... const registry = new spectator . Registry ( getSpectatorConfig ()); registry . logger = logger ; registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); function handleTermination () { metrics . stop (); registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); //... your app handleTermination ();","title":"Netflix Integration"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/","text":"Node.js runtime CPU metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.cpuUsage \u00b6 Percentage of CPU time the Node.js process is consuming, from 0..100. The usage is divided into the following categories: system : CPU time spent running the kernel. user : CPU time spent running user space (non-kernel) processes. Unit: percent Dimensions: id : The category of CPU usage. Example: { \"tags\" : { \"id\" : \"system\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 0.8954088417692685 }, { \"tags\" : { \"id\" : \"user\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 4.659007745141895 }","title":"CPU"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#nodejscpuusage","text":"Percentage of CPU time the Node.js process is consuming, from 0..100. The usage is divided into the following categories: system : CPU time spent running the kernel. user : CPU time spent running user space (non-kernel) processes. Unit: percent Dimensions: id : The category of CPU usage. Example: { \"tags\" : { \"id\" : \"system\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 0.8954088417692685 }, { \"tags\" : { \"id\" : \"user\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 4.659007745141895 }","title":"nodejs.cpuUsage"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/","text":"Node.js runtime event loop metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.eventLoop \u00b6 The time it takes for the event loop to complete. This is sampled twice per second. Unit: seconds nodejs.eventLoopLag \u00b6 The time that the event loop is running behind, as measured by attempting to execute a timer once per second. Unit: seconds","title":"Event Loop"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#nodejseventloop","text":"The time it takes for the event loop to complete. This is sampled twice per second. Unit: seconds","title":"nodejs.eventLoop"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#nodejseventlooplag","text":"The time that the event loop is running behind, as measured by attempting to execute a timer once per second. Unit: seconds","title":"nodejs.eventLoopLag"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/","text":"Node.js runtime file descriptor metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. openFileDescriptorsCount \u00b6 Number of file descriptors currently open. Unit: file descriptors maxFileDescriptorsCount \u00b6 The maximum number of file descriptors that can be open at the same time. Unit: file descriptors","title":"File Descriptor"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#openfiledescriptorscount","text":"Number of file descriptors currently open. Unit: file descriptors","title":"openFileDescriptorsCount"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#maxfiledescriptorscount","text":"The maximum number of file descriptors that can be open at the same time. Unit: file descriptors","title":"maxFileDescriptorsCount"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/","text":"Node.js runtime garbage collection metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.gc.allocationRate \u00b6 The rate at which the app is allocating memory. Unit: bytes/second nodejs.gc.liveDataSize \u00b6 The size of the old_space after a major GC event. Unit: bytes nodejs.gc.maxDataSize \u00b6 The maximum amount of memory the nodejs process is allowed to use. This is primarily used for gaining perspective on the liveDataSize . Unit: bytes nodejs.gc.pause \u00b6 The time it takes to complete different GC events. Event categories: scavenge : The most common garbage collection method. Node will typically trigger one of these every time the VM is idle. markSweepCompact : The heaviest type of garbage collection V8 may do. If you see many of these happening you will need to either keep fewer objects around in your process or increase V8's heap limit. incrementalMarking : A phased garbage collection that interleaves collection with application logic to reduce the amount of time the application is paused. processWeakCallbacks : After a garbage collection occurs, V8 will call any weak reference callbacks registered for objects that have been freed. This measurement is from the start of the first weak callback to the end of the last for a given garbage collection. Unit: seconds Dimensions: id : The GC event category. nodejs.gc.promotionRate \u00b6 The rate at which data is being moved from new_space to old_space . Unit: bytes/second","title":"Garbarge Collection"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcallocationrate","text":"The rate at which the app is allocating memory. Unit: bytes/second","title":"nodejs.gc.allocationRate"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgclivedatasize","text":"The size of the old_space after a major GC event. Unit: bytes","title":"nodejs.gc.liveDataSize"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcmaxdatasize","text":"The maximum amount of memory the nodejs process is allowed to use. This is primarily used for gaining perspective on the liveDataSize . Unit: bytes","title":"nodejs.gc.maxDataSize"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcpause","text":"The time it takes to complete different GC events. Event categories: scavenge : The most common garbage collection method. Node will typically trigger one of these every time the VM is idle. markSweepCompact : The heaviest type of garbage collection V8 may do. If you see many of these happening you will need to either keep fewer objects around in your process or increase V8's heap limit. incrementalMarking : A phased garbage collection that interleaves collection with application logic to reduce the amount of time the application is paused. processWeakCallbacks : After a garbage collection occurs, V8 will call any weak reference callbacks registered for objects that have been freed. This measurement is from the start of the first weak callback to the end of the last for a given garbage collection. Unit: seconds Dimensions: id : The GC event category.","title":"nodejs.gc.pause"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcpromotionrate","text":"The rate at which data is being moved from new_space to old_space . Unit: bytes/second","title":"nodejs.gc.promotionRate"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/","text":"Node.js runtime heap metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Data is gathered from the v8.getHeapStatistics method. Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.doesZapGarbage \u00b6 Whether or not the --zap_code_space option is enabled. This makes V8 overwrite heap garbage with a bit pattern. The RSS footprint (resident memory set) gets bigger because it continuously touches all heap pages and that makes them less likely to get swapped out by the operating system. Unit: boolean nodejs.heapSizeLimit \u00b6 The absolute limit the heap cannot exceed (default limit or --max_old_space_size ). Unit: bytes nodejs.mallocedMemory \u00b6 Current amount of memory, obtained via malloc . Unit: bytes nodejs.peakMallocedMemory \u00b6 Peak amount of memory, obtained via malloc . Unit: bytes nodejs.totalAvailableSize \u00b6 Available heap size. Unit: bytes nodejs.totalHeapSize \u00b6 Memory V8 has allocated for the heap. This can grow if usedHeap needs more. Unit: bytes nodejs.totalHeapSizeExecutable \u00b6 Memory for compiled bytecode and JITed code. Unit: bytes nodejs.totalPhysicalSize \u00b6 Committed size. Unit: bytes nodejs.usedHeapSize \u00b6 Memory used by application data. Unit: bytes","title":"Heap"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#metrics","text":"Data is gathered from the v8.getHeapStatistics method.","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsdoeszapgarbage","text":"Whether or not the --zap_code_space option is enabled. This makes V8 overwrite heap garbage with a bit pattern. The RSS footprint (resident memory set) gets bigger because it continuously touches all heap pages and that makes them less likely to get swapped out by the operating system. Unit: boolean","title":"nodejs.doesZapGarbage"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsheapsizelimit","text":"The absolute limit the heap cannot exceed (default limit or --max_old_space_size ). Unit: bytes","title":"nodejs.heapSizeLimit"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsmallocedmemory","text":"Current amount of memory, obtained via malloc . Unit: bytes","title":"nodejs.mallocedMemory"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejspeakmallocedmemory","text":"Peak amount of memory, obtained via malloc . Unit: bytes","title":"nodejs.peakMallocedMemory"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalavailablesize","text":"Available heap size. Unit: bytes","title":"nodejs.totalAvailableSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalheapsize","text":"Memory V8 has allocated for the heap. This can grow if usedHeap needs more. Unit: bytes","title":"nodejs.totalHeapSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalheapsizeexecutable","text":"Memory for compiled bytecode and JITed code. Unit: bytes","title":"nodejs.totalHeapSizeExecutable"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalphysicalsize","text":"Committed size. Unit: bytes","title":"nodejs.totalPhysicalSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsusedheapsize","text":"Memory used by application data. Unit: bytes","title":"nodejs.usedHeapSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/","text":"Node.js runtime heap space metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Data is gathered from the v8.getHeapSpaceStatistics method, for each space listed. Space categories: new_space : Where new allocations happen; it is fast to allocate and collect garbage here. Objects living in the New Space are called the Young Generation. old_space : Object that survived the New Space collector are promoted here; they are called the Old Generation. Allocation in the Old Space is fast, but collection is expensive so it is less frequently performed. code_space : Contains executable code and therefore is marked executable. map_space : Contains map objects only. large_object_space : Contains promoted large objects which exceed the size limits of other spaces. Each object gets its own mmap region of memory and these objects are never moved by GC. Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.spaceSize \u00b6 The allocated size of the space. Unit: bytes Dimensions: id : Space category. nodejs.spaceUsedSize \u00b6 The used size of the space. Unit: bytes Dimensions: id : Space category. nodejs.spaceAvailableSize \u00b6 The available size of the space. Unit: bytes Dimensions: id : Space category. nodejs.physicalSpaceSize \u00b6 The physical size of the space. Unit: bytes Dimensions: id : Space category.","title":"Heap Space"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#metrics","text":"Data is gathered from the v8.getHeapSpaceStatistics method, for each space listed. Space categories: new_space : Where new allocations happen; it is fast to allocate and collect garbage here. Objects living in the New Space are called the Young Generation. old_space : Object that survived the New Space collector are promoted here; they are called the Old Generation. Allocation in the Old Space is fast, but collection is expensive so it is less frequently performed. code_space : Contains executable code and therefore is marked executable. map_space : Contains map objects only. large_object_space : Contains promoted large objects which exceed the size limits of other spaces. Each object gets its own mmap region of memory and these objects are never moved by GC.","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspacesize","text":"The allocated size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspaceusedsize","text":"The used size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceUsedSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspaceavailablesize","text":"The available size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceAvailableSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsphysicalspacesize","text":"The physical size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.physicalSpaceSize"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/","text":"Node.js runtime memory metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.rss \u00b6 Resident Set Size, which is the total memory allocated for the process execution. This includes the Code Segment, Stack (local variables and pointers) and Heap (objects and closures). Unit: bytes nodejs.heapTotal \u00b6 Total size of the allocated heap. Unit: bytes nodejs.heapUsed \u00b6 Memory used during the execution of our process. Unit: bytes nodejs.external \u00b6 Memory usage of C++ objects bound to JavaScript objects managed by V8. Unit: bytes","title":"Memory"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsrss","text":"Resident Set Size, which is the total memory allocated for the process execution. This includes the Code Segment, Stack (local variables and pointers) and Heap (objects and closures). Unit: bytes","title":"nodejs.rss"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsheaptotal","text":"Total size of the allocated heap. Unit: bytes","title":"nodejs.heapTotal"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsheapused","text":"Memory used during the execution of our process. Unit: bytes","title":"nodejs.heapUsed"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsexternal","text":"Memory usage of C++ objects bound to JavaScript objects managed by V8. Unit: bytes","title":"nodejs.external"},{"location":"spectator/lang/nodejs/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/nodejs/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/nodejs/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/nodejs/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/nodejs/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/py/usage/","text":"Project \u00b6 Source PyPI Product Lifecycle: Beta Module Name: netflix-spectator-py This module can be used to instrument an application using Counters, Distribution Summaries, Gauges, Timers, and Percentile Timers using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator. Install Library \u00b6 Install the library from PyPI: pip install netflix-spectator-py Instrumenting Code \u00b6 from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment () GlobalRegistry . gauge ( 'server.capacity' ) . set ( 50 ) Usage \u00b6 The import of the GlobalRegistry will start a daemon thread that will publish metrics in the background. The cache will be flushed upon normal interpreter termination using atexit , with the following exceptions: The program is killed by a signal not handled by Python. A Python fatal internal error is detected. When os._exit() is called. If you do not want the GlobalRegistry to automatically start at module import, then set the following environment variable: SPECTATOR_PY_DISABLE_AUTO_START_GLOBAL = 1 With this configuration, you will have to manually call the GlobalRegistry.start() and GlobalRegistry.stop() methods. Failure to do so will prevent metric publishing. Common Tagging \u00b6 This library does not add the nf.node tag to the published metrics. If you need it, remember to add it. Gunicorn Preload \u00b6 If you are using this library while running behind Gunicorn, make sure that you do not use the --preload flag, because it can cause issues with how the background thread operates due to loading the application code before the worker processes are forked. On the Paved Path, with EzConfig, that is achieved with the following configuration: WSGI_GUNICORN_PRELOAD = undef Netflix Integration \u00b6 Add the internal configuration for the Spectator Python client, so that it can send metrics to an Atlas Aggregator cluster. Replace SMARTIPROXY_HOSTNAME with the hostname of the internal SmartiProxy service. pip install -i https://SMARTIPROXY_HOSTNAME/pypi netflix-spectator-pyconf","title":"Usage"},{"location":"spectator/lang/py/usage/#project","text":"Source PyPI Product Lifecycle: Beta Module Name: netflix-spectator-py This module can be used to instrument an application using Counters, Distribution Summaries, Gauges, Timers, and Percentile Timers using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator.","title":"Project"},{"location":"spectator/lang/py/usage/#install-library","text":"Install the library from PyPI: pip install netflix-spectator-py","title":"Install Library"},{"location":"spectator/lang/py/usage/#instrumenting-code","text":"from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment () GlobalRegistry . gauge ( 'server.capacity' ) . set ( 50 )","title":"Instrumenting Code"},{"location":"spectator/lang/py/usage/#usage","text":"The import of the GlobalRegistry will start a daemon thread that will publish metrics in the background. The cache will be flushed upon normal interpreter termination using atexit , with the following exceptions: The program is killed by a signal not handled by Python. A Python fatal internal error is detected. When os._exit() is called. If you do not want the GlobalRegistry to automatically start at module import, then set the following environment variable: SPECTATOR_PY_DISABLE_AUTO_START_GLOBAL = 1 With this configuration, you will have to manually call the GlobalRegistry.start() and GlobalRegistry.stop() methods. Failure to do so will prevent metric publishing.","title":"Usage"},{"location":"spectator/lang/py/usage/#common-tagging","text":"This library does not add the nf.node tag to the published metrics. If you need it, remember to add it.","title":"Common Tagging"},{"location":"spectator/lang/py/usage/#gunicorn-preload","text":"If you are using this library while running behind Gunicorn, make sure that you do not use the --preload flag, because it can cause issues with how the background thread operates due to loading the application code before the worker processes are forked. On the Paved Path, with EzConfig, that is achieved with the following configuration: WSGI_GUNICORN_PRELOAD = undef","title":"Gunicorn Preload"},{"location":"spectator/lang/py/usage/#netflix-integration","text":"Add the internal configuration for the Spectator Python client, so that it can send metrics to an Atlas Aggregator cluster. Replace SMARTIPROXY_HOSTNAME with the hostname of the internal SmartiProxy service. pip install -i https://SMARTIPROXY_HOSTNAME/pypi netflix-spectator-pyconf","title":"Netflix Integration"},{"location":"spectator/lang/py/meters/counter/","text":"from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment ()","title":"Counters"},{"location":"spectator/lang/py/meters/dist-summary/","text":"","title":"Distribution Summaries"},{"location":"spectator/lang/py/meters/gauge/","text":"","title":"Gauges"},{"location":"spectator/lang/py/meters/percentile-timer/","text":"","title":"Percentile Timers"},{"location":"spectator/lang/py/meters/timer/","text":"","title":"Timers"},{"location":"spectator/lang/rb/usage/","text":"Project \u00b6 Source RubyGems Product Lifecycle: Alpha Module Name: netflix-spectator-rb This implements a basic Spectator library for instrumenting Ruby applications, sending metrics to an Atlas aggregator service. Install Library \u00b6 Install the library from RubyGems: gem install spectator-rb Instrumenting Code \u00b6 require 'spectator' class Response attr_accessor :status , :size def initialize ( status , size ) @status = status @size = size end end class Request attr_reader :country def initialize ( country ) @country = country end end class ExampleServer def initialize ( registry ) @registry = registry @req_count_id = registry . new_id ( 'server.requestCount' ) @req_latency = registry . timer ( 'server.requestLatency' ) @resp_sizes = registry . distribution_summary ( 'server.responseSizes' ) end def expensive_computation ( request ) # ... end def handle_request ( request ) start = @registry . clock . monotonic_time # initialize response response = Response . new ( 200 , 64 ) # Update the counter id with dimensions based on the request. The # counter will then be looked up in the registry which should be # fairly cheap, such as lookup of id object in a map # However, it is more expensive than having a local variable set # to the counter. cnt_id = @req_count_id . with_tag ( :country , request . country ) . with_tag ( :status , response . status . to_s ) @registry . counter_with_id ( cnt_id ) . increment # ... @req_latency . record ( @registry . clock . monotonic_time - start ) @resp_sizes . record ( response . size ) # timers can also time a given block # this is equivalent to: # start = @registry.clock.monotonic_time # expensive_computation(request) # @registry.timer('server.computeTime').record(@registry.clock.monotonic_time - start) @registry . timer ( 'server.computeTime' ) . time { expensive_computation ( request ) } # ... end end config = { common_tags : { :'nf.app' => 'foo' }, frequency : 0 . 5 , uri : 'http://localhost:8080/api/v4/publish' } registry = Spectator :: Registry . new ( config ) registry . start server = ExampleServer . new ( registry ) # ... # process some requests requests = [ Request . new ( 'us' ), Request . new ( 'ar' ), Request . new ( 'ar' ) ] requests . each { | req | server . handle_request ( req ) } sleep ( 2 ) registry . stop Netflix Integration \u00b6 Add the internal configuration for the Spectator Ruby client, so that it can send metrics to an Atlas Aggregator cluster. If you are using the internal Artifactory, add the dependency to your Gemfile: gem 'netflix-spectator-config' gem 'netflix-spectator-rb' If you are not using the internal Artifactory, then you can do the following, replacing STASH_HOSTNAME_AND_PORT with appropriate values: gem 'netflix-spectator-config' , git : 'ssh://git@STASH_HOSTNAME_AND_PORT/cldmta/nflx-spectator-rb.git' gem 'netflix-spectator-rb' Once the configuration Gem is installed, it is used as follows: require 'spectator_config' require 'spectator' config = SpectatorConfig . config registry = Spectator :: Registry . new ( config ) registry . start # ... registry . stop","title":"Usage"},{"location":"spectator/lang/rb/usage/#project","text":"Source RubyGems Product Lifecycle: Alpha Module Name: netflix-spectator-rb This implements a basic Spectator library for instrumenting Ruby applications, sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/rb/usage/#install-library","text":"Install the library from RubyGems: gem install spectator-rb","title":"Install Library"},{"location":"spectator/lang/rb/usage/#instrumenting-code","text":"require 'spectator' class Response attr_accessor :status , :size def initialize ( status , size ) @status = status @size = size end end class Request attr_reader :country def initialize ( country ) @country = country end end class ExampleServer def initialize ( registry ) @registry = registry @req_count_id = registry . new_id ( 'server.requestCount' ) @req_latency = registry . timer ( 'server.requestLatency' ) @resp_sizes = registry . distribution_summary ( 'server.responseSizes' ) end def expensive_computation ( request ) # ... end def handle_request ( request ) start = @registry . clock . monotonic_time # initialize response response = Response . new ( 200 , 64 ) # Update the counter id with dimensions based on the request. The # counter will then be looked up in the registry which should be # fairly cheap, such as lookup of id object in a map # However, it is more expensive than having a local variable set # to the counter. cnt_id = @req_count_id . with_tag ( :country , request . country ) . with_tag ( :status , response . status . to_s ) @registry . counter_with_id ( cnt_id ) . increment # ... @req_latency . record ( @registry . clock . monotonic_time - start ) @resp_sizes . record ( response . size ) # timers can also time a given block # this is equivalent to: # start = @registry.clock.monotonic_time # expensive_computation(request) # @registry.timer('server.computeTime').record(@registry.clock.monotonic_time - start) @registry . timer ( 'server.computeTime' ) . time { expensive_computation ( request ) } # ... end end config = { common_tags : { :'nf.app' => 'foo' }, frequency : 0 . 5 , uri : 'http://localhost:8080/api/v4/publish' } registry = Spectator :: Registry . new ( config ) registry . start server = ExampleServer . new ( registry ) # ... # process some requests requests = [ Request . new ( 'us' ), Request . new ( 'ar' ), Request . new ( 'ar' ) ] requests . each { | req | server . handle_request ( req ) } sleep ( 2 ) registry . stop","title":"Instrumenting Code"},{"location":"spectator/lang/rb/usage/#netflix-integration","text":"Add the internal configuration for the Spectator Ruby client, so that it can send metrics to an Atlas Aggregator cluster. If you are using the internal Artifactory, add the dependency to your Gemfile: gem 'netflix-spectator-config' gem 'netflix-spectator-rb' If you are not using the internal Artifactory, then you can do the following, replacing STASH_HOSTNAME_AND_PORT with appropriate values: gem 'netflix-spectator-config' , git : 'ssh://git@STASH_HOSTNAME_AND_PORT/cldmta/nflx-spectator-rb.git' gem 'netflix-spectator-rb' Once the configuration Gem is installed, it is used as follows: require 'spectator_config' require 'spectator' config = SpectatorConfig . config registry = Spectator :: Registry . new ( config ) registry . start # ... registry . stop","title":"Netflix Integration"},{"location":"spectator/lang/rb/meters/counter/","text":"","title":"Counters"},{"location":"spectator/lang/rb/meters/dist-summary/","text":"","title":"Distribution Summaries"},{"location":"spectator/lang/rb/meters/gauge/","text":"","title":"Gauges"},{"location":"spectator/lang/rb/meters/percentile-timer/","text":"","title":"Percentile Timers"},{"location":"spectator/lang/rb/meters/timer/","text":"","title":"Timers"},{"location":"spectator/patterns/cardinality-limiter/","text":"Cardinality Limiter \u00b6 Helper functions to help manage the cardinality of tag values. This should be used anywhere you cannot guarantee that the tag values being used are strictly bounded. There is support for two different modes: (1) selecting the first N values that are seen, or (2) selecting the most frequent N values that are seen. Example usage: class WebServer { // Limiter instance, should be shared for all uses of that tag value private final Function & lt ; String , String & gt ; pathLimiter = CardinalityLimiters . mostFrequent ( 10 ); private final Registry registry ; private final Id baseId ; public WebServer ( Registry registry ) { this . registry = registry ; this . baseId = registry . createId ( \"server.requestCount\" ); } public Response handleRequest ( Request req ) { Response res = doSomething ( req ); // Update metrics, use limiter to restrict the set of values for the // path and avoid an explosion String pathValue = pathLimiter . apply ( req . getPath ()); Id id = baseId . withTag ( \"path\" , pathValue ) . withTag ( \"status\" , res . getStatus ()); registry . counter ( id ). increment (); } }","title":"Cardinality Limiter"},{"location":"spectator/patterns/cardinality-limiter/#cardinality-limiter","text":"Helper functions to help manage the cardinality of tag values. This should be used anywhere you cannot guarantee that the tag values being used are strictly bounded. There is support for two different modes: (1) selecting the first N values that are seen, or (2) selecting the most frequent N values that are seen. Example usage: class WebServer { // Limiter instance, should be shared for all uses of that tag value private final Function & lt ; String , String & gt ; pathLimiter = CardinalityLimiters . mostFrequent ( 10 ); private final Registry registry ; private final Id baseId ; public WebServer ( Registry registry ) { this . registry = registry ; this . baseId = registry . createId ( \"server.requestCount\" ); } public Response handleRequest ( Request req ) { Response res = doSomething ( req ); // Update metrics, use limiter to restrict the set of values for the // path and avoid an explosion String pathValue = pathLimiter . apply ( req . getPath ()); Id id = baseId . withTag ( \"path\" , pathValue ) . withTag ( \"status\" , res . getStatus ()); registry . counter ( id ). increment (); } }","title":"Cardinality Limiter"},{"location":"spectator/patterns/gauge-poller/","text":"Gauge Poller \u00b6 Helper for polling gauges in a background thread. A shared executor is used with a single thread. If registered gauge methods are cheap as they should be, then this should be plenty of capacity to process everything regularly. If not, then this will help limit the damage to a single core and avoid causing problems for the application.","title":"Gauge Poller"},{"location":"spectator/patterns/gauge-poller/#gauge-poller","text":"Helper for polling gauges in a background thread. A shared executor is used with a single thread. If registered gauge methods are cheap as they should be, then this should be plenty of capacity to process everything regularly. If not, then this will help limit the damage to a single core and avoid causing problems for the application.","title":"Gauge Poller"},{"location":"spectator/patterns/interval-counter/","text":"Interval Counter \u00b6 A counter that also keeps track of the time since last update.","title":"Interval Counter"},{"location":"spectator/patterns/interval-counter/#interval-counter","text":"A counter that also keeps track of the time since last update.","title":"Interval Counter"},{"location":"spectator/patterns/long-task-timer/","text":"Long Task Timer \u00b6 Timer intended to track a small number of long running tasks. Example would be something like a batch hadoop job. Though \"long running\" is a bit subjective the assumption is that anything over a minute is long running. A regular Timer just records the duration and has no information until the task is complete. As an example, consider a chart showing request latency to a typical web server. The expectation is many short requests, so the timer will be getting updated many times per second. Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally, all data can be refreshed in a few minutes. If the AWS services are having problems, it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata. The charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale: Long Task Timer:","title":"Long Task Timer"},{"location":"spectator/patterns/long-task-timer/#long-task-timer","text":"Timer intended to track a small number of long running tasks. Example would be something like a batch hadoop job. Though \"long running\" is a bit subjective the assumption is that anything over a minute is long running. A regular Timer just records the duration and has no information until the task is complete. As an example, consider a chart showing request latency to a typical web server. The expectation is many short requests, so the timer will be getting updated many times per second. Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally, all data can be refreshed in a few minutes. If the AWS services are having problems, it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata. The charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale: Long Task Timer:","title":"Long Task Timer"},{"location":"spectator/patterns/percentile-timer/","text":"Percentile Timers \u00b6 A Timer that buckets the counts, to allow for estimating percentiles. This Timer type will track the data distribution for the timer by maintaining a set of Counters. The distribution can then be used on the server side to estimate percentiles, while still allowing for arbitrary slicing and dicing based on dimensions. Warning Please be selective about what you measure as there is significant overhead on both the client and storage side. Usually only one or two key performance indicators (KPIs) per application. Limit the tag cardinality as much as possible. For example, only include an endpoint tag, not a user agent or response code. Use one of the other meter types whenever possible. In order to maintain the data distribution, they have a higher storage cost, with a worst-case of up to 300X that of a standard Timer. Be diligent about any additional dimensions added to Percentile Timers and ensure that they have a small bounded cardinality. In addition, it is highly recommended to set a range, whenever possible, to restrict the worst case overhead. When using the builder, the range will default from 10 ms to 1 minute. Based on data at Netflix, this is the most common range for request latencies and restricting to this window reduces the worst case multiple from 276X to 58X. Range Recommendations \u00b6 The range should be the SLA boundary or failure point for the activity. Explicitly setting the range allows us to optimize for the important range of values and reduce the overhead associated with tracking the data distribution. For example, suppose you are making a client call and timeout after 10 seconds. Setting the range to 10 seconds will restrict the possible set of buckets used to those approaching the boundary. So we can still detect if it is nearing failure, but percentiles that are further away from the range may be inflated compared to the actual value. Bucket Distribution \u00b6 The set of buckets is generated by using powers of 4 and incrementing by one-third of the previous power of 4 in between as long as the value is less than the next power of 4 minus the delta. Base: 1, 2, 3 4 (4^1), delta = 1 5, 6, 7, ..., 14, 16 (4^2), delta = 5 21, 26, 31, ..., 56, 64 (4^3), delta = 21 ...","title":"Percentile Timer"},{"location":"spectator/patterns/percentile-timer/#percentile-timers","text":"A Timer that buckets the counts, to allow for estimating percentiles. This Timer type will track the data distribution for the timer by maintaining a set of Counters. The distribution can then be used on the server side to estimate percentiles, while still allowing for arbitrary slicing and dicing based on dimensions. Warning Please be selective about what you measure as there is significant overhead on both the client and storage side. Usually only one or two key performance indicators (KPIs) per application. Limit the tag cardinality as much as possible. For example, only include an endpoint tag, not a user agent or response code. Use one of the other meter types whenever possible. In order to maintain the data distribution, they have a higher storage cost, with a worst-case of up to 300X that of a standard Timer. Be diligent about any additional dimensions added to Percentile Timers and ensure that they have a small bounded cardinality. In addition, it is highly recommended to set a range, whenever possible, to restrict the worst case overhead. When using the builder, the range will default from 10 ms to 1 minute. Based on data at Netflix, this is the most common range for request latencies and restricting to this window reduces the worst case multiple from 276X to 58X.","title":"Percentile Timers"},{"location":"spectator/patterns/percentile-timer/#range-recommendations","text":"The range should be the SLA boundary or failure point for the activity. Explicitly setting the range allows us to optimize for the important range of values and reduce the overhead associated with tracking the data distribution. For example, suppose you are making a client call and timeout after 10 seconds. Setting the range to 10 seconds will restrict the possible set of buckets used to those approaching the boundary. So we can still detect if it is nearing failure, but percentiles that are further away from the range may be inflated compared to the actual value.","title":"Range Recommendations"},{"location":"spectator/patterns/percentile-timer/#bucket-distribution","text":"The set of buckets is generated by using powers of 4 and incrementing by one-third of the previous power of 4 in between as long as the value is less than the next power of 4 minus the delta. Base: 1, 2, 3 4 (4^1), delta = 1 5, 6, 7, ..., 14, 16 (4^2), delta = 5 21, 26, 31, ..., 56, 64 (4^3), delta = 21 ...","title":"Bucket Distribution"},{"location":"spectator/patterns/polled-meter/","text":"Polled Meter \u00b6 Helper for configuring a meter that will receive a value by regularly polling the source in the background. Example usage: Registry registry = ... AtomicLong connections = PolledMeter . using ( registry ) . withName ( \"server.currentConnections\" ) . monitorValue ( new AtomicLong ()); // When a connection is added connections . incrementAndGet (); // When a connection is removed connections . decrementAndGet (); Polling frequency will depend on the underlying Registry implementation, but users should assume it will be frequently checked and that the provided function is cheap. Users should keep in mind that polling will not capture all activity, just sample it at some frequency. For example, if monitoring a queue, then a meter will only tell you the last sampled size when the value is reported. If more details are needed, then use an alternative type and ensure that all changes are reported when they occur. For example, consider tracking the number of currently established connections to a server. Using a polled meter will show the last sampled number when reported. An alternative would be to report the number of connections to a Distribution Summary every time a connection is added or removed. The distribution summary would provide more accurate tracking such as max and average number of connections across an interval of time. The polled meter would not provide that level of detail. If multiple values are monitored with the same id, then the values will be aggregated and the sum will be reported. For example, registering multiple meters for active threads in a thread pool with the same id would produce a value that is the overall number of active threads. For other behaviors, manage it on the user side and avoid multiple registrations.","title":"Polled Meter"},{"location":"spectator/patterns/polled-meter/#polled-meter","text":"Helper for configuring a meter that will receive a value by regularly polling the source in the background. Example usage: Registry registry = ... AtomicLong connections = PolledMeter . using ( registry ) . withName ( \"server.currentConnections\" ) . monitorValue ( new AtomicLong ()); // When a connection is added connections . incrementAndGet (); // When a connection is removed connections . decrementAndGet (); Polling frequency will depend on the underlying Registry implementation, but users should assume it will be frequently checked and that the provided function is cheap. Users should keep in mind that polling will not capture all activity, just sample it at some frequency. For example, if monitoring a queue, then a meter will only tell you the last sampled size when the value is reported. If more details are needed, then use an alternative type and ensure that all changes are reported when they occur. For example, consider tracking the number of currently established connections to a server. Using a polled meter will show the last sampled number when reported. An alternative would be to report the number of connections to a Distribution Summary every time a connection is added or removed. The distribution summary would provide more accurate tracking such as max and average number of connections across an interval of time. The polled meter would not provide that level of detail. If multiple values are monitored with the same id, then the values will be aggregated and the sum will be reported. For example, registering multiple meters for active threads in a thread pool with the same id would produce a value that is the overall number of active threads. For other behaviors, manage it on the user side and avoid multiple registrations.","title":"Polled Meter"},{"location":"spectator/specs/ipc/","text":"IPC \u00b6 This is a description of the Common IPC Metrics that can be published by various IPC libraries, with the goal of allowing consolidated monitoring and analysis across differing IPC implementations. Dimensions Common to All Metrics \u00b6 Not all dimensions are applicable for all of the metrics, and later in the sections for each specific metric, the applicable dimensions are specified. Also note that not all dimensions have been implemented or are applicable for all implementations. ipc.protocol : A short name of the network protocol in use, eg. grpc , http_1 , http_2 , udp , etc ... ipc.vip : The Eureka VIP address used to find the the server. ipc.result : Was this considered by the implementation to be successful. Allowed Values = [ success , failure ]. ipc.status : One of a predefined list of status values indicating the general result, eg. success, bad_request, timeout, etc\u2026 See the ipc.status values section below . ipc.status.detail : For cases where the ipc.status needs to be further subdivided, this tag can hold an additional more specific detail, likely ipc-implementation specific. eg status of connection_error and detail of no_servers / connect_timeout / ssl_handshake_failure. ipc.failure.injected : Indicates that an artificial failure was injected into the request processing for testing purposes. The outcome of that failure will be reflected in the other error tags. Allowed Values = [true] ipc.endpoint : The name of the endpoint/function/feature the message was sent to within the server (eg. the URL path prefix for a java servlet, or the grpc endpoint name). ipc.attempt : Which attempt at sending this message is this. Allowed Values = [ initial , second , third_up ] ( initial is the first attempt, second is 2nd attempt but first retry , third_up means third or higher attempt). ipc.attempt.final : Indicates if this request was the final attempt of potentially multiple retry attempts. Allowed Values = [ true , false ]. ipc.server.app : The nf.app of the server the message is being sent to . ipc.server.cluster : The nf.cluster of the server the message is being sent to . ipc.server.asg : The nf.asg of the server the message is being sent to . ipc.client.app : The nf.app of the server the message is being sent from . ipc.client.cluster : The nf.cluster of the server the message is being sent from . ipc.client.asg : The nf.asg of the server the message is being sent from . owner : The library/impl publishing the metrics, eg. evcache, zuul, grpc, nodequark, platform_1_ipc, geoclient, etc ... id : Conceptual name of service. Equivalent of RestClient name in NIWS. Allowed Values for ipc.status Dimension \u00b6 success : The request was successfully processed and responded to, as far as the client or server know. bad_request : There was a problem with the clients' request causing it not to be fulfilled. unexpected_error : The client or server encountered an unexpected error processing the request. connection_error : There was an error with the underlying network connection either during establishment or while in use. unavailable : There were no servers available to process the request. throttled : The request was rejected due to the client or server considering the server to be above capacity. timeout : The request could not or would not be complete within the configured threshold (either on client or server). cancelled : The client cancelled the request before it was completed. access_denied : The request was denied access for authentication or authorization reasons. Server Metrics \u00b6 ipc.server.call \u00b6 This is a percentile timer that is recorded for each inbound message to a server. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.call.size.inbound \u00b6 This is a distribution summary of the size in bytes of inbound messages received by a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.call.size.outbound \u00b6 This is a distribution summary of the size in bytes of outbound messages sent from a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.inflight \u00b6 This is a distribution summary that shows the number of inbound IPC messages currently being processed in a server. Unit: inflight message count Dimensions: ipc.protocol ipc.endpoint ipc.client.app ipc.client.cluster ipc.client.asg owner id Client Metrics \u00b6 ipc.client.call \u00b6 This is a percentile timer that is recorded for each outbound message from a client. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.attempt.final ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.call.size.inbound \u00b6 This is a distribution summary of the size in bytes of inbound messages received by a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.call.size.outbound \u00b6 This is a distribution summary of the size in bytes of outbound messages sent from a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.inflight \u00b6 This is a distribution summary that shows the number of currently outstanding outbound IPC messages from a client. Unit: inflight message count Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"IPC"},{"location":"spectator/specs/ipc/#ipc","text":"This is a description of the Common IPC Metrics that can be published by various IPC libraries, with the goal of allowing consolidated monitoring and analysis across differing IPC implementations.","title":"IPC"},{"location":"spectator/specs/ipc/#dimensions-common-to-all-metrics","text":"Not all dimensions are applicable for all of the metrics, and later in the sections for each specific metric, the applicable dimensions are specified. Also note that not all dimensions have been implemented or are applicable for all implementations. ipc.protocol : A short name of the network protocol in use, eg. grpc , http_1 , http_2 , udp , etc ... ipc.vip : The Eureka VIP address used to find the the server. ipc.result : Was this considered by the implementation to be successful. Allowed Values = [ success , failure ]. ipc.status : One of a predefined list of status values indicating the general result, eg. success, bad_request, timeout, etc\u2026 See the ipc.status values section below . ipc.status.detail : For cases where the ipc.status needs to be further subdivided, this tag can hold an additional more specific detail, likely ipc-implementation specific. eg status of connection_error and detail of no_servers / connect_timeout / ssl_handshake_failure. ipc.failure.injected : Indicates that an artificial failure was injected into the request processing for testing purposes. The outcome of that failure will be reflected in the other error tags. Allowed Values = [true] ipc.endpoint : The name of the endpoint/function/feature the message was sent to within the server (eg. the URL path prefix for a java servlet, or the grpc endpoint name). ipc.attempt : Which attempt at sending this message is this. Allowed Values = [ initial , second , third_up ] ( initial is the first attempt, second is 2nd attempt but first retry , third_up means third or higher attempt). ipc.attempt.final : Indicates if this request was the final attempt of potentially multiple retry attempts. Allowed Values = [ true , false ]. ipc.server.app : The nf.app of the server the message is being sent to . ipc.server.cluster : The nf.cluster of the server the message is being sent to . ipc.server.asg : The nf.asg of the server the message is being sent to . ipc.client.app : The nf.app of the server the message is being sent from . ipc.client.cluster : The nf.cluster of the server the message is being sent from . ipc.client.asg : The nf.asg of the server the message is being sent from . owner : The library/impl publishing the metrics, eg. evcache, zuul, grpc, nodequark, platform_1_ipc, geoclient, etc ... id : Conceptual name of service. Equivalent of RestClient name in NIWS.","title":"Dimensions Common to All Metrics"},{"location":"spectator/specs/ipc/#allowed-values-for-ipcstatus-dimension","text":"success : The request was successfully processed and responded to, as far as the client or server know. bad_request : There was a problem with the clients' request causing it not to be fulfilled. unexpected_error : The client or server encountered an unexpected error processing the request. connection_error : There was an error with the underlying network connection either during establishment or while in use. unavailable : There were no servers available to process the request. throttled : The request was rejected due to the client or server considering the server to be above capacity. timeout : The request could not or would not be complete within the configured threshold (either on client or server). cancelled : The client cancelled the request before it was completed. access_denied : The request was denied access for authentication or authorization reasons.","title":"Allowed Values for ipc.status Dimension"},{"location":"spectator/specs/ipc/#server-metrics","text":"","title":"Server Metrics"},{"location":"spectator/specs/ipc/#ipcservercall","text":"This is a percentile timer that is recorded for each inbound message to a server. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call"},{"location":"spectator/specs/ipc/#ipcservercallsizeinbound","text":"This is a distribution summary of the size in bytes of inbound messages received by a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call.size.inbound"},{"location":"spectator/specs/ipc/#ipcservercallsizeoutbound","text":"This is a distribution summary of the size in bytes of outbound messages sent from a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call.size.outbound"},{"location":"spectator/specs/ipc/#ipcserverinflight","text":"This is a distribution summary that shows the number of inbound IPC messages currently being processed in a server. Unit: inflight message count Dimensions: ipc.protocol ipc.endpoint ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.inflight"},{"location":"spectator/specs/ipc/#client-metrics","text":"","title":"Client Metrics"},{"location":"spectator/specs/ipc/#ipcclientcall","text":"This is a percentile timer that is recorded for each outbound message from a client. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.attempt.final ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call"},{"location":"spectator/specs/ipc/#ipcclientcallsizeinbound","text":"This is a distribution summary of the size in bytes of inbound messages received by a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call.size.inbound"},{"location":"spectator/specs/ipc/#ipcclientcallsizeoutbound","text":"This is a distribution summary of the size in bytes of outbound messages sent from a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call.size.outbound"},{"location":"spectator/specs/ipc/#ipcclientinflight","text":"This is a distribution summary that shows the number of currently outstanding outbound IPC messages from a client. Unit: inflight message count Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.inflight"}]}