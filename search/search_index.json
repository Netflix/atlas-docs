{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atlas \u00b6 Warning These docs are still a work in progress. For now you should still refer to the Atlas Wiki . Atlas was developed by Netflix to manage dimensional time series data for near real-time operational insight. Atlas features in-memory data storage, allowing it to gather and report very large numbers of metrics, very quickly. Atlas captures operational intelligence. Whereas business intelligence is data gathered for analyzing trends over time, operational intelligence provides a picture of what is currently happening within a system. Atlas was built because the existing systems Netflix was using for operational intelligence were not able to cope with the increase in metrics we were seeing as we expanded our operations in the cloud. In 2011, we were monitoring 2 million metrics related to our streaming systems. By 2014, we were at 1.2 billion metrics and the numbers continue to rise. Atlas is designed to handle this large quantity of data and can scale with the hardware we use to analyze and store it. For details and background on the project please read through the overview page. Check out the [[getting started]] page for an introduction to using Atlas in the cloud environment. Once you've explored the example, check out the stack language references to see the various types of information you can access.","title":"Introduction"},{"location":"#atlas","text":"Warning These docs are still a work in progress. For now you should still refer to the Atlas Wiki . Atlas was developed by Netflix to manage dimensional time series data for near real-time operational insight. Atlas features in-memory data storage, allowing it to gather and report very large numbers of metrics, very quickly. Atlas captures operational intelligence. Whereas business intelligence is data gathered for analyzing trends over time, operational intelligence provides a picture of what is currently happening within a system. Atlas was built because the existing systems Netflix was using for operational intelligence were not able to cope with the increase in metrics we were seeing as we expanded our operations in the cloud. In 2011, we were monitoring 2 million metrics related to our streaming systems. By 2014, we were at 1.2 billion metrics and the numbers continue to rise. Atlas is designed to handle this large quantity of data and can scale with the hardware we use to analyze and store it. For details and background on the project please read through the overview page. Check out the [[getting started]] page for an introduction to using Atlas in the cloud environment. Once you've explored the example, check out the stack language references to see the various types of information you can access.","title":"Atlas"},{"location":"overview/","text":"Atlas is the system Netflix uses to manage dimensional time-series data for near real-time operational insight. It was primarily created to address issues with scale and query capability in the previous system. History \u00b6 In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume: Rolling pushes to Red/Black deployments . Leveraging auto-scaling for large clusters . Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe . This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed. Since that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years: The growth in raw volume required increased query capability to actually use the data. Goals \u00b6 The main goals for Atlas were to build a system that provided: A Common API Scale Dimensionality Common API \u00b6 Epic did a number of things really well that we didn't want to lose when transitioning. In particular: Normalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking Many of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data. Scale \u00b6 As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request. Dimensionality \u00b6 This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like: com.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US That breaks down to: Key Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US Since it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions. Query Layer \u00b6 In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup: We have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point. Allowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system. Stack Language \u00b6 One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include: Embedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency Graph Example \u00b6 To illustrate, this is a sample graph image: This graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing . If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability): http://atlas/api/v1/graph ?tz=UTC &e=2012-01-01T08:00 &s=e-8h &w=500 &h=150 &l=0 &q=nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, :dup,10,0.1,0.02,:des, 0.85,:mul, :2over,:lt, :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend Adding some comments to the stack expression to explain a bit what is going on: # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction # using the copy on the top of the stack. For # a description of the parameters see the DES # reference page. 10,0.1,0.02,:des, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Before After # 4. 4. actual # 3. 3. prediction # 2. actual 2. actual # 1. prediction 1. prediction :2over, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend See the stack language page for more information. Memory Storage \u00b6 Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap. Speed \u00b6 The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small. As an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line. Though obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation. Resilience \u00b6 What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb: Data becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully As a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain. With this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn't need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror. The publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API . Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can't update data we would have partial loss for a single shard if the same shard was missing in another mirror. Historical data can also fail in which case graphs would not be able to show data for some older windows. This doesn't have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn't show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle. After data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters. Cost \u00b6 Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs. The first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica. The second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly: Cluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases Using these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed. Snapshot of index sizes for one region in our environment: < 6h < 4d < 16d Ecosystem \u00b6 Internally there is a lot of tooling and infrastructure built up around Atlas. We are planning to open source many of these tools as time permits. This project is the first step for that with the query layer and some of the in-heap memory storage. Some additional parts that should come in the future: User interfaces Main UI for browsing data and constructing queries. Dashboards Alerts Platform Inline aggregation of reported data before storage layer Storage options using off-heap memory and lucene Percentile backend Publish and persistence applications EMR processing for computing rollups and analysis Poller for SNMP, healthchecks, etc Client Supports integrating servo with Atlas Local rollups and alerting Analytics Metrics volume report Canary analysis Outlier and anomaly detection These projects were originally developed and run internally and thus only needed to be setup by our team and assume many internal infrastructure pieces to run. There is a goal to try and make this easier, but it will take some time.","title":"Overview"},{"location":"overview/#history","text":"In May of 2011, Netflix was using a home-grown solution called Epic to manage time-series data. Epic was a combination of perl CGI scripts, RRDTool logging, and MySQL. We were tracking around 2M distinct time series and the monitoring system was regularly failing to keep up with the volume of data. In addition there were a number of trends in the company which presaged a drastic increase in metric volume: Rolling pushes to Red/Black deployments . Leveraging auto-scaling for large clusters . Netflix has always used auto-scaling groups in AWS, but initially most were configured with fixed size and just used as a group and to replace instances. Expansion internationally into Latin America and Europe . This led to an increase in the number of countries being tracked for key metrics and for Europe it was the first move into additional AWS regions. With additional regions we also wanted to have better isolation so a problem with monitoring in one region would not impact another, but at the same time have a mechanism to provide a global view if needed. Since that time the metric volume has continued to grow quickly. The graph below shows the increase in metrics measured over last few years: The growth in raw volume required increased query capability to actually use the data.","title":"History"},{"location":"overview/#goals","text":"The main goals for Atlas were to build a system that provided: A Common API Scale Dimensionality","title":"Goals"},{"location":"overview/#common-api","text":"Epic did a number of things really well that we didn't want to lose when transitioning. In particular: Normalization and consolidation Flexible legends that scale independently of the chart Math, especially handling of NaN values representing no data Holt-Winters used for alerting Visualization options Deep linking Many of these are capabilities that are provided by the RRDTool library Epic was using, but most alternatives we looked at fell short in these categories. In addition, we have uses for other 3rd party services like CloudWatch and it is desirable to have common query capability for that data.","title":"Common API"},{"location":"overview/#scale","text":"As indicated in the history section, metrics volume was growing and we needed a system that could keep up. For a long time our biggest concern was write volume, however, we also wanted to scale in terms of the amount of data we could read or aggregate as part of a graph request.","title":"Scale"},{"location":"overview/#dimensionality","text":"This is a decision that was made because users were already doing it in ways that were hard to support. Epic only support a simple name with some special case system dimensions of cluster and node. Many users were creating names like: com.netflix.eds.nccp.successful.requests.uiversion.nccprt-authorization.devtypid-101.clver-PHL_0AB.uiver-UI_169_mid.geo-US That breaks down to: Key Value name com.netflix.eds.nccp.successful.requests.uiversion nccprt authorization devtypid 101 clver PHL_0AB uiver UI_169_mid geo US Since it was all mangled into a name with different conventions by team, users would have to resort to complex regular expressions to slice and dice the data based on the dimensions.","title":"Dimensionality"},{"location":"overview/#query-layer","text":"In order to get a common API, have flexibility for backend implementations, and provide merged views across backends we built a query layer that can be hierarchically composed. The diagram below shows the main Netflix setup: We have isolated regional deployments in each region we operate in as well as a global deployment that can combine the results from multiple regions. The query and aggregation operations can be performed on the fan out so most of the big summarization operations will distribute the computation across the tree and typically to an optimized storage layer at some point. Allowing the query and rendering layer to work on multiple backends also makes it easier for us to consider transitioning to other backends in the future such as OpenTSDB or InfluxDB. Switching to Atlas one of the biggest hurdles was compatibility and transitioning to the new system.","title":"Query Layer"},{"location":"overview/#stack-language","text":"One of our key requirements was to be able to have deep links into a particular chart and to be able to reliably pass around or embed these images via email, wikis, html pages, etc. In addition, the user who receives the link should be able to tweak the result. Atlas uses a simple stack language that has a minimal punctuation and allows arbitrarily complex graph expressions to be encoded in a URL friendly way. This means that all images can be accessed using a GET request. The stack language is also simple to parse and interpret, allowing it to be easily consumed from a variety of tools. The core features include: Embedding and linking using a GET request URL friendly stack language Few special symbols (comma, colon, parenthesis) Easy to extend Basic operations Query: and, or, equal, regex, has key, not Aggregation: sum, count, min, max, group by Consolidation: aggregate across time Math: add, subtract, multiply, etc Boolean: and, or, lt, gt, etc Graph settings: legends, area, transparency","title":"Stack Language"},{"location":"overview/#graph-example","text":"To illustrate, this is a sample graph image: This graph shows the number of requests per second and compares that with a prediction line generated using double exponential smoothing . If the number of requests per second falls below the prediction, it indicates an alert would trigger using the vertical spans. The url to generate this image follows (newlines added for readability): http://atlas/api/v1/graph ?tz=UTC &e=2012-01-01T08:00 &s=e-8h &w=500 &h=150 &l=0 &q=nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, :dup,10,0.1,0.02,:des, 0.85,:mul, :2over,:lt, :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend Adding some comments to the stack expression to explain a bit what is going on: # Query to generate the input line nf.cluster,alerttest,:eq, name,requestsPerSecond,:eq,:and, :sum, # Create a copy on the stack :dup, # Apply a DES function to generate a prediction # using the copy on the top of the stack. For # a description of the parameters see the DES # reference page. 10,0.1,0.02,:des, # Used to set a threshold. The prediction should # be roughly equal to the line, in this case the # threshold would be 85% of the prediction. 0.85,:mul, # Before After # 4. 4. actual # 3. 3. prediction # 2. actual 2. actual # 1. prediction 1. prediction :2over, # Create a boolean signal line that is 1 # for datapoints where the actual value is # less than the prediction and 0 where it # is greater than or equal the prediction. # The 1 values are where the alert should # trigger. :lt, # Apply presentation details. :rot,$name,:legend, :rot,prediction,:legend, :rot,:vspan,60,:alpha,alert+triggered,:legend See the stack language page for more information.","title":"Graph Example"},{"location":"overview/#memory-storage","text":"Storage for Atlas has been a bit of a sore point. We have tried many backends and ended up moving more and more to a model where pretty much all data is stored in memory either in or off the java heap.","title":"Memory Storage"},{"location":"overview/#speed","text":"The primary goal for Atlas is to support queries over dimensional time series data so we can slice and dice to drill down into problems. This means we frequently have a need to perform a large aggregations that involve many data points even though the final result set might be small. As an example consider a simple graph showing the number of requests per second hitting a service for the last 3 hours. Assuming minute resolution that is 180 datapoints for the final output. On a typical service we would get one time series per node showing the number of requests so if we have 100 nodes the intermediate result set is around 18k datapoints. For one service users went hog wild with dimensions breaking down requests by device (~1000s) and country (~50) leading to about 50k time series per node. If we still assume 100 nodes that is about 900M datapoints for the same 3h line. Though obviously we have to be mindful about the explosion of dimensions, we also want that where possible to be a decision based on cost and business value rather than a technical limitation.","title":"Speed"},{"location":"overview/#resilience","text":"What all has to be working in order for the monitoring system to work? If it falls over what is involved in getting it back up? Our focus is primarily operational insight so the top priority is to be able to determine what is going on right now. This leads to the following rules of thumb: Data becomes exponentially less important as it gets older Restoring service is more important than preventing data loss Try to degrade gracefully As a result the internal Atlas deployment breaks up the data into multiple windows based on the window of data they contain. With this setup we can show the last 6h of data as long as clients can successfully publish. The data is all in memory sharded across machines in the 6h clusters. Because the data and index are all in memory on the local node each instance is self contained and doesn't need any external service to function. We typically run multiple mirrors of the 6h cluster so data is replicated and we can handle loss of an instance. In AWS we run each mirror in a different zone so that a zone failure will only impact a single mirror. The publish cluster needs to know all of the instance in the mirror cluster and takes care of splitting the traffic up so it goes to the correct shard. The set of mirror instances and shards are assigned based on slots from the Edda autoScalingGroups API . Since the set of instances for the mirrors change rarely, the publish instances can cache the Edda response and still retain successfully publish most data if Edda fails. If an instance is replaced and we can't update data we would have partial loss for a single shard if the same shard was missing in another mirror. Historical data can also fail in which case graphs would not be able to show data for some older windows. This doesn't have to be fully continuous, for example a common use case for us is to look at week-over-week (WoW) charts even though the span of the chart might only be a few hours. If the < 4d cluster fails but the < 16d cluster is functioning we could still serve that graph even though we couldn't show a continuous graph for the full week. A graph would still be shown but would be missing data in the middle. After data is written to the mirrors, they will flush to a persistence layer that is responsible for writing the data to the long term storage in S3. The data at full resolution is kept in S3 and we use hadoop (Elastic MapReduce) for processing the data to perform corrective merging of data from the mirrors, generate reports, and perform rollups into a form that can be loaded into the historical clusters.","title":"Resilience"},{"location":"overview/#cost","text":"Keeping all data in memory is expensive in-particular with the large growth rate of data. The combination of dimensionality and time based partitioning used for resilience also give us a way to help manage costs. The first way is in controlling the number of replicas. In most cases we are using replicas for redundancy not to provide additional query capacity. For historical data that can be reloaded from stable storage we typically run only one replica as the duration of partial downtime was not deemed to be worth the cost for an additional replica. The second way is as part of the hadoop processing we can compute rollups so that we have a much smaller data volume to load in historical clusters. At Netflix the typical policy is roughly: Cluster Policy < 6h Keeps all data received < 4d ago Keeps most data, we do early rollup by dropping the node dimension on some business metrics < 16d ago Rollup by dropping the node dimension on all metrics older Explicit whitelist, typically recommend BI systems for these use-cases Using these policies we get greatly reduced index sizes for the number of distinct time series despite a significant amount of churn. With auto-scaling and red/black deployment models the set of instances change frequently so typically the intersection of distinct time series from one day to the next is less than 50%. Rollups target the dimensions which lead to that churn giving us much smaller index sizes. Also, in many cases dimensions like node that lead to this increase become less relevant after the node goes away. Deep-dive or investigative use-cases can still access the data using hadoop if needed. Snapshot of index sizes for one region in our environment: < 6h < 4d < 16d","title":"Cost"},{"location":"overview/#ecosystem","text":"Internally there is a lot of tooling and infrastructure built up around Atlas. We are planning to open source many of these tools as time permits. This project is the first step for that with the query layer and some of the in-heap memory storage. Some additional parts that should come in the future: User interfaces Main UI for browsing data and constructing queries. Dashboards Alerts Platform Inline aggregation of reported data before storage layer Storage options using off-heap memory and lucene Percentile backend Publish and persistence applications EMR processing for computing rollups and analysis Poller for SNMP, healthchecks, etc Client Supports integrating servo with Atlas Local rollups and alerting Analytics Metrics volume report Canary analysis Outlier and anomaly detection These projects were originally developed and run internally and thus only needed to be setup by our team and assume many internal infrastructure pieces to run. There is a goal to try and make this easier, but it will take some time.","title":"Ecosystem"},{"location":"api/fetch/","text":"","title":"Fetch"},{"location":"api/graph/","text":"","title":"Graph"},{"location":"api/tags/","text":"","title":"Tags"},{"location":"asl/alerting-expressions/","text":"The stack language provides some basic techniques to convert an input line into a set of signals that can be used to trigger and visualize alert conditions. This section assumes a familiarity with the stack language and the alerting philosophy . Signal Line \u00b6 A signal line is a time series that indicates whether or not a condition is true for a particular interval. They are modelled by having zero indicate false and non-zero, typically 1, indicating true. Alerting expressions map some input time series to a set of signal lines that indicate true when in a triggering state. Threshold Alerts \u00b6 To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum Lets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query: The result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine. Dampening \u00b6 Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurrences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count. Input Rolling Count Dampened Signal nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count , 4, :gt Visualization \u00b6 A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph. nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :2over , :gt , :vspan , 40, :alpha , triggered, :legend , :rot , input, :legend , :rot , threshold, :legend , :rot Summary \u00b6 You should now know the basics of crafting an alert expression using the stack language. Other topics that may be of interest: Alerting Philosophy : overview of best practices associated with alerts. Stack Language Reference : comprehensive list of avialable operators. DES : double exponential smoothing. A technique for detecting anomalies in normally clean input signals where a precise threshold is unknown. For example, the requests per second hitting a service.","title":"Alerting Expressions"},{"location":"asl/alerting-expressions/#signal-line","text":"A signal line is a time series that indicates whether or not a condition is true for a particular interval. They are modelled by having zero indicate false and non-zero, typically 1, indicating true. Alerting expressions map some input time series to a set of signal lines that indicate true when in a triggering state.","title":"Signal Line"},{"location":"asl/alerting-expressions/#threshold-alerts","text":"To start we need an input metric. For this example the input will be a sample metric showing high CPU usage for a period: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum Lets say we want to trigger an alert when the CPU usage goes above 80%. To do that simply use the :gt operator and append 80,:gt to the query: The result is a signal line that is non-zero, typically 1, when in a triggering state and zero when everything is fine.","title":"Threshold Alerts"},{"location":"asl/alerting-expressions/#dampening","text":"Our threshold alert above will trigger if the CPU usage is ever recorded to be above the threshold. Alert conditions are often combined with a check for the number of occurrences. This is done by using the :rolling-count operator to get a line showing how many times the input signal has been true withing a specified window and then applying a second threshold to the rolling count. Input Rolling Count Dampened Signal nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :gt , 5, :rolling-count , 4, :gt","title":"Dampening"},{"location":"asl/alerting-expressions/#visualization","text":"A signal line is useful to tell whether or not something is in a triggered state, but can be difficult for a person to follow. Alert expressions can be visualized by showing the input, threshold, and triggering state on the same graph. nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and , :sum , 80, :2over , :gt , :vspan , 40, :alpha , triggered, :legend , :rot , input, :legend , :rot , threshold, :legend , :rot","title":"Visualization"},{"location":"asl/alerting-expressions/#summary","text":"You should now know the basics of crafting an alert expression using the stack language. Other topics that may be of interest: Alerting Philosophy : overview of best practices associated with alerts. Stack Language Reference : comprehensive list of avialable operators. DES : double exponential smoothing. A technique for detecting anomalies in normally clean input signals where a precise threshold is unknown. For example, the requests per second hitting a service.","title":"Summary"},{"location":"asl/tutorial/","text":"Introduction \u00b6 What is a Stack Language? \u00b6 Atlas Stack Language \u00b6 Basic Operators \u00b6 Choosing \u00b6 Aggregation \u00b6 Math \u00b6 Unary Operators \u00b6 Binary Operators \u00b6 Stateful Operators \u00b6 Filtering Operators \u00b6 Presentation \u00b6 Stack Manipulation \u00b6 Execution Modes \u00b6","title":"Tutorial"},{"location":"asl/tutorial/#introduction","text":"","title":"Introduction"},{"location":"asl/tutorial/#what-is-a-stack-language","text":"","title":"What is a Stack Language?"},{"location":"asl/tutorial/#atlas-stack-language","text":"","title":"Atlas Stack Language"},{"location":"asl/tutorial/#basic-operators","text":"","title":"Basic Operators"},{"location":"asl/tutorial/#choosing","text":"","title":"Choosing"},{"location":"asl/tutorial/#aggregation","text":"","title":"Aggregation"},{"location":"asl/tutorial/#math","text":"","title":"Math"},{"location":"asl/tutorial/#unary-operators","text":"","title":"Unary Operators"},{"location":"asl/tutorial/#binary-operators","text":"","title":"Binary Operators"},{"location":"asl/tutorial/#stateful-operators","text":"","title":"Stateful Operators"},{"location":"asl/tutorial/#filtering-operators","text":"","title":"Filtering Operators"},{"location":"asl/tutorial/#presentation","text":"","title":"Presentation"},{"location":"asl/tutorial/#stack-manipulation","text":"","title":"Stack Manipulation"},{"location":"asl/tutorial/#execution-modes","text":"","title":"Execution Modes"},{"location":"asl/ref/all/","text":"Warning Deprecated: use :by instead. This operation is primarily intended for debugging and results can be confusing unless you have detailed understanding of Atlas internals. Input Stack: Query \u21e8 Output Stack: DataExpr Avoid aggregation and output all time series that match the query.","title":"all"},{"location":"asl/ref/and/","text":"There are two variants of the :and operator. Choosing \u00b6 Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 AND q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if both of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 AND ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a AND b) where a and b are the corresponding intervals in the input time series. For example: Time a b a AND b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a and b are both non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt , :and","title":"and"},{"location":"asl/ref/and/#choosing","text":"Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 AND q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if both of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :and When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/and/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 AND ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a AND b) where a and b are the corresponding intervals in the input time series. For example: Time a b a AND b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a and b are both non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 310, :lt , :and","title":"Math"},{"location":"asl/ref/bottomk-others-avg/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes an average aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-avg","title":"bottomk-others-avg"},{"location":"asl/ref/bottomk-others-max/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a max aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-max","title":"bottomk-others-max"},{"location":"asl/ref/bottomk-others-min/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a min aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-min","title":"bottomk-others-min"},{"location":"asl/ref/bottomk-others-sum/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic and computes a sum aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk-others-sum","title":"bottomk-others-sum"},{"location":"asl/ref/bottomk/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the smallest value for the specified summary statistic . Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :bottomk In some cases it can be useful to see an aggregate summary of the other time series that were not part of the bottom set. This can be accomplished using the :bottomk-others-$(aggr) operators. Permitted aggregations are avg , max , min , and sum . For more details see: :bottomk-others-avg :bottomk-others-max :bottomk-others-min :bottomk-others-sum","title":"bottomk"},{"location":"asl/ref/by/","text":"Group by operator. There are two variants of the :by operator. Aggregation \u00b6 Input Stack: keys: List[String] AggregationFunction \u21e8 Output Stack: DataExpr Groups the matching time series by a set of keys and applies an aggregation to matches of the group. name,ssCpu, :re , (,name,), :by When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpu User alerttest i-0123 [1.0, 2.0, NaN] ssCpu System alerttest i-0123 [3.0, 4.0, 5.0] ssCpu User nccp i-0abc [8.0, 7.0, 6.0] ssCpu System nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpu User api i-0456 [1.0, 2.0, 2.0] The aggregation function will be applied independently for each group. In this example above there are two matching values for the group by key name . This leads to a final result of: Name Data ssCpuSystem [9.0, 11.0, 13.0] ssCpuUser [10.0, 11.0, 8.0] The name tag is included in the result set since it is used for the grouping. Math \u00b6 Input Stack: keys: List[String] TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Groups the time series from the input expression by a set of keys and applies an aggregation to matches of the group. The keys used for this grouping must be a subset of keys from the initial group by clause. Example: Before After name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by , :count , (,nf.cluster,), :by","title":"by"},{"location":"asl/ref/by/#aggregation","text":"Input Stack: keys: List[String] AggregationFunction \u21e8 Output Stack: DataExpr Groups the matching time series by a set of keys and applies an aggregation to matches of the group. name,ssCpu, :re , (,name,), :by When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpu User alerttest i-0123 [1.0, 2.0, NaN] ssCpu System alerttest i-0123 [3.0, 4.0, 5.0] ssCpu User nccp i-0abc [8.0, 7.0, 6.0] ssCpu System nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpu User api i-0456 [1.0, 2.0, 2.0] The aggregation function will be applied independently for each group. In this example above there are two matching values for the group by key name . This leads to a final result of: Name Data ssCpuSystem [9.0, 11.0, 13.0] ssCpuUser [10.0, 11.0, 8.0] The name tag is included in the result set since it is used for the grouping.","title":"Aggregation"},{"location":"asl/ref/by/#math","text":"Input Stack: keys: List[String] TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Groups the time series from the input expression by a set of keys and applies an aggregation to matches of the group. The keys used for this grouping must be a subset of keys from the initial group by clause. Example: Before After name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by name,sps, :eq , :sum , (,nf.cluster,nf.node,), :by , :count , (,nf.cluster,), :by","title":"Math"},{"location":"asl/ref/cf-avg/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be average.","title":"cf-avg"},{"location":"asl/ref/cf-max/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be max.","title":"cf-max"},{"location":"asl/ref/cf-min/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be min.","title":"cf-min"},{"location":"asl/ref/cf-sum/","text":"Input Stack: AggregationFunction \u21e8 Output Stack: AggregationFunction Force the consolidation function to be sum.","title":"cf-sum"},{"location":"asl/ref/count/","text":"Count aggregation operator. There are two variants of the :count operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the number of time series that match the query and have a value for a given interval. name,ssCpuUser, :eq , :count When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the count. This leads to a final result of: Name Data ssCpuUser [3.0, 3.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the number of time series from the input expression and have a value for a given interval. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :count","title":"count"},{"location":"asl/ref/count/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the number of time series that match the query and have a value for a given interval. name,ssCpuUser, :eq , :count When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the count. This leads to a final result of: Name Data ssCpuUser [3.0, 3.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/count/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the number of time series from the input expression and have a value for a given interval. Example: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :count","title":"Math"},{"location":"asl/ref/eq/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k == v): Query Select time series that have a specified value for a key. For example, consider the following query: name,ssCpuUser, :eq When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"eq"},{"location":"asl/ref/false/","text":"Input Stack: \u21e8 Output Stack: Query Query expression that will not match any input time series. See also :true .","title":"false"},{"location":"asl/ref/filter/","text":"Input Stack: TimeSeriesExpr TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Filters the results of a grouped expression by another expression. The filter expression is a set of signal time series indicating if the corresponding time series from the original expression should be shown. Simple example that suppresses all lines: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , 0, :filter Filtering is most commonly performed using the summary statistics for the original expression. For example, to show only the lines that have an average value across the query window greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :dup , avg, :stat , 5e3, :gt , :over , avg, :stat , 20e3, :lt , :and , :filter There are helpers, :stat-$(name) , to express this common pattern more easily for filters. They act as place holders for the specified statistic on the input time series. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. See the :stat operator for more details on available statistics. For this example, :stat-avg would be used: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-avg , 5e3, :gt , :stat-avg , 20e3, :lt , :and , :filter","title":"filter"},{"location":"asl/ref/ge/","text":"Greater than or equal operator. There are two variants of the :ge operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k >= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :ge When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 >= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a >= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a >= b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Info Note, the data points have floating point values. It is advisable to avoid relying on an exact equality match. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :ge","title":"ge"},{"location":"asl/ref/ge/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k >= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :ge When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/ge/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 >= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a >= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a >= b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Info Note, the data points have floating point values. It is advisable to avoid relying on an exact equality match. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :ge","title":"Math"},{"location":"asl/ref/gt/","text":"Greater than operator. There are two variants of the :gt operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k > v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than a specified value. For example, consider the following query: name,ssCpuSystem, :gt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 > ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a > b) where a and b are the corresponding intervals in the input time series. For example: Time a b a > b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :gt","title":"gt"},{"location":"asl/ref/gt/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k > v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is greater than a specified value. For example, consider the following query: name,ssCpuSystem, :gt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/gt/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 > ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a > b) where a and b are the corresponding intervals in the input time series. For example: Time a b a > b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 0.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 0.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :gt","title":"Math"},{"location":"asl/ref/has/","text":"Input Stack: k: String \u21e8 Output Stack: Query Select time series that have a specified key. For example, consider the following query: nf.node, :has When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp ssCpuUser api i-0456","title":"has"},{"location":"asl/ref/in/","text":"Input Stack: vs: List[String] k: String \u21e8 Output Stack: (k in vs): Query Select time series where the value for a key is in the specified set. For example, consider the following query: name,(,ssCpuUser,ssCpuSystem,), :in When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"in"},{"location":"asl/ref/le/","text":"Less than or equal operator. There are two variants of the :le operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k <= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :le When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 <= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a <= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a <= b 00:01 0.0 0.0 1.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :le","title":"le"},{"location":"asl/ref/le/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k <= v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than or equal to a specified value. For example, consider the following query: name,ssCpuSystem, :le When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/le/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 <= ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a <= b) where a and b are the corresponding intervals in the input time series. For example: Time a b a <= b 00:01 0.0 0.0 1.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :le","title":"Math"},{"location":"asl/ref/lt/","text":"Less than operator. There are two variants of the :lt operator. Choosing \u00b6 Input Stack: v: String k: String \u21e8 Output Stack: (k < v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than a specified value. For example, consider the following query: name,ssCpuSystem, :lt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 < ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a < b) where a and b are the corresponding intervals in the input time series. For example: Time a b a < b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :lt","title":"lt"},{"location":"asl/ref/lt/#choosing","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k < v): Query This first variant is used for choosing the set of time series to operate on. It selects time series that have a value for a key that is less than a specified value. For example, consider the following query: name,ssCpuSystem, :lt When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/lt/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 < ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a < b) where a and b are the corresponding intervals in the input time series. For example: Time a b a < b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 0.0 00:03 1.0 1.0 0.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for intervals where the condition is true and 0.0 for intervals where it is false. Example: Before After minuteOfHour, :time , hourOfDay, :time minuteOfHour, :time , hourOfDay, :time , :lt","title":"Math"},{"location":"asl/ref/max/","text":"Max aggregation operator. There are two variants of the :max operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the maximum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :max When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [8.0, 7.0, 6.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the maximum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :max","title":"max"},{"location":"asl/ref/max/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the maximum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :max When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [8.0, 7.0, 6.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/max/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the maximum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :max","title":"Math"},{"location":"asl/ref/min/","text":"Min aggregation operator. There are two variants of the :min operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the minimum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :min When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [1.0, 2.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the minimum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :min","title":"min"},{"location":"asl/ref/min/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Select the minimum value for corresponding times across all matching time series. name,ssCpuUser, :eq , :min When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the max. This leads to a final result of: Name Data ssCpuUser [1.0, 2.0, 2.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/min/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Select the minimum value for corresponding times across the time series resulting from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :sum , (,nf.cluster,), :by name,sps, :eq , :sum , (,nf.cluster,), :by , :min","title":"Math"},{"location":"asl/ref/not/","text":"Input Stack: q: Query \u21e8 Output Stack: (!q): Query Select time series that have a specified key. For example, consider the following query: nf.node, :has , :not When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp ssCpuUser api i-0456","title":"not"},{"location":"asl/ref/or/","text":"There are two variants of the :or operator. Choosing \u00b6 Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 OR q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if either of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :or When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456 Math \u00b6 Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 OR ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a OR b) where a and b are the corresponding intervals in the input time series. For example: Time a b a OR b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a or b are non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt , :or","title":"or"},{"location":"asl/ref/or/#choosing","text":"Input Stack: q2: Query q1: Query \u21e8 Output Stack: (q1 OR q2): Query This first variant is used for choosing the set of time series to operate on. It is a binary operator that matches if either of the sub-queries match. For example, consider the following query: nf.app,alerttest, :eq , name,ssCpuUser, :eq , :or When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpuUser alerttest i-0123 ssCpuSystem alerttest i-0123 ssCpuUser nccp i-0abc ssCpuSystem nccp i-0abc numRequests nccp i-0abc ssCpuUser api i-0456","title":"Choosing"},{"location":"asl/ref/or/#math","text":"Input Stack: ts2: TimeSeriesExpr ts1: TimeSeriesExpr \u21e8 Output Stack: (ts1 OR ts2): TimeSeriesExpr Compute a new time series where each interval has the value (a OR b) where a and b are the corresponding intervals in the input time series. For example: Time a b a OR b 00:01 0.0 0.0 0.0 00:01 0.0 1.0 1.0 00:02 1.0 0.0 1.0 00:03 1.0 1.0 1.0 00:04 0.5 1.7 1.0 The result will be a signal time series that will be 1.0 for all intervals where the corresponding values of a or b are non-zero. Example: Before After minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt minuteOfDay, :time , :dup , 300, :gt , :swap , 290, :lt , :or","title":"Math"},{"location":"asl/ref/re/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k=~/^v/): Query Warning Regular expressions can be expensive to check and should be avoided if possible. When designing data to publish ensure that common query patterns would not need the use of regular expressions. Select time series where the value for a key matches the specified regular expression. For example, consider the following query: name,ssCpu, :re When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpu User alerttest i-0123 ssCpu System alerttest i-0123 ssCpu User nccp i-0abc ssCpu System nccp i-0abc numRequests nccp i-0abc ssCpu User api i-0456 The regular expression value will be automatically anchored at the start and the matching is case sensitive. Always try to have a simple prefix on the expression to allow for more efficient matching of the expression. For more information on supported patterns, see the Java regular expressions documentation.","title":"re"},{"location":"asl/ref/reic/","text":"Input Stack: v: String k: String \u21e8 Output Stack: (k=~/^v/i): Query Warning Ignoring the case will always result if a full scan for the key. This should be used sparingly and only for tag queries. If a case-insensitive match is not required, use :re intead. Select time series where the value for a key matches the specified regular expression with case insenitive matching. For example, consider the following query: name,ssCPU, :reic When matching against the sample data in the table below, the highlighted time series would be included in the result set: Name nf.app nf.node ssCpu User alerttest i-0123 ssCpu System alerttest i-0123 ssCpu User nccp i-0abc ssCpu System nccp i-0abc numRequests nccp i-0abc ssCpu User api i-0456 Notice that the casing for the query does not match the data. The regular expression value will be automatically anchored at the start. For more information on supported patterns, see the Java regular expressions documentation.","title":"reic"},{"location":"asl/ref/stat-avg-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to avg,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-avg-mf","title":"stat-avg-mf"},{"location":"asl/ref/stat-avg/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the avg,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have an average value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-avg , 5e3, :gt , :stat-avg , 20e3, :lt , :and , :filter","title":"stat-avg"},{"location":"asl/ref/stat-count/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the count,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the count value is greater than 50: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-count , 50, :gt , :filter","title":"stat-count"},{"location":"asl/ref/stat-last/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the last,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the last value is greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-last , 5e3, :gt , :stat-last , 20e3, :lt , :and , :filter","title":"stat-last"},{"location":"asl/ref/stat-max-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to max,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-max-mf","title":"stat-max-mf"},{"location":"asl/ref/stat-max/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the max,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have a maximum value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-max , 5e3, :gt , :stat-max , 20e3, :lt , :and , :filter","title":"stat-max"},{"location":"asl/ref/stat-min-mf/","text":"Warning Deprecated: use :stat instead. Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Equivalent to min,:stat . Example of usage: Before After name,sps, :eq , :sum name,sps, :eq , :sum , :stat-min-mf","title":"stat-min-mf"},{"location":"asl/ref/stat-min/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the min,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines that have a minimum value greater than 5k and less than 20k: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-min , 5e3, :gt , :stat-min , 20e3, :lt , :and , :filter","title":"stat-min"},{"location":"asl/ref/stat-total/","text":"Input Stack: \u21e8 Output Stack: TimeSeriesExpr Represents the total,:stat of the input time series when used with the filter operation. The filter operator will automatically fill in the input when used so the user does not need to repeat the input expression for the filtering criteria. Example of restricting to lines where the sum of all data points for the line is greater than 1M and less than 4M: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , :stat-total , 1e6, :gt , :stat-total , 4e6, :lt , :and , :filter","title":"stat-total"},{"location":"asl/ref/stat/","text":"Input Stack: TimeSeriesExpr stat: String \u21e8 Output Stack: TimeSeriesExpr Create a summary time series showing the value of the specified summary statistic for the data points of the input time series. Valid statistic values are avg , count , max , min , last , and total . The graph below shows avg , max , min , and last for a simple input time series: The count is the number of data points for the time series. In the example above, that is five since the last value is NaN . The total is the sum of the data points for the time series. The most common usage of stats is in conjunction with :filter to restrict the set of results for grouped expression. When filtering, helper macros, :stat-$(name) , can be used to represent applying the statistic to the input time series being filtered without explicitly repeating the input expression. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , avg, :stat","title":"stat"},{"location":"asl/ref/sum/","text":"Sum aggregation operator. There are two variants of the :sum operator. Aggregation \u00b6 Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the sum of all the time series that match the query. Sum is the default aggregate used if a query is specified with no explicit aggregate function. Example with implicit sum: name,ssCpuUser, :eq Equivalent example with explicit sum: name,ssCpuUser, :eq , :sum When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the sum. This leads to a final result of: Name Data ssCpuUser [10.0, 11.0, 8.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by . Math \u00b6 Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the sum of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :max , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :sum","title":"sum"},{"location":"asl/ref/sum/#aggregation","text":"Input Stack: Query \u21e8 Output Stack: AggregationFunction Compute the sum of all the time series that match the query. Sum is the default aggregate used if a query is specified with no explicit aggregate function. Example with implicit sum: name,ssCpuUser, :eq Equivalent example with explicit sum: name,ssCpuUser, :eq , :sum When matching against the sample data in the table below, the highlighted time series would be included in the aggregate result: Name nf.app nf.node Data ssCpuUser alerttest i-0123 [1.0, 2.0, NaN] ssCpuSystem alerttest i-0123 [3.0, 4.0, 5.0] ssCpuUser nccp i-0abc [8.0, 7.0, 6.0] ssCpuSystem nccp i-0abc [6.0, 7.0, 8.0] numRequests nccp i-0abc [1.0, 2.0, 4.0] ssCpuUser api i-0456 [1.0, 2.0, 2.0] The values from the corresponding intervals will be aggregated. For the first interval using the sample data above the values are 1.0 , 8.0 , and 1.0 . Each value other than NaN contributes one to the sum. This leads to a final result of: Name Data ssCpuUser [10.0, 11.0, 8.0] The only tags for the aggregated result are those that are matched exactly ( :eq clause) as part of the choosing criteria or are included in a group by .","title":"Aggregation"},{"location":"asl/ref/sum/#math","text":"Input Stack: TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Compute the sum of all the time series from the input expression. This is typically used when there is a need to use some other aggregation for the grouping. Example: Before After name,sps, :eq , :max , (,nf.cluster,), :by name,sps, :eq , :max , (,nf.cluster,), :by , :sum","title":"Math"},{"location":"asl/ref/topk-others-avg/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes an average aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-avg","title":"topk-others-avg"},{"location":"asl/ref/topk-others-max/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a max aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-max","title":"topk-others-max"},{"location":"asl/ref/topk-others-min/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a min aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-min","title":"topk-others-min"},{"location":"asl/ref/topk-others-sum/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic and computes a sum aggregate for the other time series. Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk-others-sum","title":"topk-others-sum"},{"location":"asl/ref/topk/","text":"Input Stack: k: Int stat: String TimeSeriesExpr \u21e8 Output Stack: TimeSeriesExpr Since: 1.7 Restrict the output for a grouped expression to the k time series with the largest value for the specified summary statistic . Example of usage: Before After name,sps, :eq , (,nf.cluster,), :by name,sps, :eq , (,nf.cluster,), :by , max,2, :topk In some cases it can be useful to see an aggregate summary of the other time series that were not part of the top set. This can be accomplished using the :topk-others-$(aggr) operators. Permitted aggregations are avg , max , min , and sum . For more details see: :topk-others-avg :topk-others-max :topk-others-min :topk-others-sum","title":"topk"},{"location":"asl/ref/true/","text":"Input Stack: \u21e8 Output Stack: Query Query expression that will match any input time series. See also :false .","title":"true"},{"location":"concepts/consolidation/","text":"TODO","title":"Consolidation"},{"location":"concepts/naming/","text":"Summary \u00b6 Names Describe the measurement being collected Use camelCase Static Succinct Tags Should be used for dimensional filtering Be careful about combinatorial explosion Tag keys should be static Use id to distinguish between instances Use Base Units Names \u00b6 Describe the Measurement \u00b6 Use camelCase \u00b6 The main goal here is to promote consistency, which makes it easier for users. The choice of style is somewhat arbitrary, but camelCase was chosen because: Used by SNMP Used by Java It was commonly used at Netflix when the guideline was written The exception to this rule is where there is an established common case. For example, with Amazon regions, it is preferred to use us-east-1 rather than usEast1 as it is the more common form. Static \u00b6 There should not be any dynamic content in a metric name, such as requests.$APP_NAME . Metric names and tag keys are how users interact with the data, and dynamic values make them difficult to use. Dynamic information is better suited for tag values, such as nf.app or status . Succinct \u00b6 Long names should be avoided. In many cases, long names are the result of combining many pieces of information together into a single string. In this case, consider either discarding information that is not useful or encoding the information in tag values. Tags \u00b6 Historically, tags have been used to play one of two roles: Dimensions. This is the primary use of tags and this feature allows the data to be filtered into subsets by values of interest. Namespace. Similar to packages in Java, this allows grouping related data. This type of usage is discouraged. As a general rule, it should be possible to use the name as a pivot. If only the name is selected, then the user should be able to use other dimensions to filter the data and successfully reason about the value being shown. As a concrete example, suppose we have two metrics: The number of threads currently in a thread pool. The number of rows in a database table. Discouraged Approach \u00b6 Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"ThreadPool\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"Database\" ) . withTag ( \"table\" , \"users\" ); In this approach, if you select the name size , then it will match both the ThreadPool and Database classes. This results in a value that is the an aggregate of the number of threads and the number of items in a database, which has no meaning. Recommended Approach \u00b6 Id poolSize = registry . createId ( \"threadpool.size\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"db.size\" ) . withTag ( \"table\" , \"users\" ); This variation provides enough context, so that if just the name is selected, the value can be reasoned about and is at least potentially meaningful. This variation provides enough context in the name so that the meaning is more apparent and you can successfully reason about the values. For example, if you select threadpool.size , then you can see the total number of threads in all pools. You can then group by or select an id to further filter the data to a subset in which you have an interest. Use Base Units \u00b6 Keep measurements in base units where possible. It is better to have all timers in seconds, disk sizes in bytes, and network rates in bytes/second. This allows any SI unit prefixes applied to tick labels on a graph to have an obvious meaning, such as 1k meaning 1 kilobyte, as opposed to 1 kilo-megabyte.","title":"Naming"},{"location":"concepts/naming/#summary","text":"Names Describe the measurement being collected Use camelCase Static Succinct Tags Should be used for dimensional filtering Be careful about combinatorial explosion Tag keys should be static Use id to distinguish between instances Use Base Units","title":"Summary"},{"location":"concepts/naming/#names","text":"","title":"Names"},{"location":"concepts/naming/#describe-the-measurement","text":"","title":"Describe the Measurement"},{"location":"concepts/naming/#use-camelcase","text":"The main goal here is to promote consistency, which makes it easier for users. The choice of style is somewhat arbitrary, but camelCase was chosen because: Used by SNMP Used by Java It was commonly used at Netflix when the guideline was written The exception to this rule is where there is an established common case. For example, with Amazon regions, it is preferred to use us-east-1 rather than usEast1 as it is the more common form.","title":"Use camelCase"},{"location":"concepts/naming/#static","text":"There should not be any dynamic content in a metric name, such as requests.$APP_NAME . Metric names and tag keys are how users interact with the data, and dynamic values make them difficult to use. Dynamic information is better suited for tag values, such as nf.app or status .","title":"Static"},{"location":"concepts/naming/#succinct","text":"Long names should be avoided. In many cases, long names are the result of combining many pieces of information together into a single string. In this case, consider either discarding information that is not useful or encoding the information in tag values.","title":"Succinct"},{"location":"concepts/naming/#tags","text":"Historically, tags have been used to play one of two roles: Dimensions. This is the primary use of tags and this feature allows the data to be filtered into subsets by values of interest. Namespace. Similar to packages in Java, this allows grouping related data. This type of usage is discouraged. As a general rule, it should be possible to use the name as a pivot. If only the name is selected, then the user should be able to use other dimensions to filter the data and successfully reason about the value being shown. As a concrete example, suppose we have two metrics: The number of threads currently in a thread pool. The number of rows in a database table.","title":"Tags"},{"location":"concepts/naming/#discouraged-approach","text":"Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"ThreadPool\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"size\" ) . withTag ( \"class\" , \"Database\" ) . withTag ( \"table\" , \"users\" ); In this approach, if you select the name size , then it will match both the ThreadPool and Database classes. This results in a value that is the an aggregate of the number of threads and the number of items in a database, which has no meaning.","title":"Discouraged Approach"},{"location":"concepts/naming/#recommended-approach","text":"Id poolSize = registry . createId ( \"threadpool.size\" ) . withTag ( \"id\" , \"server-requests\" ); Id poolSize = registry . createId ( \"db.size\" ) . withTag ( \"table\" , \"users\" ); This variation provides enough context, so that if just the name is selected, the value can be reasoned about and is at least potentially meaningful. This variation provides enough context in the name so that the meaning is more apparent and you can successfully reason about the values. For example, if you select threadpool.size , then you can see the total number of threads in all pools. You can then group by or select an id to further filter the data to a subset in which you have an interest.","title":"Recommended Approach"},{"location":"concepts/naming/#use-base-units","text":"Keep measurements in base units where possible. It is better to have all timers in seconds, disk sizes in bytes, and network rates in bytes/second. This allows any SI unit prefixes applied to tick labels on a graph to have an obvious meaning, such as 1k meaning 1 kilobyte, as opposed to 1 kilo-megabyte.","title":"Use Base Units"},{"location":"concepts/normalization/","text":"Normalization \u00b6 In Atlas, this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute, instead of exactly on the minute. The values will get normalized to the minute boundary, so that all time series in the system are consistent. How a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like Spectator will automatically handle tagging based on the data source type. It is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data. Gauge \u00b6 A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 8 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 6 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Rate \u00b6 A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 7 \u2502 \u2502 \u2502 6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 3 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Here, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute. Time Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7 If many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, the value will be treated as the average rate per second over the previous step. This behavior is important to avoid under-counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to 1 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Why perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms: Avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually getting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with Gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part. Counter \u00b6 Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate . Note, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval.","title":"Normalization"},{"location":"concepts/normalization/#normalization","text":"In Atlas, this usually refers to normalizing data points to step boundaries. Suppose that values are actually getting reported at 30 seconds after the minute, instead of exactly on the minute. The values will get normalized to the minute boundary, so that all time series in the system are consistent. How a normalized value is computed depends on the data source type. Atlas supports three types indicated by the value of the atlas.dstype tag. In general, you should not need to worry about that, client libraries like Spectator will automatically handle tagging based on the data source type. It is recommended to at least skim through the normalization for gauges and rates to better understand how the values you see actually relate to measured data.","title":"Normalization"},{"location":"concepts/normalization/#gauge","text":"A value that is sampled from some source and the value is used as is. The last value received will be the value used for the interval. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 8 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 \u2502 \u2502 6 \u2502 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03","title":"Gauge"},{"location":"concepts/normalization/#rate","text":"A rate is a value representing the rate per second since the last reported value. Rate values are normalized using a weighted average. For example: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 7 \u2502 \u2502 \u2502 6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u2502 \u2502 4 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to \u2502 3 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Here, the data is reported at exactly 30s after the minute boundary. So each value represents the average rate per second for 50% of the minute. Time Value 1:01 4 * 0.5 + 2 * 0.5 = 2 + 1 = 3 1:02 2 * 0.5 + 8 * 0.5 = 1 + 4 = 5 1:03 8 * 0.5 + 6 * 0.5 = 4 + 3 = 7 If many samples are received for a given interval, then they will each be weighted based on the fraction of the interval they represent. When no previous sample exists, the value will be treated as the average rate per second over the previous step. This behavior is important to avoid under-counting the contribution from a previous interval. The example below shows what happens if there is no previous or next sample: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 5 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u2502 4 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 to 1 \u2502 \u2502 \u2502 \u2502 2 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 1:00 1:01 1:02 1:03 1:00 1:01 1:02 1:03 Why perform weighted averaging for rates instead of the simpler last value approach used with gauges? Because it gives us a better summary of what we actually know from the measurements received. In practical terms: Avoids dropping information if samples are more frequent than the step. Suppose we have a 1 minute step, but data is actually getting reported every 10s. For this example, assume we get 1, 5, 90, 5, 4, and 2. The last value normalization used with Gauges would end up with a value of 2. The rate normalization will give 17.833. Each value is a rate per second, so if you take the (1 + 5 + 90 + 5 + 4 + 2) * 10 = 1070 actual events measured during the interval. That is equivalent to 17.833 * 60 indicating we have an accurate average rate for the step size. Avoids skewing the data causing misleading spikes or drops in the aggregates. Using Atlas you will typically be looking at an aggregate of time series rather than an individual time series that was reported. With last value it can have the effect of skewing samples to a later interval. Suppose the client is reporting once a minute at 5s after the minute. That value indicates more about the previous interval than it does the current one. During traffic transitions, such as moving traffic over to a new cluster or even some auto-scaling events, differences in this skew can result in the appearance of a drop because there will be many new time series getting reported with a delayed start. For existing time series it is still skewed, but tends to be less noticeable. The weighted averaging avoids these problems for the most part.","title":"Rate"},{"location":"concepts/normalization/#counter","text":"Counter is similar to rate, except that the value reported is monotonically increasing and will be converted to a rate by the backend. The conversion is done by computing the delta between the current sample and the previous sample and dividing by the time between the samples. After that it is the same as a rate . Note, that unless the input is a montonically increasing counter it is generally better to have the client perform rate conversion. Since, the starting value is unknown, at least two samples must be received before the first delta can be computed. This means that new time series relying on counter type will be delayed by one interval.","title":"Counter"},{"location":"concepts/time-series/","text":"Time Series \u00b6 A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size . In Atlas, each time series is paired with metadata called tags that allow us to query and group the data. Tags \u00b6 A set of key value pairs associated with a time series . Each time series must have at least one tag with a key of name . To make it more concrete, here is an example of a tag set represented as a JSON object: { \"name\" : \"server.requestCount\" , \"status\" : \"200\" , \"endpoint\" : \"api\" , \"nf.app\" : \"fooserver\" , \"nf.cluster\" : \"fooserver-main\" , \"nf.stack\" : \"main\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1c\" , \"nf.node\" : \"i-12345678\" } Usage of tags typically falls into two categories: Namespace. These are tags necessary to qualify a name, so that it can be meaningfully aggregated. Using the sample above, consider computing the sum of all metrics for application fooserver . That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name . The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions. These are tags used to filter the data to a meaningful subset. They can be used to see the number of successful requests across the cluster by querying for status = 200 or the number of requests for a single node by querying for nf.node = i-12345678 . Most tags should fall into this category. When creating metrics, it is important to carefully think about how the data should be tagged. See the naming docs for more information. Metric \u00b6 A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series . A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric. Data Point \u00b6 A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter . Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size . The client library will sample the counter once for each interval and report a single value. Suppose that each circle in the diagram below represents a request: 1:00 1:01 1:02 1:03 \u251c\u2500\u25cf\u2500\u2500\u2500\u2500\u25cf\u25cf\u25cf\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 There are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be: Time Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167 Step Size \u00b6 The amount of time between two successive data points in a time series . For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary.","title":"Time Series"},{"location":"concepts/time-series/#time-series","text":"A time series is a sequence of data points reported at a consistent interval over time. The time interval between successive data points is called the step size . In Atlas, each time series is paired with metadata called tags that allow us to query and group the data.","title":"Time Series"},{"location":"concepts/time-series/#tags","text":"A set of key value pairs associated with a time series . Each time series must have at least one tag with a key of name . To make it more concrete, here is an example of a tag set represented as a JSON object: { \"name\" : \"server.requestCount\" , \"status\" : \"200\" , \"endpoint\" : \"api\" , \"nf.app\" : \"fooserver\" , \"nf.cluster\" : \"fooserver-main\" , \"nf.stack\" : \"main\" , \"nf.region\" : \"us-east-1\" , \"nf.zone\" : \"us-east-1c\" , \"nf.node\" : \"i-12345678\" } Usage of tags typically falls into two categories: Namespace. These are tags necessary to qualify a name, so that it can be meaningfully aggregated. Using the sample above, consider computing the sum of all metrics for application fooserver . That number would be meaningless. Properly modelled data should try to make the aggregates meaningful by selecting the name . The sum of all metrics with name = server.requestCount is the overall request count for the service. Dimensions. These are tags used to filter the data to a meaningful subset. They can be used to see the number of successful requests across the cluster by querying for status = 200 or the number of requests for a single node by querying for nf.node = i-12345678 . Most tags should fall into this category. When creating metrics, it is important to carefully think about how the data should be tagged. See the naming docs for more information.","title":"Tags"},{"location":"concepts/time-series/#metric","text":"A metric is a specific quantity being measured, e.g., the number of requests received by a server. In casual language about Atlas metric is often used interchangeably with time series . A time series is one way to track a metric and is the method supported by Atlas. In most cases there will be many time series for a given metric. Going back to the example, request count would usually be tagged with additional dimensions such as status and node. There is one time series for each distinct combination of tags, but conceptually it is the same metric.","title":"Metric"},{"location":"concepts/time-series/#data-point","text":"A data point is a triple consisting of tags, timestamp, and a value. It is important to understand at a high level how data points correlate with the measurement. Consider requests hitting a server, this would typically be measured using a counter . Each time a request is received the counter is incremented. There is not one data point per increment, a data point represents the behavior over a span of time called the step size . The client library will sample the counter once for each interval and report a single value. Suppose that each circle in the diagram below represents a request: 1:00 1:01 1:02 1:03 \u251c\u2500\u25cf\u2500\u2500\u2500\u2500\u25cf\u25cf\u25cf\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 There are 5 requests shown, 4 from 1:00 to 1:01, and 1 from 1:02 to 1:03. Assuming all requests incremented the same time series, i.e. all other dimensions such as status code are the same, then this would result in three data points. For counters values are always a rate per second, so for a one minute step size the total number of requests would be divided by 60 seconds. So the values stored would be: Time Value 1:01 4 / 60 = 0.0667 1:02 0 / 60 = 0.0000 1:03 1 / 60 = 0.0167","title":"Data Point"},{"location":"concepts/time-series/#step-size","text":"The amount of time between two successive data points in a time series . For Atlas the datapoints will always be on even boundaries of the step size. If data is not reported on step boundaries, it will get normalized to the boundary.","title":"Step Size"},{"location":"spectator/","text":"Simple library for instrumenting code to record dimensional time series data. At a minimum, you need to: Understand core concepts. Time Series Normalization Naming Clock Install the language-specific library and configuration bindings, where available. Support Class Descriptions Language Overview First-Class Support Java Node.js Best-Effort Support C++ Go Python Ruby Instrument some code, referring to the core usage guides on the following meter types: Counters Distribution Summaries Gauges Percentile Timers Timers After you are more familiar with the library and need assistance with more advanced topics, see the Patterns section on the left.","title":"Overview"},{"location":"spectator/core/clock/","text":"Clock \u00b6 When taking measurements or working with timers it is recommended to use the Clock interface. It provides two methods for measuring time: Wall Time \u00b6 This is what most users think of for time. It can be used to get the current time like what you would see on a wall clock. In most cases when not running in tests this will call System.currentTimeMillis() . Note that the values returned by this method may not be monotonically increasing. Just like a clock on your wall, this value can go back in time or jump forward at unpredictable intervals, if someone sets the time. On many systems, ntpd or similar daemons will be constantly keeping the time synced up with an authoritative source. With Spectator, the Clock is typically accessed via the Registry . Java usage example: // Current time in milliseconds since the epoch long currentTime = registry . clock (). wallTime (); Monotonic Time \u00b6 While it is good in general for the wall clock to show the correct time, the unpredictable changes mean it is not a good choice for measuring how long an operation took. Consider a simple example of measuring request latency on a server: long start = registry . clock (). wallTime (); handleRequest ( request , response ); long end = registry . clock (). wallTime (); reqLatencyTimer . record ( end - start , TimeUnit . MILLISECONDS ); If ntp fixes the server time between start and end , then the recorded latency will be wrong. Spectator will protect against obviously wrong measurements like negative latencies by dropping those values when they are recorded. However, the change could incorrectly shorten or lengthen the measured latency. The clock interface also provides access to a monotonic source that is only useful for measuring elapsed time, for example: long start = registry . clock (). monotonicTime (); handleRequest ( request , response ); long end = registry . clock (). monotonicTime (); reqLatencyTimer . record ( end - start , TimeUnit . NANOSECONDS ); In most cases this will map to System.nanoTime() . Note the actual value returned is not meaningful unless compared with another sample to get a delta. Manual Clock \u00b6 If timing code is written to the Clock interface, then alternative implementations can be plugged-in. For test cases, it is common to use ManualClock so that tests can be reliable and fast without having to rely on hacks like sleep or assuming something will run in less than a certain amount of time. ManualClock clock = new ManualClock (); Registry registry = new DefaultRegistry ( clock ); Timer timer = registry . timer ( \"test\" ); timer . record (() -> { doSomething (); clock . setMonotonicTime ( 42L ); }); Assert . assertEquals ( timer . totalTime (), 42L );","title":"Clock"},{"location":"spectator/core/clock/#clock","text":"When taking measurements or working with timers it is recommended to use the Clock interface. It provides two methods for measuring time:","title":"Clock"},{"location":"spectator/core/clock/#wall-time","text":"This is what most users think of for time. It can be used to get the current time like what you would see on a wall clock. In most cases when not running in tests this will call System.currentTimeMillis() . Note that the values returned by this method may not be monotonically increasing. Just like a clock on your wall, this value can go back in time or jump forward at unpredictable intervals, if someone sets the time. On many systems, ntpd or similar daemons will be constantly keeping the time synced up with an authoritative source. With Spectator, the Clock is typically accessed via the Registry . Java usage example: // Current time in milliseconds since the epoch long currentTime = registry . clock (). wallTime ();","title":"Wall Time"},{"location":"spectator/core/clock/#monotonic-time","text":"While it is good in general for the wall clock to show the correct time, the unpredictable changes mean it is not a good choice for measuring how long an operation took. Consider a simple example of measuring request latency on a server: long start = registry . clock (). wallTime (); handleRequest ( request , response ); long end = registry . clock (). wallTime (); reqLatencyTimer . record ( end - start , TimeUnit . MILLISECONDS ); If ntp fixes the server time between start and end , then the recorded latency will be wrong. Spectator will protect against obviously wrong measurements like negative latencies by dropping those values when they are recorded. However, the change could incorrectly shorten or lengthen the measured latency. The clock interface also provides access to a monotonic source that is only useful for measuring elapsed time, for example: long start = registry . clock (). monotonicTime (); handleRequest ( request , response ); long end = registry . clock (). monotonicTime (); reqLatencyTimer . record ( end - start , TimeUnit . NANOSECONDS ); In most cases this will map to System.nanoTime() . Note the actual value returned is not meaningful unless compared with another sample to get a delta.","title":"Monotonic Time"},{"location":"spectator/core/clock/#manual-clock","text":"If timing code is written to the Clock interface, then alternative implementations can be plugged-in. For test cases, it is common to use ManualClock so that tests can be reliable and fast without having to rely on hacks like sleep or assuming something will run in less than a certain amount of time. ManualClock clock = new ManualClock (); Registry registry = new DefaultRegistry ( clock ); Timer timer = registry . timer ( \"test\" ); timer . record (() -> { doSomething (); clock . setMonotonicTime ( 42L ); }); Assert . assertEquals ( timer . totalTime (), 42L );","title":"Manual Clock"},{"location":"spectator/core/meters/counter/","text":"Counter \u00b6 A Counter is used to measure the rate at which some event is occurring. Considering a simple queue, Counters could be used to measure things like the rate at which items are being inserted and removed. Counters are reported to the backend as a rate-per-second. In Atlas, the :per-step operator can be used to convert them back into a value-per-step on a graph. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Counters"},{"location":"spectator/core/meters/counter/#counter","text":"A Counter is used to measure the rate at which some event is occurring. Considering a simple queue, Counters could be used to measure things like the rate at which items are being inserted and removed. Counters are reported to the backend as a rate-per-second. In Atlas, the :per-step operator can be used to convert them back into a value-per-step on a graph.","title":"Counter"},{"location":"spectator/core/meters/counter/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/counter/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/counter/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/dist-summary/","text":"Distribution Summary \u00b6 A Distribution Summary is used to track the distribution of events. It is similar to a Timer , but more general, in that the size does not have to be a period of time. For example, a distribution summary could be used to measure the payload sizes of requests hitting a server. It is recommended to always use base units when recording the data. So, if measuring the payload size use bytes, not kilobytes or some other unit. This allows the presentation layer for graphing to use either SI or IEC prefixes in a natural manner, and you do not need to consider the meaning of something like \"milli-milliseconds\". Average Measurement (:dist-avg) \u00b6 For Timer and Distribution Summary metrics, the totalTime / totalAmount and count are collected each time a measurement is taken. If this technique was applied to a request latency metric, then you would have the average latency per request for an arbitrary grouping. These types of metrics have an explicit count based on activity. To get an average per measurement manually: statistic,totalTime,:eq,:sum, statistic,count,:eq,:sum, :div This expression can be bound to a query using the :cq (common query) operator: statistic,totalTime,:eq,:sum, statistic,count,:eq,:sum, :div nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:cq To make this process easier, Atlas provides a :dist-avg function that is used in the same manner as a built-in aggregate function: nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:dist-avg nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:dist-avg,(,nf.asg,),:by Maximum Measurement (:dist-max) \u00b6 TBD Standard Deviation of Measurement (:dist-stddev) \u00b6 TBD Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Distribution Summaries"},{"location":"spectator/core/meters/dist-summary/#distribution-summary","text":"A Distribution Summary is used to track the distribution of events. It is similar to a Timer , but more general, in that the size does not have to be a period of time. For example, a distribution summary could be used to measure the payload sizes of requests hitting a server. It is recommended to always use base units when recording the data. So, if measuring the payload size use bytes, not kilobytes or some other unit. This allows the presentation layer for graphing to use either SI or IEC prefixes in a natural manner, and you do not need to consider the meaning of something like \"milli-milliseconds\".","title":"Distribution Summary"},{"location":"spectator/core/meters/dist-summary/#average-measurement-dist-avg","text":"For Timer and Distribution Summary metrics, the totalTime / totalAmount and count are collected each time a measurement is taken. If this technique was applied to a request latency metric, then you would have the average latency per request for an arbitrary grouping. These types of metrics have an explicit count based on activity. To get an average per measurement manually: statistic,totalTime,:eq,:sum, statistic,count,:eq,:sum, :div This expression can be bound to a query using the :cq (common query) operator: statistic,totalTime,:eq,:sum, statistic,count,:eq,:sum, :div nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:cq To make this process easier, Atlas provides a :dist-avg function that is used in the same manner as a built-in aggregate function: nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:dist-avg nf.cluster,foo,:eq,name,http.req.latency,:eq,:and,:dist-avg,(,nf.asg,),:by","title":"Average Measurement (:dist-avg)"},{"location":"spectator/core/meters/dist-summary/#maximum-measurement-dist-max","text":"TBD","title":"Maximum Measurement (:dist-max)"},{"location":"spectator/core/meters/dist-summary/#standard-deviation-of-measurement-dist-stddev","text":"TBD","title":"Standard Deviation of Measurement (:dist-stddev)"},{"location":"spectator/core/meters/dist-summary/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/dist-summary/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/dist-summary/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/gauge/","text":"Gauge \u00b6 A Gauge is a value that is sampled at some point in time. Typical examples for Gauges would be the size of a queue, or the number of threads in a running state. Since Gauges are not updated inline when a state change occurs, there is no information about what might have occurred between samples. Consider monitoring the behavior of a queue of tasks. If the data is being collected once a minute, then a Gauge for the size will show the size when it was sampled (a.k.a. last-write-wins). The size may have been much higher or lower at some point during interval, but that is not known. Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Gauges"},{"location":"spectator/core/meters/gauge/#gauge","text":"A Gauge is a value that is sampled at some point in time. Typical examples for Gauges would be the size of a queue, or the number of threads in a running state. Since Gauges are not updated inline when a state change occurs, there is no information about what might have occurred between samples. Consider monitoring the behavior of a queue of tasks. If the data is being collected once a minute, then a Gauge for the size will show the size when it was sampled (a.k.a. last-write-wins). The size may have been much higher or lower at some point during interval, but that is not known.","title":"Gauge"},{"location":"spectator/core/meters/gauge/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/gauge/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/gauge/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/core/meters/timer/","text":"Timers \u00b6 A Timer is used to measure how long (in seconds) some event is taking. Two types of Timers are supported: Timer : for frequent short-duration events. LongTaskTimer : for long-running tasks. The long-duration Timer is setup so that you can track the time while an event being measured is still running. A regular Timer just records the duration and has no information until the task is complete. As an example, consider a chart showing request latency to a typical web server. The expectation is many short requests, so the timer will be getting updated many times per second. Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally, all data can be refreshed in a few minutes. If the AWS services are having problems, it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata. The charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale: Long Task Timer: Languages \u00b6 First-Class Support \u00b6 Java Node.js Experimental Support \u00b6 C++ Go Python Ruby","title":"Timers"},{"location":"spectator/core/meters/timer/#timers","text":"A Timer is used to measure how long (in seconds) some event is taking. Two types of Timers are supported: Timer : for frequent short-duration events. LongTaskTimer : for long-running tasks. The long-duration Timer is setup so that you can track the time while an event being measured is still running. A regular Timer just records the duration and has no information until the task is complete. As an example, consider a chart showing request latency to a typical web server. The expectation is many short requests, so the timer will be getting updated many times per second. Now consider a background process to refresh metadata from a data store. For example, Edda caches AWS resources such as instances, volumes, auto-scaling groups etc. Normally, all data can be refreshed in a few minutes. If the AWS services are having problems, it can take much longer. A long duration timer can be used to track the overall time for refreshing the metadata. The charts below show max latency for the refresh using a regular timer and a long task timer. Regular timer, note that the y-axis is using a logarithmic scale: Long Task Timer:","title":"Timers"},{"location":"spectator/core/meters/timer/#languages","text":"","title":"Languages"},{"location":"spectator/core/meters/timer/#first-class-support","text":"Java Node.js","title":"First-Class Support"},{"location":"spectator/core/meters/timer/#experimental-support","text":"C++ Go Python Ruby","title":"Experimental Support"},{"location":"spectator/lang/overview/","text":"The original Spectator library was written in Java , with the first stable version ( 0.35.0 ) released on Jan 18, 2016. Since then, there has been a proliferation of languages at Netflix which seek first-class observability support. After some thought and experimentation, we have settled on a strategy of developing minimal Spectator implementations in many languages, which function as thin clients that send data to Atlas. Our goal is to have partners invested in each experimental language who will provide the necessary expertise to develop idiomatic solutions, deliver real-world feedback on library usage, and shoulder some of the support and maintenance burden. We think this is a more sustainable path over the long-term than expanding our team to support N different languages for this singular polyglot use case. First-Class Support \u00b6 These libraries are fully-supported by the team and see wide use across Netflix. Issues are fixed in a timely manner and updates are published regularly. Java (GA) Node.js (GA) Best-Effort Support \u00b6 Warning Support for these languages is experimental and subject to change. Do not use these libraries unless you understand and accept that risk. C++ (Alpha) Go (Beta) Python (Beta) Ruby (Alpha)","title":"Overview"},{"location":"spectator/lang/overview/#first-class-support","text":"These libraries are fully-supported by the team and see wide use across Netflix. Issues are fixed in a timely manner and updates are published regularly. Java (GA) Node.js (GA)","title":"First-Class Support"},{"location":"spectator/lang/overview/#best-effort-support","text":"Warning Support for these languages is experimental and subject to change. Do not use these libraries unless you understand and accept that risk. C++ (Alpha) Go (Beta) Python (Beta) Ruby (Alpha)","title":"Best-Effort Support"},{"location":"spectator/lang/cpp/usage/","text":"Project \u00b6 Source Product Lifecycle: Alpha This implements a basic Spectator library for instrumenting C++ applications and sending metrics to an Atlas aggregator service. Install Library \u00b6 TBD If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 #include <spectator/registry.h> // use default values static constexpr auto kDefault = 0 ; struct Request { std :: string country ; }; struct Response { int status ; int size ; }; class Server { public : explicit Server ( spectator :: Registry * registry ) : registry_ { registry }, request_count_id_ { registry -> CreateId ( \"server.requestCount\" , spectator :: Tags {})}, request_latency_ { registry -> GetTimer ( \"server.requestLatency\" )}, response_size_ { registry -> GetDistributionSummary ( \"server.responseSizes\" )} {} Response Handle ( const Request & request ) { using spectator :: Registry ; auto start = Registry :: clock :: now (); // do some work and obtain a response... Response res { 200 , 64 }; // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. auto cnt_id = request_count_id_ -> WithTag ( \"country\" , request . country ) -> WithTag ( \"status\" , std :: to_string ( res . status )); registry_ -> GetCounter ( std :: move ( cnt_id )) -> Increment (); request_latency_ -> Record ( Registry :: clock :: now () - start ); response_size_ -> Record ( res . size ); return res ; } private : spectator :: Registry * registry_ ; std :: shared_ptr < spectator :: Id > request_count_id_ ; std :: shared_ptr < spectator :: Timer > request_latency_ ; std :: shared_ptr < spectator :: DistributionSummary > response_size_ ; }; Request get_next_request () { //... return Request { \"US\" }; } int main () { spectator :: Registry registry { spectator :: GetConfiguration ()}; registry . Start (); Server server { & registry }; for ( auto i = 1 ; i <= 3 ; ++ i ) { // get a request auto req = get_next_request (); server . Handle ( req ); } registry . Stop (); } Netflix Integration \u00b6 Copy the netflix_config.cc file from the netflix-spectator-cppconf repository in Stash to a directory where your sources reside.","title":"Usage"},{"location":"spectator/lang/cpp/usage/#project","text":"Source Product Lifecycle: Alpha This implements a basic Spectator library for instrumenting C++ applications and sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/cpp/usage/#install-library","text":"TBD If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/cpp/usage/#instrumenting-code","text":"#include <spectator/registry.h> // use default values static constexpr auto kDefault = 0 ; struct Request { std :: string country ; }; struct Response { int status ; int size ; }; class Server { public : explicit Server ( spectator :: Registry * registry ) : registry_ { registry }, request_count_id_ { registry -> CreateId ( \"server.requestCount\" , spectator :: Tags {})}, request_latency_ { registry -> GetTimer ( \"server.requestLatency\" )}, response_size_ { registry -> GetDistributionSummary ( \"server.responseSizes\" )} {} Response Handle ( const Request & request ) { using spectator :: Registry ; auto start = Registry :: clock :: now (); // do some work and obtain a response... Response res { 200 , 64 }; // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. auto cnt_id = request_count_id_ -> WithTag ( \"country\" , request . country ) -> WithTag ( \"status\" , std :: to_string ( res . status )); registry_ -> GetCounter ( std :: move ( cnt_id )) -> Increment (); request_latency_ -> Record ( Registry :: clock :: now () - start ); response_size_ -> Record ( res . size ); return res ; } private : spectator :: Registry * registry_ ; std :: shared_ptr < spectator :: Id > request_count_id_ ; std :: shared_ptr < spectator :: Timer > request_latency_ ; std :: shared_ptr < spectator :: DistributionSummary > response_size_ ; }; Request get_next_request () { //... return Request { \"US\" }; } int main () { spectator :: Registry registry { spectator :: GetConfiguration ()}; registry . Start (); Server server { & registry }; for ( auto i = 1 ; i <= 3 ; ++ i ) { // get a request auto req = get_next_request (); server . Handle ( req ); } registry . Stop (); }","title":"Instrumenting Code"},{"location":"spectator/lang/cpp/usage/#netflix-integration","text":"Copy the netflix_config.cc file from the netflix-spectator-cppconf repository in Stash to a directory where your sources reside.","title":"Netflix Integration"},{"location":"spectator/lang/cpp/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/cpp/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/cpp/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/cpp/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/cpp/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/go/usage/","text":"Project \u00b6 Source Product Lifecycle: Beta This implements a basic Spectator library for instrumenting golang applications and sending metrics to an Atlas aggregator service. Install Library \u00b6 Add a github.com/Netflix/spectator-go remote import to your code. If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 package main import ( \"github.com/Netflix/spectator-go\" \"strconv\" \"time\" ) type Server struct { registry * spectator . Registry requestCountId * spectator . Id requestLatency * spectator . Timer responseSizes * spectator . DistributionSummary } type Request struct { country string } type Response struct { status int size int64 } func ( s * Server ) Handle ( request * Request ) ( res * Response ) { clock := s . registry . Clock () start := clock . Now () // initialize res res = & Response { 200 , 64 } // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. cntId := s . requestCountId . WithTag ( \"country\" , request . country ). WithTag ( \"status\" , strconv . Itoa ( res . status )) s . registry . CounterWithId ( cntId ). Increment () // ... s . requestLatency . Record ( clock . Now (). Sub ( start )) s . responseSizes . Record ( res . size ) return } func newServer ( registry * spectator . Registry ) * Server { return & Server { registry , registry . NewId ( \"server.requestCount\" , nil ), registry . Timer ( \"server.requestLatency\" , nil ), registry . DistributionSummary ( \"server.responseSizes\" , nil ), } } func getNextRequest () * Request { // ... return & Request { \"US\" } } func main () { commonTags := map [ string ] string { \"nf.app\" : \"example\" , \"nf.region\" : \"us-west-1\" } config := & spectator . Config { Frequency : 5 * time . Second , Timeout : 1 * time . Second , Uri : \"http://example.org/api/v1/publish\" , CommonTags : commonTags } registry := spectator . NewRegistry ( config ) // optionally set custom logger (needs to implement Debugf, Infof, Errorf) // registry.SetLogger(logger) registry . Start () defer registry . Stop () // collect memory and file descriptor metrics spectator . CollectRuntimeMetrics ( registry ) server := newServer ( registry ) for i := 1 ; i < 3 ; i ++ { // get a request req := getNextRequest () server . Handle ( req ) } } Netflix Integration \u00b6 Create a Netflix Spectator Config to be used by spectator-go , replacing STASH_HOSTNAME with the hostname of the internal Stash server. import ( nfspectator \"STASH_HOSTNAME/cldmta/netflix-spectator-goconf\" spectator \"github.com/Netflix/spectator-go\" ) func main () { config := nfspectator . Config () registry := spectator . NewRegistry ( config ) registry . Start () defer registry . Stop () # ... }","title":"Usage"},{"location":"spectator/lang/go/usage/#project","text":"Source Product Lifecycle: Beta This implements a basic Spectator library for instrumenting golang applications and sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/go/usage/#install-library","text":"Add a github.com/Netflix/spectator-go remote import to your code. If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/go/usage/#instrumenting-code","text":"package main import ( \"github.com/Netflix/spectator-go\" \"strconv\" \"time\" ) type Server struct { registry * spectator . Registry requestCountId * spectator . Id requestLatency * spectator . Timer responseSizes * spectator . DistributionSummary } type Request struct { country string } type Response struct { status int size int64 } func ( s * Server ) Handle ( request * Request ) ( res * Response ) { clock := s . registry . Clock () start := clock . Now () // initialize res res = & Response { 200 , 64 } // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a map // However, it is more expensive than having a local variable set // to the counter. cntId := s . requestCountId . WithTag ( \"country\" , request . country ). WithTag ( \"status\" , strconv . Itoa ( res . status )) s . registry . CounterWithId ( cntId ). Increment () // ... s . requestLatency . Record ( clock . Now (). Sub ( start )) s . responseSizes . Record ( res . size ) return } func newServer ( registry * spectator . Registry ) * Server { return & Server { registry , registry . NewId ( \"server.requestCount\" , nil ), registry . Timer ( \"server.requestLatency\" , nil ), registry . DistributionSummary ( \"server.responseSizes\" , nil ), } } func getNextRequest () * Request { // ... return & Request { \"US\" } } func main () { commonTags := map [ string ] string { \"nf.app\" : \"example\" , \"nf.region\" : \"us-west-1\" } config := & spectator . Config { Frequency : 5 * time . Second , Timeout : 1 * time . Second , Uri : \"http://example.org/api/v1/publish\" , CommonTags : commonTags } registry := spectator . NewRegistry ( config ) // optionally set custom logger (needs to implement Debugf, Infof, Errorf) // registry.SetLogger(logger) registry . Start () defer registry . Stop () // collect memory and file descriptor metrics spectator . CollectRuntimeMetrics ( registry ) server := newServer ( registry ) for i := 1 ; i < 3 ; i ++ { // get a request req := getNextRequest () server . Handle ( req ) } }","title":"Instrumenting Code"},{"location":"spectator/lang/go/usage/#netflix-integration","text":"Create a Netflix Spectator Config to be used by spectator-go , replacing STASH_HOSTNAME with the hostname of the internal Stash server. import ( nfspectator \"STASH_HOSTNAME/cldmta/netflix-spectator-goconf\" spectator \"github.com/Netflix/spectator-go\" ) func main () { config := nfspectator . Config () registry := spectator . NewRegistry ( config ) registry . Start () defer registry . Stop () # ... }","title":"Netflix Integration"},{"location":"spectator/lang/go/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/go/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/go/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/go/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/go/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/java/servo-migration/","text":"Servo Comparison \u00b6 Servo is an alternative client monitoring library that is also developed by Netflix. Originally, Spectator was an experiment for a simpler API that wrapped Servo. It was done as a separate project to avoid breaking backwards compatibility for Servo. From a user perspective, both will be supported for a long time, but most of our efforts for future improvement will go to Spectator. For new code, it is recommended to use the spectator API. If running at Netflix , the correct bindings will be in place for both Servo and Spectator. Differences \u00b6 This section provides a quick summary of the differences between Spectator and Servo. Simpler API \u00b6 Servo gives the user a lot of control, but this makes it hard to use correctly. For example, to create a Counter, the user needs to understand the trade-offs and choose between: BasicCounter DynamicCounter ContextualCounter StepCounter Further, each of these can impact how data is reported to observers. The Spectator API focuses on the constructs a user needs to instrument the code. In Spectator, the user would always use the Registry to create a Counter . The implementation details are left up to the Registry. The registration is simpler as well to avoid common pitfalls when using Servo like overwriting a registered object. More Focused \u00b6 The goal of Spectator is instrumenting code to send to a dimensional time-series system like Atlas . Servo has goals of staying compatible with a number of legacy libraries and naming formats, exposing data to JMX, etc. Examples of how this influences decisions: No support for non-numeric data. Servo supported this feature, so that it can expose data to JMX. Exposing the numeric data registered in Spectator to JMX can be done using a registry that supports it, but there is no goal to be a general interface for exposing arbitrary data in JMX. No support for custom time units when reporting timer data. Base units should always be used for reporting and conversions can be performed in the presentation layer, if needed. It also avoids a lot of the confusion around the timer unit for the data and issues like creating aggregates that are meaningless due to mixed units. It is better to have a simple way to send correct and easy-to-understand data to the backend than many options. If you want more knobs, then you can use Servo. DI Friendly \u00b6 When Servo was originally written, dependency injection (DI) was not heavily used at Netflix. Further, Servo needed to stay compatible with a number of use-cases that were heavily static. While Spectator does have a static registry that can be used, the recommended way is to create a registry and inject it either manually or via a framework into the classes that need it. This also makes it much easier to test in isolation . Migration \u00b6 If you want to migrate from the Servo API to the Spectator API, then this section provides some guides on how Servo constructs can be ported over. The sub-sections are the class names of monitor types supported by Servo. For users at Netflix, we are not actively pushing teams to migrate or do any additional work. Servo is still supported and if it works for your use-case, then feel free to continue using it. Registration \u00b6 First read through the Servo docs on registration . With Servo, say you have a class like the following: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } public void doSomething () { ... } } The state of the class is in the member variables of an instance of Foo . If multiple instances of class Foo are created with the same value for id , then the last one will overwrite the others for the registration. So the values getting reported will only be from the last instance registered. Also the registry has a reference to the instance of Foo , so it will never go away. For Counters and Timers, one way to get around this is to use DynamicCounter and DynamicTimer , respectively. Those classes will automatically handle the registration and expire if there is no activity. They also get used for cases where the set of dimensions is not known up front. Gauges need to sample the state of something, so they need to have a reference to an object that contains the state. So the user would need to ensure that only a single copy was registered leading to patterns like: class Foo { private static class FooStats { private AtomicInteger gauge ; private Counter counter ; public FooStats ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } } private static ConcurrentHashMap < String , FooStats > STATS = new ConcurrentHashMap <> (); private final FooStats stats ; public Foo ( String id ) { stats = STATS . computeIfAbsent ( id , ( i ) -> new FooStats ( i )); } public void doSomething () { ... stats . update (); } } This ensures that there is a single copy for a given id. In spectator this example would look like: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"gauge\" ). withTag ( \"id\" , id ); gauge = registry . gauge ( gaugeId , new AtomicInteger ()); counter = registry . counter ( \"counter\" , \"id\" , id ); } public void doSomething () { ... } } Everything using the same Registry will get the same Counter instance, if the same id is used. For the Gauge, the Registry will keep a weak reference and will sum the values if multiple instances are present. Since it is a weak reference, nothing will prevent an instance of Foo from getting garbage collected. Annotations \u00b6 Annotations are not supported, use the appropriate meter type: DataSourceType Spectator Alternative COUNTER Counter Usage GAUGE Gauge Usage INFORMATIONAL Not supported BasicCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new BasicCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } } BasicGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final BasicGauge g = new BasicGauge ( MonitorConfig . builder ( \"name\" ). build (), this :: getCurrentValue ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); registry . gauge ( gaugeId , this , Foo :: getCurrentValue ); } } BasicTimer \u00b6 See the general overview of registration differences and summary of Timer usage . In Spectator, the reported unit for Timers is always seconds and cannot be changed. Seconds is the base unit and other units should only be used as a presentation detail. Servo allows the unit to be customized and defaults to milliseconds. Servo: public class Foo { private final Timer t = new BasicTimer ( MonitorConfig . builder ( \"name\" ). build (), TimeUnit . SECONDS ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . timer ( \"name\" , \"id\" , id ); } public void doSomething () { t . record (() -> { ... }); } } BasicDistributionSummary \u00b6 See the general overview of registration differences and summary of Distribution Summary usage . Servo: public class Foo { private final BasicDistributionSummary s = new BasicDistributionSummary ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { ... s . record ( getValue ()); } } Spectator: public class Foo { private final DistributionSummary s ; @Inject public Foo ( Registry registry , String id ) { s = registry . distributionSummary ( \"name\" , \"id\" , id ); } public void doSomething () { ... s . record ( getValue ()); } } BasicInformational \u00b6 Not supported, see the overview of differences . BasicStopwatch \u00b6 There isn't an explicit stopwatch class in Spectator. Use a timing call directly. Servo: public void doSomething () { Stopwatch s = timer . start (); try { ... } finally { s . stop (); } } Spectator: public void doSomething () { final long s = System . nanoTime (); try { ... } finally { timer . record ( System . nanoTime () - s , TimeUnit . NANOSECONDS ); } } BucketTimer \u00b6 TODO: find sandbox documentation or remove reference See the general overview of registration differences and the summary of sandbox documentation . In Spectator, BucketTimer is provided in the sandbox extension library and may change in future as we gain more experience using it. Servo: public class Foo { private final Timer t = new BucketTimer ( MonitorConfig . builder ( \"name\" ). build (), new BucketConfig . Builder () . withTimeUnit ( TimeUnit . MILLISECONDS ) . withBuckets ( new long [] { 500 , 2500 , 5000 , 10000 }) . build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { Id timerId = registry . createId ( \"name\" , \"id\" , id ); BucketFunction f = BucketFunctions . latency ( 10 , TimeUnit . SECONDS ); t = BucketTimer . get ( registry , timerId , f ); } public void doSomething () { t . record (() -> { ... }); } } ContextualCounter \u00b6 Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 . ContextualTimer \u00b6 Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 . DoubleGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final DoubleGauge g = new DoubleGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: import com.google.common.util.concurrent.AtomicDouble ; public class Foo { private final AtomicDouble v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicDouble ()); } } DurationTimer \u00b6 See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final DurationTimer t = new DurationTimer ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final LongTaskTimer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . longTaskTimer ( \"name\" , \"id\" , id ); } } DynamicCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final String id ; public Foo ( String id ) { this . id = id ; } public void doSomething ( Context ctxt ) { DynamicCounter . increment ( \"staticId\" , \"id\" , id ); DynamicCounter . increment ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Counter staticCounter ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticCounter = registry . counter ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { // Keeping the reference to the counter avoids additional allocations // to create the id object and the lookup cost staticCounter . increment (); // If the id is dynamic it must be looked up registry . counter ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()). increment (); // This will update the same counter as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . counter ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())). increment (); } } DynamicTimer \u00b6 See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final String id ; private final MonitorConfig staticId ; public Foo ( String id ) { this . id = id ; staticId = MonitorConfig . builder ( \"staticId\" ). withTag ( \"id\" , id ). build (); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); DynamicTimer . record ( staticId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); MonitorConfig dynamicId = MonitorConfig . builder ( \"dynamicId\" ) . withTag ( \"id\" , id ) . withTag ( \"foo\" , ctxt . getFoo ()) . build (); DynamicTimer . record ( dynamicId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Timer staticTimer ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticTimer = registry . timer ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); // Keeping the reference to the timer avoids additional allocations // to create the id object and the lookup cost staticTimer . record ( d , TimeUnit . MILLISECONDS ); // If the id is dynamic it must be looked up registry . timer ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()) . record ( d , TimeUnit . MILLISECONDS ); // This will update the same timer as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . timer ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())) . record ( d , TimeUnit . MILLISECONDS ); } } LongGauge \u00b6 See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final LongGauge g = new LongGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } } MonitorConfig \u00b6 See the documentation on naming . Servo: MonitorConfig id = MonitorConfig . builder ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ) . build (); Spectator: Id id = registry . createId ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ); // or Id id = registry . createId ( \"name\" , \"country\" , \"US\" , \"device\" , \"xbox\" ); MonitoredCache \u00b6 Not supported because Spectator does not have a direct dependency on Guava. If there is enough demand, an extension can be created. NumberGauge \u00b6 See the general overview of registration differences and summary of gauge usage . Servo: public class Foo { private final NumberGauge g = new NumberGauge ( MonitorConfig . builder ( \"name\" ). build (), new AtomicLong ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } } StatsTimer \u00b6 Not supported, see overview of differences . StepCounter \u00b6 See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new StepCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"Servo Migration"},{"location":"spectator/lang/java/servo-migration/#servo-comparison","text":"Servo is an alternative client monitoring library that is also developed by Netflix. Originally, Spectator was an experiment for a simpler API that wrapped Servo. It was done as a separate project to avoid breaking backwards compatibility for Servo. From a user perspective, both will be supported for a long time, but most of our efforts for future improvement will go to Spectator. For new code, it is recommended to use the spectator API. If running at Netflix , the correct bindings will be in place for both Servo and Spectator.","title":"Servo Comparison"},{"location":"spectator/lang/java/servo-migration/#differences","text":"This section provides a quick summary of the differences between Spectator and Servo.","title":"Differences"},{"location":"spectator/lang/java/servo-migration/#simpler-api","text":"Servo gives the user a lot of control, but this makes it hard to use correctly. For example, to create a Counter, the user needs to understand the trade-offs and choose between: BasicCounter DynamicCounter ContextualCounter StepCounter Further, each of these can impact how data is reported to observers. The Spectator API focuses on the constructs a user needs to instrument the code. In Spectator, the user would always use the Registry to create a Counter . The implementation details are left up to the Registry. The registration is simpler as well to avoid common pitfalls when using Servo like overwriting a registered object.","title":"Simpler API"},{"location":"spectator/lang/java/servo-migration/#more-focused","text":"The goal of Spectator is instrumenting code to send to a dimensional time-series system like Atlas . Servo has goals of staying compatible with a number of legacy libraries and naming formats, exposing data to JMX, etc. Examples of how this influences decisions: No support for non-numeric data. Servo supported this feature, so that it can expose data to JMX. Exposing the numeric data registered in Spectator to JMX can be done using a registry that supports it, but there is no goal to be a general interface for exposing arbitrary data in JMX. No support for custom time units when reporting timer data. Base units should always be used for reporting and conversions can be performed in the presentation layer, if needed. It also avoids a lot of the confusion around the timer unit for the data and issues like creating aggregates that are meaningless due to mixed units. It is better to have a simple way to send correct and easy-to-understand data to the backend than many options. If you want more knobs, then you can use Servo.","title":"More Focused"},{"location":"spectator/lang/java/servo-migration/#di-friendly","text":"When Servo was originally written, dependency injection (DI) was not heavily used at Netflix. Further, Servo needed to stay compatible with a number of use-cases that were heavily static. While Spectator does have a static registry that can be used, the recommended way is to create a registry and inject it either manually or via a framework into the classes that need it. This also makes it much easier to test in isolation .","title":"DI Friendly"},{"location":"spectator/lang/java/servo-migration/#migration","text":"If you want to migrate from the Servo API to the Spectator API, then this section provides some guides on how Servo constructs can be ported over. The sub-sections are the class names of monitor types supported by Servo. For users at Netflix, we are not actively pushing teams to migrate or do any additional work. Servo is still supported and if it works for your use-case, then feel free to continue using it.","title":"Migration"},{"location":"spectator/lang/java/servo-migration/#registration","text":"First read through the Servo docs on registration . With Servo, say you have a class like the following: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } public void doSomething () { ... } } The state of the class is in the member variables of an instance of Foo . If multiple instances of class Foo are created with the same value for id , then the last one will overwrite the others for the registration. So the values getting reported will only be from the last instance registered. Also the registry has a reference to the instance of Foo , so it will never go away. For Counters and Timers, one way to get around this is to use DynamicCounter and DynamicTimer , respectively. Those classes will automatically handle the registration and expire if there is no activity. They also get used for cases where the set of dimensions is not known up front. Gauges need to sample the state of something, so they need to have a reference to an object that contains the state. So the user would need to ensure that only a single copy was registered leading to patterns like: class Foo { private static class FooStats { private AtomicInteger gauge ; private Counter counter ; public FooStats ( String id ) { gauge = new AtomicInteger (); counter = new BasicCounter ( MonitorConfig . builder ( \"counter\" ). build ()); Monitors . registerObject ( id , this ); } @Monitor ( name = \"gauge\" , type = DataSourceType . GAUGE ) private int gauge () { return gauge . get (); } } private static ConcurrentHashMap < String , FooStats > STATS = new ConcurrentHashMap <> (); private final FooStats stats ; public Foo ( String id ) { stats = STATS . computeIfAbsent ( id , ( i ) -> new FooStats ( i )); } public void doSomething () { ... stats . update (); } } This ensures that there is a single copy for a given id. In spectator this example would look like: public class Foo { private AtomicInteger gauge ; private Counter counter ; public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"gauge\" ). withTag ( \"id\" , id ); gauge = registry . gauge ( gaugeId , new AtomicInteger ()); counter = registry . counter ( \"counter\" , \"id\" , id ); } public void doSomething () { ... } } Everything using the same Registry will get the same Counter instance, if the same id is used. For the Gauge, the Registry will keep a weak reference and will sum the values if multiple instances are present. Since it is a weak reference, nothing will prevent an instance of Foo from getting garbage collected.","title":"Registration"},{"location":"spectator/lang/java/servo-migration/#annotations","text":"Annotations are not supported, use the appropriate meter type: DataSourceType Spectator Alternative COUNTER Counter Usage GAUGE Gauge Usage INFORMATIONAL Not supported","title":"Annotations"},{"location":"spectator/lang/java/servo-migration/#basiccounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new BasicCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"BasicCounter"},{"location":"spectator/lang/java/servo-migration/#basicgauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final BasicGauge g = new BasicGauge ( MonitorConfig . builder ( \"name\" ). build (), this :: getCurrentValue ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); registry . gauge ( gaugeId , this , Foo :: getCurrentValue ); } }","title":"BasicGauge"},{"location":"spectator/lang/java/servo-migration/#basictimer","text":"See the general overview of registration differences and summary of Timer usage . In Spectator, the reported unit for Timers is always seconds and cannot be changed. Seconds is the base unit and other units should only be used as a presentation detail. Servo allows the unit to be customized and defaults to milliseconds. Servo: public class Foo { private final Timer t = new BasicTimer ( MonitorConfig . builder ( \"name\" ). build (), TimeUnit . SECONDS ); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . timer ( \"name\" , \"id\" , id ); } public void doSomething () { t . record (() -> { ... }); } }","title":"BasicTimer"},{"location":"spectator/lang/java/servo-migration/#basicdistributionsummary","text":"See the general overview of registration differences and summary of Distribution Summary usage . Servo: public class Foo { private final BasicDistributionSummary s = new BasicDistributionSummary ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { ... s . record ( getValue ()); } } Spectator: public class Foo { private final DistributionSummary s ; @Inject public Foo ( Registry registry , String id ) { s = registry . distributionSummary ( \"name\" , \"id\" , id ); } public void doSomething () { ... s . record ( getValue ()); } }","title":"BasicDistributionSummary"},{"location":"spectator/lang/java/servo-migration/#basicinformational","text":"Not supported, see the overview of differences .","title":"BasicInformational"},{"location":"spectator/lang/java/servo-migration/#basicstopwatch","text":"There isn't an explicit stopwatch class in Spectator. Use a timing call directly. Servo: public void doSomething () { Stopwatch s = timer . start (); try { ... } finally { s . stop (); } } Spectator: public void doSomething () { final long s = System . nanoTime (); try { ... } finally { timer . record ( System . nanoTime () - s , TimeUnit . NANOSECONDS ); } }","title":"BasicStopwatch"},{"location":"spectator/lang/java/servo-migration/#buckettimer","text":"TODO: find sandbox documentation or remove reference See the general overview of registration differences and the summary of sandbox documentation . In Spectator, BucketTimer is provided in the sandbox extension library and may change in future as we gain more experience using it. Servo: public class Foo { private final Timer t = new BucketTimer ( MonitorConfig . builder ( \"name\" ). build (), new BucketConfig . Builder () . withTimeUnit ( TimeUnit . MILLISECONDS ) . withBuckets ( new long [] { 500 , 2500 , 5000 , 10000 }) . build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { Stopwatch s = t . start (); try { ... } finally { s . stop (); } } } Spectator: public class Foo { private final Timer t ; @Inject public Foo ( Registry registry , String id ) { Id timerId = registry . createId ( \"name\" , \"id\" , id ); BucketFunction f = BucketFunctions . latency ( 10 , TimeUnit . SECONDS ); t = BucketTimer . get ( registry , timerId , f ); } public void doSomething () { t . record (() -> { ... }); } }","title":"BucketTimer"},{"location":"spectator/lang/java/servo-migration/#contextualcounter","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 .","title":"ContextualCounter"},{"location":"spectator/lang/java/servo-migration/#contextualtimer","text":"Not supported. A fixed tag list for the context is too rigid and this class was never used much at Netflix. Future work being looked at in issue-180 .","title":"ContextualTimer"},{"location":"spectator/lang/java/servo-migration/#doublegauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final DoubleGauge g = new DoubleGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: import com.google.common.util.concurrent.AtomicDouble ; public class Foo { private final AtomicDouble v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicDouble ()); } }","title":"DoubleGauge"},{"location":"spectator/lang/java/servo-migration/#durationtimer","text":"See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final DurationTimer t = new DurationTimer ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final LongTaskTimer t ; @Inject public Foo ( Registry registry , String id ) { t = registry . longTaskTimer ( \"name\" , \"id\" , id ); } }","title":"DurationTimer"},{"location":"spectator/lang/java/servo-migration/#dynamiccounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final String id ; public Foo ( String id ) { this . id = id ; } public void doSomething ( Context ctxt ) { DynamicCounter . increment ( \"staticId\" , \"id\" , id ); DynamicCounter . increment ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Counter staticCounter ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticCounter = registry . counter ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { // Keeping the reference to the counter avoids additional allocations // to create the id object and the lookup cost staticCounter . increment (); // If the id is dynamic it must be looked up registry . counter ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()). increment (); // This will update the same counter as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . counter ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())). increment (); } }","title":"DynamicCounter"},{"location":"spectator/lang/java/servo-migration/#dynamictimer","text":"See the general overview of registration differences and summary of Timer usage . Servo: public class Foo { private final String id ; private final MonitorConfig staticId ; public Foo ( String id ) { this . id = id ; staticId = MonitorConfig . builder ( \"staticId\" ). withTag ( \"id\" , id ). build (); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); DynamicTimer . record ( staticId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); MonitorConfig dynamicId = MonitorConfig . builder ( \"dynamicId\" ) . withTag ( \"id\" , id ) . withTag ( \"foo\" , ctxt . getFoo ()) . build (); DynamicTimer . record ( dynamicId , TimeUnit . SECONDS , d , TimeUnit . MILLISECONDS ); } } Spectator: public class Foo { private final Registry registry ; private final String id ; private final Timer staticTimer ; private final Id dynamicId ; @Inject public Foo ( Registry registry , String id ) { this . registry = registry ; this . id = id ; staticTimer = registry . timer ( \"staticId\" , \"id\" , id ); dynamicId = registry . createId ( \"dynamicId\" , \"id\" , id ); } public void doSomething ( Context ctxt ) { final long d = ctxt . getDurationMillis (); // Keeping the reference to the timer avoids additional allocations // to create the id object and the lookup cost staticTimer . record ( d , TimeUnit . MILLISECONDS ); // If the id is dynamic it must be looked up registry . timer ( \"dynamicId\" , \"id\" , id , \"foo\" , ctxt . getFoo ()) . record ( d , TimeUnit . MILLISECONDS ); // This will update the same timer as the line above, but the base part // of the id is precomputed to make it cheaper to construct the id. registry . timer ( dynamicId . withTag ( \"foo\" , ctxt . getFoo ())) . record ( d , TimeUnit . MILLISECONDS ); } }","title":"DynamicTimer"},{"location":"spectator/lang/java/servo-migration/#longgauge","text":"See the general overview of registration differences and summary of Gauge usage . Servo: public class Foo { private final LongGauge g = new LongGauge ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } }","title":"LongGauge"},{"location":"spectator/lang/java/servo-migration/#monitorconfig","text":"See the documentation on naming . Servo: MonitorConfig id = MonitorConfig . builder ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ) . build (); Spectator: Id id = registry . createId ( \"name\" ) . withTag ( \"country\" , \"US\" ) . withTag ( \"device\" , \"xbox\" ); // or Id id = registry . createId ( \"name\" , \"country\" , \"US\" , \"device\" , \"xbox\" );","title":"MonitorConfig"},{"location":"spectator/lang/java/servo-migration/#monitoredcache","text":"Not supported because Spectator does not have a direct dependency on Guava. If there is enough demand, an extension can be created.","title":"MonitoredCache"},{"location":"spectator/lang/java/servo-migration/#numbergauge","text":"See the general overview of registration differences and summary of gauge usage . Servo: public class Foo { private final NumberGauge g = new NumberGauge ( MonitorConfig . builder ( \"name\" ). build (), new AtomicLong ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } } Spectator: public class Foo { private final AtomicLong v ; @Inject public Foo ( Registry registry , String id ) { Id gaugeId = registry . createId ( \"name\" ). withTag ( \"id\" , id ); v = registry . gauge ( gaugeId , new AtomicLong ()); } }","title":"NumberGauge"},{"location":"spectator/lang/java/servo-migration/#statstimer","text":"Not supported, see overview of differences .","title":"StatsTimer"},{"location":"spectator/lang/java/servo-migration/#stepcounter","text":"See the general overview of registration differences and summary of Counter usage . Servo: public class Foo { private final Counter c = new StepCounter ( MonitorConfig . builder ( \"name\" ). build ()); public Foo ( String id ) { Monitors . registerObject ( id , this ); } public void doSomething () { c . increment (); } } Spectator: public class Foo { private final Counter c ; @Inject public Foo ( Registry registry , String id ) { c = registry . counter ( \"name\" , \"id\" , id ); } public void doSomething () { c . increment (); } }","title":"StepCounter"},{"location":"spectator/lang/java/testing/","text":"Testing \u00b6 Testing should be relatively straightforward if you are using injection for the Registry . Consider a sample class: public class Foo { private final Counter counter ; @Inject public Foo ( Registry registry ) { counter = registry . counter ( \"foo\" ); } public void doSomething () { counter . increment (); } } Tests will typically want to use an isolated instance of the DefaultRegistry . Simple Test \u00b6 A basic standalone test class would look something like: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { registry = new DefaultRegistry (); foo = new Foo ( registry ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } } Guice Test \u00b6 If using guice, then the TestModule can be used: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { Injector injector = Guice . createInjector ( new TestModule ()); registry = injector . getInstance ( Registry . class ); foo = injector . getInstance ( Foo . class ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } } Exceptions \u00b6 By default, for most user errors Spectator will log a warning rather than throw an exception. The rationale is that users do not often think about instrumentation and logging code causing an exception and interrupting the control flow of a program. However, for test cases it is recommended to be more aggressive and learn about problems as early as possible. This can be done by setting a system property: spectator.api.propagateWarnings=true Consider an example: private static final Id RARE_EXCEPTION_ID = null ; public void doSomethingImportant () { try { ... do work ... } catch ( RareException e ) { // There is a bug in the program, an Id is not allowed to be null. In production we do // not want it to throw and interrupt the control flow. Instrumentation should gracefully // degrade. registry . counter ( RARE_EXCEPTION_ID ). increment (); // These statements are important to provide context for operating the system // and to ensure the app continues to function properly. LOGGER . error ( \"important context for user\" , e ); properlyHandleException ( e ); } }","title":"Testing"},{"location":"spectator/lang/java/testing/#testing","text":"Testing should be relatively straightforward if you are using injection for the Registry . Consider a sample class: public class Foo { private final Counter counter ; @Inject public Foo ( Registry registry ) { counter = registry . counter ( \"foo\" ); } public void doSomething () { counter . increment (); } } Tests will typically want to use an isolated instance of the DefaultRegistry .","title":"Testing"},{"location":"spectator/lang/java/testing/#simple-test","text":"A basic standalone test class would look something like: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { registry = new DefaultRegistry (); foo = new Foo ( registry ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } }","title":"Simple Test"},{"location":"spectator/lang/java/testing/#guice-test","text":"If using guice, then the TestModule can be used: public class FooTest { private Registry registry ; private Foo foo ; @Before public void init () { Injector injector = Guice . createInjector ( new TestModule ()); registry = injector . getInstance ( Registry . class ); foo = injector . getInstance ( Foo . class ); } @Test public void doSomething () { foo . doSomething (); Assert . assertEquals ( 1 , registry . counter ( \"foo\" ). count ()); } }","title":"Guice Test"},{"location":"spectator/lang/java/testing/#exceptions","text":"By default, for most user errors Spectator will log a warning rather than throw an exception. The rationale is that users do not often think about instrumentation and logging code causing an exception and interrupting the control flow of a program. However, for test cases it is recommended to be more aggressive and learn about problems as early as possible. This can be done by setting a system property: spectator.api.propagateWarnings=true Consider an example: private static final Id RARE_EXCEPTION_ID = null ; public void doSomethingImportant () { try { ... do work ... } catch ( RareException e ) { // There is a bug in the program, an Id is not allowed to be null. In production we do // not want it to throw and interrupt the control flow. Instrumentation should gracefully // degrade. registry . counter ( RARE_EXCEPTION_ID ). increment (); // These statements are important to provide context for operating the system // and to ensure the app continues to function properly. LOGGER . error ( \"important context for user\" , e ); properlyHandleException ( e ); } }","title":"Exceptions"},{"location":"spectator/lang/java/usage/","text":"Project \u00b6 Source Javadoc Product Lifecycle: GA Requirements: Java >= 8 Install Library \u00b6 Depend on the API library, which is available in Maven Central . The only transitive dependency is slf4j . For Gradle, the dependency is specified as follows: dependencies { compile \"com.netflix.spectator:spectator-api:0.101.0\" } Pick a Registry to bind, when initializing the application. If running at Netflix, see the Netflix Integration section. Instrumenting Code \u00b6 Suppose we have a server and we want to keep track of: Number of requests received with dimensions for breaking down by status code, country, and the exception type if the request fails in an unexpected way. Latency for handling requests. Summary of the response sizes. Current number of active connections on the server. Here is some sample code that does that: // In the application initialization setup a registry Registry registry = new DefaultRegistry (); Server s = new Server ( registry ); public class Server { private final Registry registry ; private final Id requestCountId ; private final Timer requestLatency ; private final DistributionSummary responseSizes ; @Inject public Server ( Registry registry ) { this . registry = registry ; // Create a base id for the request count. The id will get refined with // additional dimensions when we receive a request. requestCountId = registry . createId ( \"server.requestCount\" ); // Create a timer for tracking the latency. The reference can be held onto // to avoid additional lookup cost in critical paths. requestLatency = registry . timer ( \"server.requestLatency\" ); // Create a distribution summary meter for tracking the response sizes. responseSizes = registry . distributionSummary ( \"server.responseSizes\" ); // Gauge type that can be sampled. In this case it will invoke the // specified method via reflection to get the value. The registry will // keep a weak reference to the object passed in so that registration will // not prevent garbage collection of the server object. registry . methodValue ( \"server.numConnections\" , this , \"getNumConnections\" ); } public Response handle ( Request req ) { final long s = System . nanoTime (); requestLatency . record (() -> { try { Response res = doSomething ( req ); // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a ConcurrentHashMap. // However, it is more expensive than having a local variable seti // to the counter. final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , res . status ()); registry . counter ( cntId ). increment (); responseSizes . record ( res . body (). size ()); return res ; } catch ( Exception e ) { final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , \"exception\" ) . withTag ( \"error\" , e . getClass (). getSimpleName ()); registry . counter ( cntId ). increment (); throw e ; } }); } public int getNumConnections () { // however we determine the current number of connections on the server } } Netflix Integration \u00b6 When running at Netflix, use the atlas-client library to enable transferring the instrumented data to Atlas . See the appropriate section for the type of project you are working on: Libraries Applications , specifically standalone apps using Guice or Governator directly. Base Server Libraries \u00b6 For libraries, the only dependency that should be needed is: com.netflix.spectator:spectator-api:0.101.0 The bindings to integrate internally should be included with the application. In your code, just inject a Registry , e.g.: public class Foo { @Inject public Foo ( Registry registry ) { ... } ... } See the testing docs for more information about creating a binding to use with tests. Libraries should not install SpectatorModule . The bindings to use for the Registry should be determined by the application that is using the library. Think of it as being like slf4j where logging configuration is up to the end-user, not the library owner. When creating a Guice module for your library, you may want to avoid binding errors if the end-user has not provided a binding for the Spectator registry. This can be done by using optional injections inside of the module, for example: // Sample library class public class MyLib { Registry registry ; @Inject public MyLib ( Registry registry ) { this . registry = registry ; } } // Guice module to configure the library and setup the bindings public class MyLibModule extends AbstractModule { private static final Logger LOGGER = LoggerFactory . getLogger ( MyLibModule . class ); @Override protected void configure () { } @Provides private MyLib provideMyLib ( OptionalInjections opts ) { return new MyLib ( opts . registry ()); } private static class OptionalInjections { @Inject ( optional = true ) private Registry registry ; Registry registry () { if ( registry == null ) { LOGGER . warn ( \"no spectator registry has been bound, so using noop implementation\" ); registry = new NoopRegistry (); } return registry ; } } } Applications \u00b6 Applications should include a dependency on the atlas-client plugin: netflix:atlas-client:latest.release Note this is an internal-only library with configs specific to the Netflix environments. It is assumed you are using Nebula so that internal Maven repositories are available for your build. When configuring with Governator, specify the AtlasModule : Injector injector = LifecycleInjector . builder () . withModules ( new AtlasModule ()) . build () . createInjector (); The Registry binding will then be available for injection as shown in the libraries section . The Insight libraries do not use any Governator or Guice specific features. It is possible to use Guice or other dependency injection frameworks directly with the following caveats: Some of the libraries use the @PostConstruct and @PreDestroy annotations for managing lifecycle. Governator adds lifecycle management and many other features on top of Guice and is the recommended way. For more minimalist support of just the lifecycle annotations on top of Guice, see iep-guice . The bindings and configuration necessary to run correctly with the internal setup are only supported as Guice modules. If you are trying to use some other dependency injection framework, then you will be responsible for either finding a way to leverage the Guice module in that framework or recreating those bindings and maintaining them as things change. It is not a paved path. Base Server \u00b6 If using base-server , then you will get the Spectator and Atlas bindings automatically.","title":"Usage"},{"location":"spectator/lang/java/usage/#project","text":"Source Javadoc Product Lifecycle: GA Requirements: Java >= 8","title":"Project"},{"location":"spectator/lang/java/usage/#install-library","text":"Depend on the API library, which is available in Maven Central . The only transitive dependency is slf4j . For Gradle, the dependency is specified as follows: dependencies { compile \"com.netflix.spectator:spectator-api:0.101.0\" } Pick a Registry to bind, when initializing the application. If running at Netflix, see the Netflix Integration section.","title":"Install Library"},{"location":"spectator/lang/java/usage/#instrumenting-code","text":"Suppose we have a server and we want to keep track of: Number of requests received with dimensions for breaking down by status code, country, and the exception type if the request fails in an unexpected way. Latency for handling requests. Summary of the response sizes. Current number of active connections on the server. Here is some sample code that does that: // In the application initialization setup a registry Registry registry = new DefaultRegistry (); Server s = new Server ( registry ); public class Server { private final Registry registry ; private final Id requestCountId ; private final Timer requestLatency ; private final DistributionSummary responseSizes ; @Inject public Server ( Registry registry ) { this . registry = registry ; // Create a base id for the request count. The id will get refined with // additional dimensions when we receive a request. requestCountId = registry . createId ( \"server.requestCount\" ); // Create a timer for tracking the latency. The reference can be held onto // to avoid additional lookup cost in critical paths. requestLatency = registry . timer ( \"server.requestLatency\" ); // Create a distribution summary meter for tracking the response sizes. responseSizes = registry . distributionSummary ( \"server.responseSizes\" ); // Gauge type that can be sampled. In this case it will invoke the // specified method via reflection to get the value. The registry will // keep a weak reference to the object passed in so that registration will // not prevent garbage collection of the server object. registry . methodValue ( \"server.numConnections\" , this , \"getNumConnections\" ); } public Response handle ( Request req ) { final long s = System . nanoTime (); requestLatency . record (() -> { try { Response res = doSomething ( req ); // Update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as lookup of id object in a ConcurrentHashMap. // However, it is more expensive than having a local variable seti // to the counter. final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , res . status ()); registry . counter ( cntId ). increment (); responseSizes . record ( res . body (). size ()); return res ; } catch ( Exception e ) { final Id cntId = requestCountId . withTag ( \"country\" , req . country ()) . withTag ( \"status\" , \"exception\" ) . withTag ( \"error\" , e . getClass (). getSimpleName ()); registry . counter ( cntId ). increment (); throw e ; } }); } public int getNumConnections () { // however we determine the current number of connections on the server } }","title":"Instrumenting Code"},{"location":"spectator/lang/java/usage/#netflix-integration","text":"When running at Netflix, use the atlas-client library to enable transferring the instrumented data to Atlas . See the appropriate section for the type of project you are working on: Libraries Applications , specifically standalone apps using Guice or Governator directly. Base Server","title":"Netflix Integration"},{"location":"spectator/lang/java/usage/#libraries","text":"For libraries, the only dependency that should be needed is: com.netflix.spectator:spectator-api:0.101.0 The bindings to integrate internally should be included with the application. In your code, just inject a Registry , e.g.: public class Foo { @Inject public Foo ( Registry registry ) { ... } ... } See the testing docs for more information about creating a binding to use with tests. Libraries should not install SpectatorModule . The bindings to use for the Registry should be determined by the application that is using the library. Think of it as being like slf4j where logging configuration is up to the end-user, not the library owner. When creating a Guice module for your library, you may want to avoid binding errors if the end-user has not provided a binding for the Spectator registry. This can be done by using optional injections inside of the module, for example: // Sample library class public class MyLib { Registry registry ; @Inject public MyLib ( Registry registry ) { this . registry = registry ; } } // Guice module to configure the library and setup the bindings public class MyLibModule extends AbstractModule { private static final Logger LOGGER = LoggerFactory . getLogger ( MyLibModule . class ); @Override protected void configure () { } @Provides private MyLib provideMyLib ( OptionalInjections opts ) { return new MyLib ( opts . registry ()); } private static class OptionalInjections { @Inject ( optional = true ) private Registry registry ; Registry registry () { if ( registry == null ) { LOGGER . warn ( \"no spectator registry has been bound, so using noop implementation\" ); registry = new NoopRegistry (); } return registry ; } } }","title":"Libraries"},{"location":"spectator/lang/java/usage/#applications","text":"Applications should include a dependency on the atlas-client plugin: netflix:atlas-client:latest.release Note this is an internal-only library with configs specific to the Netflix environments. It is assumed you are using Nebula so that internal Maven repositories are available for your build. When configuring with Governator, specify the AtlasModule : Injector injector = LifecycleInjector . builder () . withModules ( new AtlasModule ()) . build () . createInjector (); The Registry binding will then be available for injection as shown in the libraries section . The Insight libraries do not use any Governator or Guice specific features. It is possible to use Guice or other dependency injection frameworks directly with the following caveats: Some of the libraries use the @PostConstruct and @PreDestroy annotations for managing lifecycle. Governator adds lifecycle management and many other features on top of Guice and is the recommended way. For more minimalist support of just the lifecycle annotations on top of Guice, see iep-guice . The bindings and configuration necessary to run correctly with the internal setup are only supported as Guice modules. If you are trying to use some other dependency injection framework, then you will be responsible for either finding a way to leverage the Guice module in that framework or recreating those bindings and maintaining them as things change. It is not a paved path.","title":"Applications"},{"location":"spectator/lang/java/usage/#base-server","text":"If using base-server , then you will get the Spectator and Atlas bindings automatically.","title":"Base Server"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/","text":"Buffer Pools \u00b6 Buffer pools, such as direct byte buffers, can be monitored at a high level using the BufferPoolMXBean provided by the JDK. Getting Started \u00b6 To get information about buffer pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ()); Metrics \u00b6 jvm.buffer.count \u00b6 Gauge showing the current number of distinct buffers. Unit: count Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files. jvm.buffer.memoryUsed \u00b6 Gauge showing the current number of bytes used by all buffers. Unit: bytes Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"Buffer Pools"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#buffer-pools","text":"Buffer pools, such as direct byte buffers, can be monitored at a high level using the BufferPoolMXBean provided by the JDK.","title":"Buffer Pools"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#getting-started","text":"To get information about buffer pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ());","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#jvmbuffercount","text":"Gauge showing the current number of distinct buffers. Unit: count Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"jvm.buffer.count"},{"location":"spectator/lang/java/ext/jvm-buffer-pools/#jvmbuffermemoryused","text":"Gauge showing the current number of bytes used by all buffers. Unit: bytes Dimensions: id : type of buffers. Value will be either direct for direct byte buffers or mapped for memory mapped files.","title":"jvm.buffer.memoryUsed"},{"location":"spectator/lang/java/ext/jvm-gc-causes/","text":"GC Causes \u00b6 The various GC causes aren't well documented. The list provided here comes from the gcCause.cpp file in the jdk and we include some information on what these mean for the application. System.gc__ \u00b6 Something called System.gc() . If you are seeing this once an hour it is likely related to the RMI GC interval. For more details see: Unexplained System.gc() calls due to Remote Method Invocation (RMI) or explict garbage collections sun.rmi.dgc.client.gcInterval FullGCAlot \u00b6 Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+FullGCALot , that will trigger a full GC at a regular interval for testing purposes. ScavengeAlot \u00b6 Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+ScavengeALot , that will trigger a minor GC at a regular interval for testing purposes. Allocation_Profiler \u00b6 Prior to java 8 you would see this if running with the -Xaprof setting. It would be triggered just before the jvm exits. The -Xaprof option was removed in java 8. JvmtiEnv_ForceGarbageCollection \u00b6 Something called the JVM tool interface function ForceGarbageCollection . Look at the -agentlib param to java to see what agents are configured. GCLocker_Initiated_GC \u00b6 The GC locker prevents GC from occurring when JNI code is in a critical region . If GC is needed while a thread is in a critical region, then it will allow them to complete, i.e. call the corresponding release function. Other threads will not be permitted to enter a critical region. Once all threads are out of critical regions a GC event will be triggered. Heap_Inspection_Initiated_GC \u00b6 GC was initiated by an inspection operation on the heap. For example you can trigger this with jmap : $ jmap -histo:live <pid> Heap_Dump_Initiated_GC \u00b6 GC was initiated before dumping the heap. For example you can trigger this with jmap : $ jmap -dump:live,format=b,file=heap.out <pid> Another common example would be clicking the Heap Dump button on the Monitor tab in jvisualvm . WhiteBox_Initiated_Young_GC \u00b6 Most likely you'll never see this value. Used for testing hotspot, it indicates something called sun.hotspot.WhiteBox.youngGC() . No_GC \u00b6 Used for CMS to indicate concurrent phases. Allocation_Failure \u00b6 Usually this means that there is an allocation request that is bigger than the available space in young generation and will typically be associated with a minor GC. For G1 this will likely be a major GC and it is more common to see G1_Evacuation_Pause for routine minor collections. On linux the jvm will trigger a GC if the kernel indicates there isn't much memory left via mem_notify . Tenured_Generation_Full \u00b6 Not used? Permanent_Generation_Full \u00b6 Triggered as a result of an allocation failure in PermGen . Pre java 8. Metadata_GC_Threshold \u00b6 Triggered as a result of an allocation failure in Metaspace . Metaspace replaced PermGen was added in java 8. CMS_Generation_Full \u00b6 Not used? CMS_Initial_Mark \u00b6 Initial mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . CMS_Final_Remark \u00b6 Remark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . CMS_Concurrent_Mark \u00b6 Concurrent mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC . Old_Generation_Expanded_On_Last_Scavenge \u00b6 Not used? Old_Generation_Too_Full_To_Scavenge \u00b6 Not used? Ergonomics \u00b6 This indicates you are using the adaptive size policy, -XX:+UseAdaptiveSizePolicy and is on by default for recent versions, with the parallel collector ( -XX:+UseParallelGC ). For more details see The Why of GC Ergonomics . G1_Evacuation_Pause \u00b6 An evacuation pause is the most common young gen cause for G1 and indicates that it is copying live objects from one set of regions, young and sometimes young + old, to another set of regions. For more details see Understanding G1 GC Logs . G1_Humongous_Allocation \u00b6 A humongous allocation is one where the size is greater than 50% of the G1 region size. Before a humongous allocation the jvm checks if it should do a routine evacuation pause without regard to the actual allocation size, but if triggered due to this check the cause will be listed as humongous allocation. This cause is also used for any collections used to free up enough space for the allocation. Last_ditch_collection \u00b6 For perm gen (java 7 or earlier) and metaspace (java 8+) a last ditch collection will be triggered if an allocation fails and the memory pool cannot be expanded. ILLEGAL_VALUE_- last_gc_cause -_ILLEGAL_VALUE \u00b6 Included for completeness, but you should never see this value. unknown_GCCause \u00b6 Included for completeness, but you should never see this value.","title":"GC Causes"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#gc-causes","text":"The various GC causes aren't well documented. The list provided here comes from the gcCause.cpp file in the jdk and we include some information on what these mean for the application.","title":"GC Causes"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#systemgc__","text":"Something called System.gc() . If you are seeing this once an hour it is likely related to the RMI GC interval. For more details see: Unexplained System.gc() calls due to Remote Method Invocation (RMI) or explict garbage collections sun.rmi.dgc.client.gcInterval","title":"System.gc__"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#fullgcalot","text":"Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+FullGCALot , that will trigger a full GC at a regular interval for testing purposes.","title":"FullGCAlot"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#scavengealot","text":"Most likely you'll never see this value. In debug builds of the jdk there is an option, -XX:+ScavengeALot , that will trigger a minor GC at a regular interval for testing purposes.","title":"ScavengeAlot"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#allocation_profiler","text":"Prior to java 8 you would see this if running with the -Xaprof setting. It would be triggered just before the jvm exits. The -Xaprof option was removed in java 8.","title":"Allocation_Profiler"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#jvmtienv_forcegarbagecollection","text":"Something called the JVM tool interface function ForceGarbageCollection . Look at the -agentlib param to java to see what agents are configured.","title":"JvmtiEnv_ForceGarbageCollection"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#gclocker_initiated_gc","text":"The GC locker prevents GC from occurring when JNI code is in a critical region . If GC is needed while a thread is in a critical region, then it will allow them to complete, i.e. call the corresponding release function. Other threads will not be permitted to enter a critical region. Once all threads are out of critical regions a GC event will be triggered.","title":"GCLocker_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#heap_inspection_initiated_gc","text":"GC was initiated by an inspection operation on the heap. For example you can trigger this with jmap : $ jmap -histo:live <pid>","title":"Heap_Inspection_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#heap_dump_initiated_gc","text":"GC was initiated before dumping the heap. For example you can trigger this with jmap : $ jmap -dump:live,format=b,file=heap.out <pid> Another common example would be clicking the Heap Dump button on the Monitor tab in jvisualvm .","title":"Heap_Dump_Initiated_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#whitebox_initiated_young_gc","text":"Most likely you'll never see this value. Used for testing hotspot, it indicates something called sun.hotspot.WhiteBox.youngGC() .","title":"WhiteBox_Initiated_Young_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#no_gc","text":"Used for CMS to indicate concurrent phases.","title":"No_GC"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#allocation_failure","text":"Usually this means that there is an allocation request that is bigger than the available space in young generation and will typically be associated with a minor GC. For G1 this will likely be a major GC and it is more common to see G1_Evacuation_Pause for routine minor collections. On linux the jvm will trigger a GC if the kernel indicates there isn't much memory left via mem_notify .","title":"Allocation_Failure"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#tenured_generation_full","text":"Not used?","title":"Tenured_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#permanent_generation_full","text":"Triggered as a result of an allocation failure in PermGen . Pre java 8.","title":"Permanent_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#metadata_gc_threshold","text":"Triggered as a result of an allocation failure in Metaspace . Metaspace replaced PermGen was added in java 8.","title":"Metadata_GC_Threshold"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_generation_full","text":"Not used?","title":"CMS_Generation_Full"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_initial_mark","text":"Initial mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Initial_Mark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_final_remark","text":"Remark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Final_Remark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#cms_concurrent_mark","text":"Concurrent mark phase of CMS, for more details see Phases of CMS . Unfortunately it doesn't appear to be reported via the mbeans and we just get No_GC .","title":"CMS_Concurrent_Mark"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#old_generation_expanded_on_last_scavenge","text":"Not used?","title":"Old_Generation_Expanded_On_Last_Scavenge"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#old_generation_too_full_to_scavenge","text":"Not used?","title":"Old_Generation_Too_Full_To_Scavenge"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#ergonomics","text":"This indicates you are using the adaptive size policy, -XX:+UseAdaptiveSizePolicy and is on by default for recent versions, with the parallel collector ( -XX:+UseParallelGC ). For more details see The Why of GC Ergonomics .","title":"Ergonomics"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#g1_evacuation_pause","text":"An evacuation pause is the most common young gen cause for G1 and indicates that it is copying live objects from one set of regions, young and sometimes young + old, to another set of regions. For more details see Understanding G1 GC Logs .","title":"G1_Evacuation_Pause"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#g1_humongous_allocation","text":"A humongous allocation is one where the size is greater than 50% of the G1 region size. Before a humongous allocation the jvm checks if it should do a routine evacuation pause without regard to the actual allocation size, but if triggered due to this check the cause will be listed as humongous allocation. This cause is also used for any collections used to free up enough space for the allocation.","title":"G1_Humongous_Allocation"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#last_ditch_collection","text":"For perm gen (java 7 or earlier) and metaspace (java 8+) a last ditch collection will be triggered if an allocation fails and the memory pool cannot be expanded.","title":"Last_ditch_collection"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#illegal_value_-last_gc_cause-_illegal_value","text":"Included for completeness, but you should never see this value.","title":"ILLEGAL_VALUE_-last_gc_cause-_ILLEGAL_VALUE"},{"location":"spectator/lang/java/ext/jvm-gc-causes/#unknown_gccause","text":"Included for completeness, but you should never see this value.","title":"unknown_GCCause"},{"location":"spectator/lang/java/ext/jvm-gc/","text":"Garbage Collection \u00b6 The GC module registers with the notification emitter of the GarbageCollectorMXBean to provide some basic GC logging and metrics. Getting started Logging Metrics Alerting Getting Started \u00b6 For using it internally at Netflix, see the Java Usage guide, otherwise keep reading this section. Requirements \u00b6 This library relies on the notification emitter added in 7u4, but there are known issues prior to 7u40. There is also a regression impacting Java 9 and higher, see #502 and JDK-8196325 for more information. For G1, it is recommended to be on the latest version available. Dependencies \u00b6 com.netflix.spectator:spectator-ext-gc:0.101.0 Start Reporting \u00b6 Then in the initialization for the application: import com.netflix.spectator.gc.GcLogger ; ... // Keep a single instance of the logger GcLogger gc = new GcLogger (); gc . start ( null ); Logging \u00b6 After GC events, an INFO level log message will get reported using slf4j. This makes it easy to see GC events in the context of other log messages for the application. The logger name is com.netflix.spectator.gc.GcLogger and the message will look like: ${GC_TYPE}: ${COLLECTOR_NAME}, id=${N}, at=${START_TIME}, duration=${T}ms, cause=[${CAUSE}], ${TOTAL_USAGE_BEFORE} => ${TOTAL_USAGE_AFTER} / ${MAX_SIZE} (${PERCENT_USAGE_BEFORE} => ${PERCENT_USAGE_AFTER}) The id can be used to verify events were not skipped or correlate with other sources like detailed GC logs. See GC causes for more details on the possible causes. Sample: 2014-08-31 02:02:24,724 INFO [com.netflix.spectator.gc.GcLogger] YOUNG: ParNew, id=5281, at=Sun Aug 31 02:02:24 UTC 2014, duration=2ms, cause=[Allocation Failure], 0.4G => 0.3G / 1.8G (24.3% => 16.6%) Metrics \u00b6 jvm.gc.allocationRate \u00b6 The allocation rate measures how fast the application is allocating memory. It is a counter that is incremented after a GC event by the amount youngGen.sizeBeforeGC . Technically, right now it is: youngGen.sizeBeforeGC - youngGen.sizeAfterGC However, youngGen.sizeAfterGC should be 0 and thus the size of young gen before the GC is the amount allocated since the previous GC event. Unit: bytes/second Dimensions: n/a jvm.gc.promotionRate \u00b6 The promotion rate measures how fast data is being moved from young generation into the old generation. It is a counter that is incremented after a GC event by the amount: abs(oldGen.sizeAfterGC - oldGen.sizeBeforeGC) Unit: bytes/second Dimensions: n/a jvm.gc.liveDataSize \u00b6 The live data size is the size of the old generation after a major GC. The image below shows how the live data size view compares to a metric showing the current size of the memory pool: Unit: bytes Dimensions: n/a jvm.gc.maxDataSize \u00b6 Maximum size for the old generation. Primary use-case is for gaining perspective on the the live data size. Unit: bytes Dimensions: n/a jvm.gc.pause \u00b6 Pause time for a GC event. All of the values reported are stop the world pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page. jvm.gc.concurrentPhaseTime \u00b6 Time spent in concurrent phases of CMS pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page. Alerting \u00b6 This section assumes the data is available in Atlas , but users of other systems should be able to take the idea and make it work. For all of these alerts it is recommended to check them on instance. At Netflix that can be done by selecting the option in alert ui: Max Pause Time \u00b6 Example to trigger an alert if the pause time exceeds 500 milliseconds: name,jvm.gc.pause,:eq, statistic,max,:eq, :and, :max,(,cause,),:by, 0.5,:gt, $cause,:legend Heap Pressure \u00b6 Example to trigger an alert if the live data size is over 70% of the heap: name,jvm.gc.liveDataSize,:eq,:max, name,jvm.gc.maxDataSize,:eq,:max, :div,100,:mul, 70,:gt, percentUsed,:legend","title":"Garbage Collection"},{"location":"spectator/lang/java/ext/jvm-gc/#garbage-collection","text":"The GC module registers with the notification emitter of the GarbageCollectorMXBean to provide some basic GC logging and metrics. Getting started Logging Metrics Alerting","title":"Garbage Collection"},{"location":"spectator/lang/java/ext/jvm-gc/#getting-started","text":"For using it internally at Netflix, see the Java Usage guide, otherwise keep reading this section.","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-gc/#requirements","text":"This library relies on the notification emitter added in 7u4, but there are known issues prior to 7u40. There is also a regression impacting Java 9 and higher, see #502 and JDK-8196325 for more information. For G1, it is recommended to be on the latest version available.","title":"Requirements"},{"location":"spectator/lang/java/ext/jvm-gc/#dependencies","text":"com.netflix.spectator:spectator-ext-gc:0.101.0","title":"Dependencies"},{"location":"spectator/lang/java/ext/jvm-gc/#start-reporting","text":"Then in the initialization for the application: import com.netflix.spectator.gc.GcLogger ; ... // Keep a single instance of the logger GcLogger gc = new GcLogger (); gc . start ( null );","title":"Start Reporting"},{"location":"spectator/lang/java/ext/jvm-gc/#logging","text":"After GC events, an INFO level log message will get reported using slf4j. This makes it easy to see GC events in the context of other log messages for the application. The logger name is com.netflix.spectator.gc.GcLogger and the message will look like: ${GC_TYPE}: ${COLLECTOR_NAME}, id=${N}, at=${START_TIME}, duration=${T}ms, cause=[${CAUSE}], ${TOTAL_USAGE_BEFORE} => ${TOTAL_USAGE_AFTER} / ${MAX_SIZE} (${PERCENT_USAGE_BEFORE} => ${PERCENT_USAGE_AFTER}) The id can be used to verify events were not skipped or correlate with other sources like detailed GC logs. See GC causes for more details on the possible causes. Sample: 2014-08-31 02:02:24,724 INFO [com.netflix.spectator.gc.GcLogger] YOUNG: ParNew, id=5281, at=Sun Aug 31 02:02:24 UTC 2014, duration=2ms, cause=[Allocation Failure], 0.4G => 0.3G / 1.8G (24.3% => 16.6%)","title":"Logging"},{"location":"spectator/lang/java/ext/jvm-gc/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcallocationrate","text":"The allocation rate measures how fast the application is allocating memory. It is a counter that is incremented after a GC event by the amount youngGen.sizeBeforeGC . Technically, right now it is: youngGen.sizeBeforeGC - youngGen.sizeAfterGC However, youngGen.sizeAfterGC should be 0 and thus the size of young gen before the GC is the amount allocated since the previous GC event. Unit: bytes/second Dimensions: n/a","title":"jvm.gc.allocationRate"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcpromotionrate","text":"The promotion rate measures how fast data is being moved from young generation into the old generation. It is a counter that is incremented after a GC event by the amount: abs(oldGen.sizeAfterGC - oldGen.sizeBeforeGC) Unit: bytes/second Dimensions: n/a","title":"jvm.gc.promotionRate"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgclivedatasize","text":"The live data size is the size of the old generation after a major GC. The image below shows how the live data size view compares to a metric showing the current size of the memory pool: Unit: bytes Dimensions: n/a","title":"jvm.gc.liveDataSize"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcmaxdatasize","text":"Maximum size for the old generation. Primary use-case is for gaining perspective on the the live data size. Unit: bytes Dimensions: n/a","title":"jvm.gc.maxDataSize"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcpause","text":"Pause time for a GC event. All of the values reported are stop the world pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page.","title":"jvm.gc.pause"},{"location":"spectator/lang/java/ext/jvm-gc/#jvmgcconcurrentphasetime","text":"Time spent in concurrent phases of CMS pauses. Unit: seconds Dimensions: action : action performed by the garbage collector ( getGcAction ). There is no guarantee, but the typical values seen are end_of_major_GC and end_of_minor_GC . cause : cause that instigated GC ( getGcCause ). For an explanation of common causes see the GC causes page.","title":"jvm.gc.concurrentPhaseTime"},{"location":"spectator/lang/java/ext/jvm-gc/#alerting","text":"This section assumes the data is available in Atlas , but users of other systems should be able to take the idea and make it work. For all of these alerts it is recommended to check them on instance. At Netflix that can be done by selecting the option in alert ui:","title":"Alerting"},{"location":"spectator/lang/java/ext/jvm-gc/#max-pause-time","text":"Example to trigger an alert if the pause time exceeds 500 milliseconds: name,jvm.gc.pause,:eq, statistic,max,:eq, :and, :max,(,cause,),:by, 0.5,:gt, $cause,:legend","title":"Max Pause Time"},{"location":"spectator/lang/java/ext/jvm-gc/#heap-pressure","text":"Example to trigger an alert if the live data size is over 70% of the heap: name,jvm.gc.liveDataSize,:eq,:max, name,jvm.gc.maxDataSize,:eq,:max, :div,100,:mul, 70,:gt, percentUsed,:legend","title":"Heap Pressure"},{"location":"spectator/lang/java/ext/jvm-memory-pools/","text":"Memory Pools \u00b6 Uses the MemoryPoolMXBean provided by the JDK to monitor the sizes of java memory spaces such as perm gen, eden, old gen, etc. Getting Started \u00b6 To get information about memory pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ()); Metrics \u00b6 jvm.memory.used \u00b6 Gauge reporting the current amount of memory used. For the young and old gen pools this metric will typically have a sawtooth pattern. For alerting or detecting memory pressure the live data size is probably a better option. Unit: bytes Dimensions: see metric dimensions jvm.memory.committed \u00b6 Gauge reporting the current amount of memory committed. From the javadocs , committed is: The amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. Unit: bytes Dimensions: see metric dimensions jvm.memory.max \u00b6 Gauge reporting the max amount of memory that can be used. From the javadocs , max is: The maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used > committed even if used <= max would still be true (for example, when the system is low on virtual memory). Unit: bytes Dimensions: see metric dimensions Metric Dimensions \u00b6 All memory metrics have the following dimensions: id : name of the memory pool being reported. The names of the pools vary depending on the garbage collector algorithm being used. memtype : type of memory. It has two possible values: HEAP and NON_HEAP . For more information see the javadocs for MemoryType .","title":"Memory Pools"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#memory-pools","text":"Uses the MemoryPoolMXBean provided by the JDK to monitor the sizes of java memory spaces such as perm gen, eden, old gen, etc.","title":"Memory Pools"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#getting-started","text":"To get information about memory pools in spectator just setup registration of standard MXBeans. Note, if you are building an app at Netflix this should happen automatically via the normal platform initialization. import com.netflix.spectator.api.Spectator ; import com.netflix.spectator.jvm.Jmx ; Jmx . registerStandardMXBeans ( Spectator . registry ());","title":"Getting Started"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemoryused","text":"Gauge reporting the current amount of memory used. For the young and old gen pools this metric will typically have a sawtooth pattern. For alerting or detecting memory pressure the live data size is probably a better option. Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.used"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemorycommitted","text":"Gauge reporting the current amount of memory committed. From the javadocs , committed is: The amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.committed"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#jvmmemorymax","text":"Gauge reporting the max amount of memory that can be used. From the javadocs , max is: The maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used > committed even if used <= max would still be true (for example, when the system is low on virtual memory). Unit: bytes Dimensions: see metric dimensions","title":"jvm.memory.max"},{"location":"spectator/lang/java/ext/jvm-memory-pools/#metric-dimensions","text":"All memory metrics have the following dimensions: id : name of the memory pool being reported. The names of the pools vary depending on the garbage collector algorithm being used. memtype : type of memory. It has two possible values: HEAP and NON_HEAP . For more information see the javadocs for MemoryType .","title":"Metric Dimensions"},{"location":"spectator/lang/java/ext/log4j1/","text":"Log4j1 Appender \u00b6 Custom appender for log4j1 to track the number of log messages reported. Note Log4j 1.x has reached end of life and is no longer supported by Apache. This extension is provided for some users that have difficulty moving to a supported version of log4j. Getting Started \u00b6 To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j1:0.101.0 Then in your log4j configuration specify the com.netflix.spectator.log4j.SpectatorAppender . In a properties file it would look something like: log4j.rootLogger=ALL, A1 log4j.appender.A1=com.netflix.spectator.log4j.SpectatorAppender Metrics \u00b6 log4j.numMessages \u00b6 Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: loglevel : standard log level of the events. log4j.numStackTraces \u00b6 Counter for the number of messages with stack traces written to the logs. Unit: messages/second Dimensions: loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"Log4j1 Appender"},{"location":"spectator/lang/java/ext/log4j1/#log4j1-appender","text":"Custom appender for log4j1 to track the number of log messages reported. Note Log4j 1.x has reached end of life and is no longer supported by Apache. This extension is provided for some users that have difficulty moving to a supported version of log4j.","title":"Log4j1 Appender"},{"location":"spectator/lang/java/ext/log4j1/#getting-started","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j1:0.101.0 Then in your log4j configuration specify the com.netflix.spectator.log4j.SpectatorAppender . In a properties file it would look something like: log4j.rootLogger=ALL, A1 log4j.appender.A1=com.netflix.spectator.log4j.SpectatorAppender","title":"Getting Started"},{"location":"spectator/lang/java/ext/log4j1/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/log4j1/#log4jnummessages","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: loglevel : standard log level of the events.","title":"log4j.numMessages"},{"location":"spectator/lang/java/ext/log4j1/#log4jnumstacktraces","text":"Counter for the number of messages with stack traces written to the logs. Unit: messages/second Dimensions: loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"log4j.numStackTraces"},{"location":"spectator/lang/java/ext/log4j2/","text":"Log4j2 Appender \u00b6 Custom appender for log4j2 to track the number of log messages reported. Getting Started \u00b6 To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j2:0.101.0 Then in your application initialization: Registry registry = ... SpectatorAppender . addToRootLogger ( registry , // Registry to use \"spectator\" , // Name for the appender false ); // Should stack traces be ignored? This will add the appender to the root logger and register a listener so it will get re-added if the configuration changes. You can also use the appender by specifying it in the log4j2 configuration, but this will cause some of the loggers in Spectator to get created before log4j is properly initialized and result in some lost log messages. With that caveat in mind, if you need the additional flexibility of using the configuration then specify the Spectator appender: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration monitorInterval= \"5\" status= \"warn\" > <Appenders> <Spectator name= \"root\" /> </Appenders> <Loggers> <Root level= \"debug\" > <AppenderRef ref= \"root\" /> </Root> </Loggers> </Configuration> Metrics \u00b6 log4j.numMessages \u00b6 Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. log4j.numStackTraces \u00b6 Counter for the number of messages with stack traces written to the logs. This will only be collected if the ignoreExceptions flag is set to false for the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"Log4j2 Appender"},{"location":"spectator/lang/java/ext/log4j2/#log4j2-appender","text":"Custom appender for log4j2 to track the number of log messages reported.","title":"Log4j2 Appender"},{"location":"spectator/lang/java/ext/log4j2/#getting-started","text":"To use it simply add a dependency: com.netflix.spectator:spectator-ext-log4j2:0.101.0 Then in your application initialization: Registry registry = ... SpectatorAppender . addToRootLogger ( registry , // Registry to use \"spectator\" , // Name for the appender false ); // Should stack traces be ignored? This will add the appender to the root logger and register a listener so it will get re-added if the configuration changes. You can also use the appender by specifying it in the log4j2 configuration, but this will cause some of the loggers in Spectator to get created before log4j is properly initialized and result in some lost log messages. With that caveat in mind, if you need the additional flexibility of using the configuration then specify the Spectator appender: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration monitorInterval= \"5\" status= \"warn\" > <Appenders> <Spectator name= \"root\" /> </Appenders> <Loggers> <Root level= \"debug\" > <AppenderRef ref= \"root\" /> </Root> </Loggers> </Configuration>","title":"Getting Started"},{"location":"spectator/lang/java/ext/log4j2/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/log4j2/#log4jnummessages","text":"Counters showing the number of messages that have been passed to the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events.","title":"log4j.numMessages"},{"location":"spectator/lang/java/ext/log4j2/#log4jnumstacktraces","text":"Counter for the number of messages with stack traces written to the logs. This will only be collected if the ignoreExceptions flag is set to false for the appender. Unit: messages/second Dimensions: appender : name of the spectator appender. loglevel : standard log level of the events. exception : simple class name for the exception that was thrown. file : file name for where the exception was thrown.","title":"log4j.numStackTraces"},{"location":"spectator/lang/java/ext/placeholders/","text":"Placeholders \u00b6 The placeholders extension allows for identifiers to be created with dimensions that will get filled in based on the context when an activity occurs. The primary use-cases are to support: Optional dimensions that can be conditionally enabled. Pulling dimensions from another context such as a thread local store. This can make it is easier to share the across various parts of the code. Dependencies \u00b6 To use the placeholders support add a dependency on: com.netflix.spectator:spectator-ext-placeholders:0.101.0 Usage \u00b6 Placeholder support is available for activity based types including counters , timers , and distribution summaries . To get started create a PlaceholderFactory from the registry: PlaceholderFactory factory = PlaceholderFactory . from ( registry ); Then use the factory to create an identifier using a TagFactory to dynamically fetch the value for a given dimension when some activity occurs. Suppose we want to use a dynamic configuration library such as Archaius to conditionally enable a dimension with high cardinality: public class Server { private final Context context ; private final Counter rps ; public Server ( Context context , PropertyFactory props , Registry registry ) { this . context = context ; // Property that can be dynamically updated to indicate whether or not // detailed dimensions should be added to metrics. Property < Boolean > enabled = props . getProperty ( \"server.detailedMetricsEnabled\" ) . asBoolean ( false ); // Factory for creating instances of the counter using placeholders PlaceholderFactory factory = PlaceholderFactory . from ( registry ); // Create the underlying id with 4 possible dimensions: // * method and status - low cardinality and always added if available // in the context. // * geo and device - high cardinality and only available if the property // to enable detailed metrics is set to true. PlaceholderId rpsId = factory . createId ( \"server.requests\" ) . withTagFactory ( TagFactory . from ( \"method\" , context :: getMethod )) . withTagFactory ( TagFactory . from ( \"status\" , context :: getStatus )) . withTagFactory ( new DetailedDimension ( \"geo\" , enabled , context :: getGeo )) . withTagFactory ( new DetailedDimension ( \"device\" , enabled , context :: getDevice )); rps = factory . counter ( rpsId ); } public Response handle ( Request request ) { fillInContext ( request ); Response response = process ( request ); fillInContext ( response ); // Update the counter, the placeholders will be resolved when the activity, in // this case the increment is called. rps . increment (); return response ; } // Tag factory that can be controlled with an enabled property. private static class DetailedDimension implements TagFactory { private final String name ; private final Supplier < String > valueFunc ; DetailedDimension ( String name , Property < Boolean > enabled , Supplier < String > valueFunc ) { this . name = name ; this . enabled = enabled ; this . valueFunc = valueFunc ; } @Override public String name () { return name ; } @Override public Tag createTag () { return enabled . get () ? new BasicTag ( name , valueFunc . get ()) : null ; } } }","title":"Placeholders"},{"location":"spectator/lang/java/ext/placeholders/#placeholders","text":"The placeholders extension allows for identifiers to be created with dimensions that will get filled in based on the context when an activity occurs. The primary use-cases are to support: Optional dimensions that can be conditionally enabled. Pulling dimensions from another context such as a thread local store. This can make it is easier to share the across various parts of the code.","title":"Placeholders"},{"location":"spectator/lang/java/ext/placeholders/#dependencies","text":"To use the placeholders support add a dependency on: com.netflix.spectator:spectator-ext-placeholders:0.101.0","title":"Dependencies"},{"location":"spectator/lang/java/ext/placeholders/#usage","text":"Placeholder support is available for activity based types including counters , timers , and distribution summaries . To get started create a PlaceholderFactory from the registry: PlaceholderFactory factory = PlaceholderFactory . from ( registry ); Then use the factory to create an identifier using a TagFactory to dynamically fetch the value for a given dimension when some activity occurs. Suppose we want to use a dynamic configuration library such as Archaius to conditionally enable a dimension with high cardinality: public class Server { private final Context context ; private final Counter rps ; public Server ( Context context , PropertyFactory props , Registry registry ) { this . context = context ; // Property that can be dynamically updated to indicate whether or not // detailed dimensions should be added to metrics. Property < Boolean > enabled = props . getProperty ( \"server.detailedMetricsEnabled\" ) . asBoolean ( false ); // Factory for creating instances of the counter using placeholders PlaceholderFactory factory = PlaceholderFactory . from ( registry ); // Create the underlying id with 4 possible dimensions: // * method and status - low cardinality and always added if available // in the context. // * geo and device - high cardinality and only available if the property // to enable detailed metrics is set to true. PlaceholderId rpsId = factory . createId ( \"server.requests\" ) . withTagFactory ( TagFactory . from ( \"method\" , context :: getMethod )) . withTagFactory ( TagFactory . from ( \"status\" , context :: getStatus )) . withTagFactory ( new DetailedDimension ( \"geo\" , enabled , context :: getGeo )) . withTagFactory ( new DetailedDimension ( \"device\" , enabled , context :: getDevice )); rps = factory . counter ( rpsId ); } public Response handle ( Request request ) { fillInContext ( request ); Response response = process ( request ); fillInContext ( response ); // Update the counter, the placeholders will be resolved when the activity, in // this case the increment is called. rps . increment (); return response ; } // Tag factory that can be controlled with an enabled property. private static class DetailedDimension implements TagFactory { private final String name ; private final Supplier < String > valueFunc ; DetailedDimension ( String name , Property < Boolean > enabled , Supplier < String > valueFunc ) { this . name = name ; this . enabled = enabled ; this . valueFunc = valueFunc ; } @Override public String name () { return name ; } @Override public Tag createTag () { return enabled . get () ? new BasicTag ( name , valueFunc . get ()) : null ; } } }","title":"Usage"},{"location":"spectator/lang/java/ext/thread-pools/","text":"Thread Pools \u00b6 Java's ThreadPoolExecutor exposes several properties that are useful to monitor to assess the health, performance, and configuration of the pool. Getting Started \u00b6 To report thread pool metrics, one can attach a ThreadPoolMonitor in the following manner: import com.netflix.spectator.api.patterns.ThreadPoolMonitor ; ThreadPoolMonitor . attach ( registry , myThreadPoolExecutor , \"my-thread-pool\" ); The thread pool's properties will be polled regularly in the background and will report metrics to the provided registry. The third parameter will be added to each metric as an id dimension, if provided. However, if the value is null or an empty string, then a default will be used as the id . Metrics \u00b6 threadpool.taskCount \u00b6 Counter of the total number of tasks that have been scheduled. Unit: tasks/second Data Source: ThreadPoolExecutor#getTaskCount() threadpool.completedTaskCount \u00b6 Counter of the total number of tasks that have completed. Unit: tasks/second Data Source: ThreadPoolExecutor#getCompletedTaskCount() threadpool.currentThreadsBusy \u00b6 Gauge showing the current number of threads actively doing work. Unit: count Data Source: ThreadPoolExecutor#getActiveCount() threadpool.maxThreads \u00b6 Gauge showing the current maximum number of threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getMaximumPoolSize() threadpool.poolSize \u00b6 Gauge showing the current size of the pool. Unit: count Data Source: ThreadPoolExecutor#getPoolSize() threadpool.corePoolSize \u00b6 Gauge showing the current maximum number of core threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getCorePoolSize() threadpool.queueSize \u00b6 Gauge showing the current number of threads queued for execution. Unit: count Data Source: ThreadPoolExecutor#getQueue().size()","title":"Thread Pools"},{"location":"spectator/lang/java/ext/thread-pools/#thread-pools","text":"Java's ThreadPoolExecutor exposes several properties that are useful to monitor to assess the health, performance, and configuration of the pool.","title":"Thread Pools"},{"location":"spectator/lang/java/ext/thread-pools/#getting-started","text":"To report thread pool metrics, one can attach a ThreadPoolMonitor in the following manner: import com.netflix.spectator.api.patterns.ThreadPoolMonitor ; ThreadPoolMonitor . attach ( registry , myThreadPoolExecutor , \"my-thread-pool\" ); The thread pool's properties will be polled regularly in the background and will report metrics to the provided registry. The third parameter will be added to each metric as an id dimension, if provided. However, if the value is null or an empty string, then a default will be used as the id .","title":"Getting Started"},{"location":"spectator/lang/java/ext/thread-pools/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/java/ext/thread-pools/#threadpooltaskcount","text":"Counter of the total number of tasks that have been scheduled. Unit: tasks/second Data Source: ThreadPoolExecutor#getTaskCount()","title":"threadpool.taskCount"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcompletedtaskcount","text":"Counter of the total number of tasks that have completed. Unit: tasks/second Data Source: ThreadPoolExecutor#getCompletedTaskCount()","title":"threadpool.completedTaskCount"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcurrentthreadsbusy","text":"Gauge showing the current number of threads actively doing work. Unit: count Data Source: ThreadPoolExecutor#getActiveCount()","title":"threadpool.currentThreadsBusy"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolmaxthreads","text":"Gauge showing the current maximum number of threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getMaximumPoolSize()","title":"threadpool.maxThreads"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolpoolsize","text":"Gauge showing the current size of the pool. Unit: count Data Source: ThreadPoolExecutor#getPoolSize()","title":"threadpool.poolSize"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolcorepoolsize","text":"Gauge showing the current maximum number of core threads configured for the pool. Unit: count Data Source: ThreadPoolExecutor#getCorePoolSize()","title":"threadpool.corePoolSize"},{"location":"spectator/lang/java/ext/thread-pools/#threadpoolqueuesize","text":"Gauge showing the current number of threads queued for execution. Unit: count Data Source: ThreadPoolExecutor#getQueue().size()","title":"threadpool.queueSize"},{"location":"spectator/lang/java/meters/counter/","text":"Java Counters \u00b6 Counters are created using the Registry , which is be setup as part of application initialization. For example: public class Queue { private final Counter insertCounter ; private final Counter removeCounter ; private final QueueImpl impl ; @Inject public Queue ( Registry registry ) { insertCounter = registry . counter ( \"queue.insert\" ); removeCounter = registry . counter ( \"queue.remove\" ); impl = new QueueImpl (); } Then call increment when an event occurs: public void insert ( Object obj ) { insertCounter . increment (); impl . insert ( obj ); } public Object remove () { if ( impl . nonEmpty ()) { removeCounter . increment (); return impl . remove (); } else { return null ; } } Optionally, an amount can be passed in when calling increment. This is useful when a collection of events happen together. public void insertAll ( Collection < Object > objs ) { insertCounter . increment ( objs . size ()); impl . insertAll ( objs ); } }","title":"Counters"},{"location":"spectator/lang/java/meters/counter/#java-counters","text":"Counters are created using the Registry , which is be setup as part of application initialization. For example: public class Queue { private final Counter insertCounter ; private final Counter removeCounter ; private final QueueImpl impl ; @Inject public Queue ( Registry registry ) { insertCounter = registry . counter ( \"queue.insert\" ); removeCounter = registry . counter ( \"queue.remove\" ); impl = new QueueImpl (); } Then call increment when an event occurs: public void insert ( Object obj ) { insertCounter . increment (); impl . insert ( obj ); } public Object remove () { if ( impl . nonEmpty ()) { removeCounter . increment (); return impl . remove (); } else { return null ; } } Optionally, an amount can be passed in when calling increment. This is useful when a collection of events happen together. public void insertAll ( Collection < Object > objs ) { insertCounter . increment ( objs . size ()); impl . insertAll ( objs ); } }","title":"Java Counters"},{"location":"spectator/lang/java/meters/dist-summary/","text":"Java Distribution Summaries \u00b6 Distribution Summaries are created using the Registry , which will be setup as part of application initialization. For example: public class Server { private final DistributionSummary requestSize ; @Inject public Server ( Registry registry ) { requestSize = registry . distributionSummary ( \"server.requestSize\" ); } Then call record when an event occurs: public Response handle ( Request request ) { requestSize . record ( request . sizeInBytes ()); } }","title":"Distribution Summaries"},{"location":"spectator/lang/java/meters/dist-summary/#java-distribution-summaries","text":"Distribution Summaries are created using the Registry , which will be setup as part of application initialization. For example: public class Server { private final DistributionSummary requestSize ; @Inject public Server ( Registry registry ) { requestSize = registry . distributionSummary ( \"server.requestSize\" ); } Then call record when an event occurs: public Response handle ( Request request ) { requestSize . record ( request . sizeInBytes ()); } }","title":"Java Distribution Summaries"},{"location":"spectator/lang/java/meters/gauge/","text":"Java Gauges \u00b6 Polled Gauges \u00b6 The most common use of Gauges is by registering a hook with Spectator, so that it will poll the values in the background. This is done by using the PolledMeter helper class. A Polled Gauge is registered by passing in an id, a reference to the object, and a function to get or compute a numeric value based on the object. Note that a Gauge should only be registered once, not on each update. Consider this example of a web server tracking the number of connections: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; public HttpServer ( Registry registry ) { numConnections = PolledMeter . using ( registry ) . withName ( \"server.numConnections\" ) . monitorValue ( new AtomicInteger ( 0 )); } public void onConnectionCreated () { numConnections . incrementAndGet (); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); ... } ... } The Spectator Registry will keep a weak reference to the object. If the object is garbage collected, then it will automatically drop the registration. In the example above, the Registry will have a weak reference to numConnections and the server instance will have a strong reference to numConnections . If the server instance goes away, then the Gauge will as well. When multiple Gauges are registered with the same id, the reported value will be the sum of the matches. For example, if multiple instances of the HttpServer class were created on different ports, then the value server.numConnections would be the total number of connections across all server instances. If a different behavior is desired, then ensure your usage does not perform multiple registrations. There are several different ways to register a Gauge: Using Number \u00b6 A Gauge can also be created based on an implementation of Number. Note the Number implementation should be thread-safe. For example: AtomicInteger size = new AtomicInteger (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( size ); The call will return the Number so the registration can be inline on the assignment: AtomicInteger size = PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( new AtomicInteger ()); Updates to the value are performed by updating the Number instance directly. Using Lambda \u00b6 Specify a lambda that takes the object as parameter. public class Queue { @Inject public Queue ( Registry registry ) { PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( this , Queue :: size ); } ... } Warning Be careful to avoid creating a reference to the object in the lambda. It will prevent garbage collection and can lead to a memory leak in the application. For example, by calling size without using the passed in object there will be a reference to this : PolledMeter.using(registry) .withName(\"queue.size\") .monitorValue(this, obj -> size()); Collection Sizes \u00b6 For classes that implement Collection or Map , there are helpers: Queue queue = new LinkedBlockingQueue (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorSize ( queue ); Map < String , String > cache = new ConcurrentMap <> (); PolledMeter . using ( registry ) . withName ( \"cache.size\" ) . monitorSize ( cache ); Monotonic Counters \u00b6 A common technique used by some libraries is to expose a monotonically increasing counter that represents the number of events since the system was initialized. An example of that in the JDK is ThreadPoolExecutor.getCompletedTaskCount , which returns the number of completed tasks on the thread pool. For sources like this, the monitorMonotonicCounter method can be used: // For an implementation of Number LongAdder tasks = new LongAdder (); PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( tasks ); // Or using a lambda ThreadPoolExecutor executor = ... PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( executor , ThreadPoolExecutor :: getCompletedTaskCount ); For thread pools specifically, there are better options for getting standard metrics. See the docs for the Thread Pools extension for more information. Active Gauges \u00b6 Gauges can also be set directly by the user. In this mode, the user is responsible for regularly updating the value of the Gauge by calling set . Looking at the HttpServer example, with an active gauge, it would look like: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; private Gauge gauge ; public HttpServer ( Registry registry ) { numConnections = new AtomicInteger (); gauge = registry . gauge ( \"server.numConnections\" ); gauge . set ( numConnections . get ()); } public void onConnectionCreated () { numConnections . incrementAndGet (); gauge . set ( numConnections . get ()); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); gauge . set ( numConnections . get ()); ... } ... }","title":"Gauges"},{"location":"spectator/lang/java/meters/gauge/#java-gauges","text":"","title":"Java Gauges"},{"location":"spectator/lang/java/meters/gauge/#polled-gauges","text":"The most common use of Gauges is by registering a hook with Spectator, so that it will poll the values in the background. This is done by using the PolledMeter helper class. A Polled Gauge is registered by passing in an id, a reference to the object, and a function to get or compute a numeric value based on the object. Note that a Gauge should only be registered once, not on each update. Consider this example of a web server tracking the number of connections: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; public HttpServer ( Registry registry ) { numConnections = PolledMeter . using ( registry ) . withName ( \"server.numConnections\" ) . monitorValue ( new AtomicInteger ( 0 )); } public void onConnectionCreated () { numConnections . incrementAndGet (); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); ... } ... } The Spectator Registry will keep a weak reference to the object. If the object is garbage collected, then it will automatically drop the registration. In the example above, the Registry will have a weak reference to numConnections and the server instance will have a strong reference to numConnections . If the server instance goes away, then the Gauge will as well. When multiple Gauges are registered with the same id, the reported value will be the sum of the matches. For example, if multiple instances of the HttpServer class were created on different ports, then the value server.numConnections would be the total number of connections across all server instances. If a different behavior is desired, then ensure your usage does not perform multiple registrations. There are several different ways to register a Gauge:","title":"Polled Gauges"},{"location":"spectator/lang/java/meters/gauge/#using-number","text":"A Gauge can also be created based on an implementation of Number. Note the Number implementation should be thread-safe. For example: AtomicInteger size = new AtomicInteger (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( size ); The call will return the Number so the registration can be inline on the assignment: AtomicInteger size = PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( new AtomicInteger ()); Updates to the value are performed by updating the Number instance directly.","title":"Using Number"},{"location":"spectator/lang/java/meters/gauge/#using-lambda","text":"Specify a lambda that takes the object as parameter. public class Queue { @Inject public Queue ( Registry registry ) { PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorValue ( this , Queue :: size ); } ... } Warning Be careful to avoid creating a reference to the object in the lambda. It will prevent garbage collection and can lead to a memory leak in the application. For example, by calling size without using the passed in object there will be a reference to this : PolledMeter.using(registry) .withName(\"queue.size\") .monitorValue(this, obj -> size());","title":"Using Lambda"},{"location":"spectator/lang/java/meters/gauge/#collection-sizes","text":"For classes that implement Collection or Map , there are helpers: Queue queue = new LinkedBlockingQueue (); PolledMeter . using ( registry ) . withName ( \"queue.size\" ) . monitorSize ( queue ); Map < String , String > cache = new ConcurrentMap <> (); PolledMeter . using ( registry ) . withName ( \"cache.size\" ) . monitorSize ( cache );","title":"Collection Sizes"},{"location":"spectator/lang/java/meters/gauge/#monotonic-counters","text":"A common technique used by some libraries is to expose a monotonically increasing counter that represents the number of events since the system was initialized. An example of that in the JDK is ThreadPoolExecutor.getCompletedTaskCount , which returns the number of completed tasks on the thread pool. For sources like this, the monitorMonotonicCounter method can be used: // For an implementation of Number LongAdder tasks = new LongAdder (); PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( tasks ); // Or using a lambda ThreadPoolExecutor executor = ... PolledMeter . using ( registry ) . withName ( \"pool.completedTasks\" ) . monitorMonotonicCounter ( executor , ThreadPoolExecutor :: getCompletedTaskCount ); For thread pools specifically, there are better options for getting standard metrics. See the docs for the Thread Pools extension for more information.","title":"Monotonic Counters"},{"location":"spectator/lang/java/meters/gauge/#active-gauges","text":"Gauges can also be set directly by the user. In this mode, the user is responsible for regularly updating the value of the Gauge by calling set . Looking at the HttpServer example, with an active gauge, it would look like: class HttpServer { // Tracks the number of current connections to the server private AtomicInteger numConnections ; private Gauge gauge ; public HttpServer ( Registry registry ) { numConnections = new AtomicInteger (); gauge = registry . gauge ( \"server.numConnections\" ); gauge . set ( numConnections . get ()); } public void onConnectionCreated () { numConnections . incrementAndGet (); gauge . set ( numConnections . get ()); ... } public void onConnectionClosed () { numConnections . decrementAndGet (); gauge . set ( numConnections . get ()); ... } ... }","title":"Active Gauges"},{"location":"spectator/lang/java/meters/percentile-timer/","text":"Java Percentile Timers \u00b6 TBD","title":"Percentile Timers"},{"location":"spectator/lang/java/meters/percentile-timer/#java-percentile-timers","text":"TBD","title":"Java Percentile Timers"},{"location":"spectator/lang/java/meters/timer/","text":"Java Timers \u00b6 Timer \u00b6 To get started, create an instance using the Registry : public class Server { private final Registry registry ; private final Timer requestLatency ; @Inject public Server ( Registry registry ) { this . registry = registry ; requestLatency = registry . timer ( \"server.requestLatency\" ); } Then wrap the call you need to measure, preferably using a lambda: public Response handle ( Request request ) { return requestLatency . record (() -> handleImpl ( request )); } The lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like: public Response handle ( Request request ) { final long start = registry . clock (). monotonicTime (); try { return handleImpl ( request ); } finally { final long end = registry . clock (). monotonicTime (); requestLatency . record ( end - start , TimeUnit . NANOSECONDS ); } } This example uses the Clock from the Registry, which can be useful for testing, if you need to control the timing. In actual usage, it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times, to avoid occasionally having bogus measurements due to time adjustments. For more information, see the Clock documentation . LongTaskTimer \u00b6 To get started, create an instance using the Registry : import com.netflix.spectator.api.patterns.LongTaskTimer ; public class MetadataService { private final LongTaskTimer metadataRefresh ; @Inject public MetadataService ( Registry registry ) { metadataRefresh = LongTaskTimer . get ( registry , registry . createId ( \"metadata.refreshDuration\" )); // setup background thread to call refresh() } private void refresh () { final int id = metadataRefresh . start (); try { refreshImpl (); } finally { metadataRefresh . stop ( id ); } } The id value returned by the start method is used to keep track of a particular task being measured by the Timer. It must be stopped using the provided id. Note that unlike a regular Timer that does not do anything until the final duration is recorded, a long duration Timer will report as two Gauges: duration : total duration spent within all currently running tasks. activeTasks : number of currently running tasks. This means that you can see what is happening while the task is running, but you need to keep in mind: The meter id is fixed before the task begins. There is no way to change tags based on the run, e.g., update a different Timer, if an exception is thrown. Being a Gauge, it is inappropriate for short tasks. In particular, Gauges are sampled and if it is not sampled during the execution, or the sampling period is a significant subset of the expected duration, then the duration value will not be meaningful. Like a regular Timer, the duration Timer also supports using a lambda to simplify the common case: private void refresh () { metadataRefresh . record ( this :: refreshImpl ); }","title":"Timers"},{"location":"spectator/lang/java/meters/timer/#java-timers","text":"","title":"Java Timers"},{"location":"spectator/lang/java/meters/timer/#timer","text":"To get started, create an instance using the Registry : public class Server { private final Registry registry ; private final Timer requestLatency ; @Inject public Server ( Registry registry ) { this . registry = registry ; requestLatency = registry . timer ( \"server.requestLatency\" ); } Then wrap the call you need to measure, preferably using a lambda: public Response handle ( Request request ) { return requestLatency . record (() -> handleImpl ( request )); } The lambda variants will handle exceptions for you and ensure the record happens as part of a finally block using the monotonic time. It could also have been done more explicitly like: public Response handle ( Request request ) { final long start = registry . clock (). monotonicTime (); try { return handleImpl ( request ); } finally { final long end = registry . clock (). monotonicTime (); requestLatency . record ( end - start , TimeUnit . NANOSECONDS ); } } This example uses the Clock from the Registry, which can be useful for testing, if you need to control the timing. In actual usage, it will typically get mapped to the system clock. It is recommended to use a monotonically increasing source for measuring the times, to avoid occasionally having bogus measurements due to time adjustments. For more information, see the Clock documentation .","title":"Timer"},{"location":"spectator/lang/java/meters/timer/#longtasktimer","text":"To get started, create an instance using the Registry : import com.netflix.spectator.api.patterns.LongTaskTimer ; public class MetadataService { private final LongTaskTimer metadataRefresh ; @Inject public MetadataService ( Registry registry ) { metadataRefresh = LongTaskTimer . get ( registry , registry . createId ( \"metadata.refreshDuration\" )); // setup background thread to call refresh() } private void refresh () { final int id = metadataRefresh . start (); try { refreshImpl (); } finally { metadataRefresh . stop ( id ); } } The id value returned by the start method is used to keep track of a particular task being measured by the Timer. It must be stopped using the provided id. Note that unlike a regular Timer that does not do anything until the final duration is recorded, a long duration Timer will report as two Gauges: duration : total duration spent within all currently running tasks. activeTasks : number of currently running tasks. This means that you can see what is happening while the task is running, but you need to keep in mind: The meter id is fixed before the task begins. There is no way to change tags based on the run, e.g., update a different Timer, if an exception is thrown. Being a Gauge, it is inappropriate for short tasks. In particular, Gauges are sampled and if it is not sampled during the execution, or the sampling period is a significant subset of the expected duration, then the duration value will not be meaningful. Like a regular Timer, the duration Timer also supports using a lambda to simplify the common case: private void refresh () { metadataRefresh . record ( this :: refreshImpl ); }","title":"LongTaskTimer"},{"location":"spectator/lang/java/registry/metrics3/","text":"Metrics3 Registry \u00b6 Registry that uses metrics3 as the underlying implementation. To use the metrics registry, add a dependency on the spectator-reg-metrics3 library. For gradle: com.netflix.spectator:spectator-reg-metrics3:0.101.0 Then when initializing the application, use the MetricsRegistry . For more information see the metrics3 example .","title":"Metrics3"},{"location":"spectator/lang/java/registry/metrics3/#metrics3-registry","text":"Registry that uses metrics3 as the underlying implementation. To use the metrics registry, add a dependency on the spectator-reg-metrics3 library. For gradle: com.netflix.spectator:spectator-reg-metrics3:0.101.0 Then when initializing the application, use the MetricsRegistry . For more information see the metrics3 example .","title":"Metrics3 Registry"},{"location":"spectator/lang/java/registry/overview/","text":"Registry \u00b6 The Registry is the main class for managing a set of meters. A Meter is a class for collecting a set of measurements about your application. Choose Implementation \u00b6 The core Spectator library, spectator-api , comes with the following Registry implementations: Class Dependency Description DefaultRegistry spectator-api Updates local counters, frequently used with unit tests . NoopRegistry spectator-api Does nothing, tries to make operations as cheap as possible. This implementation is typically used to help understand the overhead being created due to instrumentation. It can also be useful in testing to help ensure that no side effects were introduced where the instrumentation is now needed in order for the application for function properly. MetricsRegistry spectator-reg-metrics3 Map to metrics3 library . This implementation is typically used for reporting to local files, JMX, or other backends like Graphite. Note that it uses a hierarchical naming scheme rather than the dimensional naming used by Spectator, so the names will get flattened when mapped to this Registry. It is recommended for libraries to write code against the Registry interface and allow the implementation to get injected by the user of the library. The simplest way is to accept the Registry via the constructor, for example: public class HttpServer { public HttpServer ( Registry registry ) { // use registry to collect measurements } } The user of the class can then provide the implementation: Registry registry = new DefaultRegistry (); HttpServer server = new HttpServer ( registry ); More complete examples can be found on the testing page or in the spectator-examples repo . Working with Ids \u00b6 Spectator is primarily intended for collecting data for dimensional time series backends like Atlas . The ids used for looking up a Meter in the Registry consist of a name and set of tags. Ids will be consumed many times by users after the data has been reported, so they should be chosen with some care and thought about how they will get used. See the conventions page for some general guidelines. Ids are created via the Registry, for example: Id id = registry . createId ( \"server.requestCount\" ); The ids are immutable, so they can be freely passed around and used in a concurrent context. Tags can be added when an id is created: Id id = registry . createId ( \"server.requestCount\" , \"status\" , \"2xx\" , \"method\" , \"GET\" ); Or by using withTag and withTags on an existing id: public class HttpServer { private final Id baseId ; public HttpServer ( Registry registry ) { baseId = registry . createId ( \"server.requestCount\" ); } private void handleRequestComplete ( HttpRequest req , HttpResponse res ) { // Remember Id is immutable, withTags will return a copy with the // the additional metadata Id reqId = baseId . withTags ( \"status\" , res . getStatus (), \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } private void handleRequestError ( HttpRequest req , Throwable t ) { // Can also be added individually using `withTag`. However, it is better // for performance to batch modifications using `withTags`. Id reqId = baseId . withTag ( \"error\" , t . getClass (). getSimpleName ()) . withTag ( \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } } Collecting Measurements \u00b6 Once you have an id, the Registry can be used to get an instance of a Meter to record a measurement. Meters can roughly be categorized in two groups: Active \u00b6 Active Meters are ones that are called directly when some event occurs. There are three basic types supported: Counters measure how often something is occurring. This will be reported to backend systems as a rate-per-second. For example, the number of requests processed by a web server. Timers measure how long something took. For example, the latency of requests processed by a web server. Distribution Summaries measure the size of something. For example, the entity sizes for requests processed by a web server. Passive \u00b6 Passive Meters are ones where the Registry has a reference to get the value when needed. For example, the number of current connections on a web server or the number threads that are currently in use. These will be Gauges . Global Registry \u00b6 There are some use-cases where injecting the Registry is not possible or is too cumbersome. The main example from the core Spectator libraries is the log4j appender . The Global Registry is useful there because logging is often initialized before any other systems and Spectator itself uses logging via the slf4j api which is quite likely being bound to log4j when that the appender is being used. By using the Global Registry, the logging initialization can proceed before the Spectator initialization in the application. Though any measurements taken before a Registry instance has been added will be lost. The Global Registry is accessed using: Registry registry = Spectator . globalRegistry (); By default, it will not record anything. For a specific registry instance you can choose to configure it to work with the Global Registry by calling add : public void init () { Registry registry = // Choose an appropriate implementation // Add it to the global registry so it will receive // any activity on the global registry Spectator . globalRegistry (). add ( registry ); } Any measurements taken while no Registries are added to the global instance will be lost. If multiple Registries are added, all will receive updates made to the Global Registry.","title":"Overview"},{"location":"spectator/lang/java/registry/overview/#registry","text":"The Registry is the main class for managing a set of meters. A Meter is a class for collecting a set of measurements about your application.","title":"Registry"},{"location":"spectator/lang/java/registry/overview/#choose-implementation","text":"The core Spectator library, spectator-api , comes with the following Registry implementations: Class Dependency Description DefaultRegistry spectator-api Updates local counters, frequently used with unit tests . NoopRegistry spectator-api Does nothing, tries to make operations as cheap as possible. This implementation is typically used to help understand the overhead being created due to instrumentation. It can also be useful in testing to help ensure that no side effects were introduced where the instrumentation is now needed in order for the application for function properly. MetricsRegistry spectator-reg-metrics3 Map to metrics3 library . This implementation is typically used for reporting to local files, JMX, or other backends like Graphite. Note that it uses a hierarchical naming scheme rather than the dimensional naming used by Spectator, so the names will get flattened when mapped to this Registry. It is recommended for libraries to write code against the Registry interface and allow the implementation to get injected by the user of the library. The simplest way is to accept the Registry via the constructor, for example: public class HttpServer { public HttpServer ( Registry registry ) { // use registry to collect measurements } } The user of the class can then provide the implementation: Registry registry = new DefaultRegistry (); HttpServer server = new HttpServer ( registry ); More complete examples can be found on the testing page or in the spectator-examples repo .","title":"Choose Implementation"},{"location":"spectator/lang/java/registry/overview/#working-with-ids","text":"Spectator is primarily intended for collecting data for dimensional time series backends like Atlas . The ids used for looking up a Meter in the Registry consist of a name and set of tags. Ids will be consumed many times by users after the data has been reported, so they should be chosen with some care and thought about how they will get used. See the conventions page for some general guidelines. Ids are created via the Registry, for example: Id id = registry . createId ( \"server.requestCount\" ); The ids are immutable, so they can be freely passed around and used in a concurrent context. Tags can be added when an id is created: Id id = registry . createId ( \"server.requestCount\" , \"status\" , \"2xx\" , \"method\" , \"GET\" ); Or by using withTag and withTags on an existing id: public class HttpServer { private final Id baseId ; public HttpServer ( Registry registry ) { baseId = registry . createId ( \"server.requestCount\" ); } private void handleRequestComplete ( HttpRequest req , HttpResponse res ) { // Remember Id is immutable, withTags will return a copy with the // the additional metadata Id reqId = baseId . withTags ( \"status\" , res . getStatus (), \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } private void handleRequestError ( HttpRequest req , Throwable t ) { // Can also be added individually using `withTag`. However, it is better // for performance to batch modifications using `withTags`. Id reqId = baseId . withTag ( \"error\" , t . getClass (). getSimpleName ()) . withTag ( \"method\" , req . getMethod (). name ()); registry . counter ( reqId ). increment (); } }","title":"Working with Ids"},{"location":"spectator/lang/java/registry/overview/#collecting-measurements","text":"Once you have an id, the Registry can be used to get an instance of a Meter to record a measurement. Meters can roughly be categorized in two groups:","title":"Collecting Measurements"},{"location":"spectator/lang/java/registry/overview/#active","text":"Active Meters are ones that are called directly when some event occurs. There are three basic types supported: Counters measure how often something is occurring. This will be reported to backend systems as a rate-per-second. For example, the number of requests processed by a web server. Timers measure how long something took. For example, the latency of requests processed by a web server. Distribution Summaries measure the size of something. For example, the entity sizes for requests processed by a web server.","title":"Active"},{"location":"spectator/lang/java/registry/overview/#passive","text":"Passive Meters are ones where the Registry has a reference to get the value when needed. For example, the number of current connections on a web server or the number threads that are currently in use. These will be Gauges .","title":"Passive"},{"location":"spectator/lang/java/registry/overview/#global-registry","text":"There are some use-cases where injecting the Registry is not possible or is too cumbersome. The main example from the core Spectator libraries is the log4j appender . The Global Registry is useful there because logging is often initialized before any other systems and Spectator itself uses logging via the slf4j api which is quite likely being bound to log4j when that the appender is being used. By using the Global Registry, the logging initialization can proceed before the Spectator initialization in the application. Though any measurements taken before a Registry instance has been added will be lost. The Global Registry is accessed using: Registry registry = Spectator . globalRegistry (); By default, it will not record anything. For a specific registry instance you can choose to configure it to work with the Global Registry by calling add : public void init () { Registry registry = // Choose an appropriate implementation // Add it to the global registry so it will receive // any activity on the global registry Spectator . globalRegistry (). add ( registry ); } Any measurements taken while no Registries are added to the global instance will be lost. If multiple Registries are added, all will receive updates made to the Global Registry.","title":"Global Registry"},{"location":"spectator/lang/nodejs/usage/","text":"Project \u00b6 spectator-js \u00b6 Source NPM Product Lifecycle: GA Module Name: nflx-spectator This module can be used to instrument an application using counters, distribution summaries, gauges, long task timers, timers, and more complex meter types (like Bucket or Percentile Timers) using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator. spectator-js-nodejsmetrics \u00b6 Source NPM Product Lifecycle: GA Module Name: nflx-spectator-nodejsmetrics Generate Node.js runtime metrics using the spectator-js Node module. Install Libraries \u00b6 Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" } } Instrumenting Code \u00b6 'use strict' ; const spectator = require ( 'nflx-spectator' ); // Netflix applications can use the nflx-spectator-config node module available // internally through artifactory to generate the config required by nflx-spectator function getConfig () { return { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' , timeout : 1000 // milliseconds } } class Response { constructor ( status , size ) { this . status = status ; this . size = size ; } } class Server { constructor ( registry ) { this . registry = registry ; // create a base Id, to which we'll add some dynamic tags later this . requestCountId = registry . createId ( 'server.requestCount' , { version : 'v1' }); this . requestLatency = registry . timer ( 'server.requestLatency' ); this . responseSize = registry . distributionSummary ( 'server.responseSizes' ); } handle ( request ) { const start = this . registry . hrtime (); // do some work based on request and obtain a response const res = new Response ( 200 , 64 ); // update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as a lookup of an id object in a map // However, it is more expensive than having a local variable set // to the counter const counterId = this . requestCountId . withTags ({ country : request . country , status : res . status }); this . registry . counter ( counterId ). increment (); this . requestLatency . record ( this . registry . hrtime ( start )); this . responseSize . record ( res . size ); return res ; } } const config = getConfig (); const registry = new spectator . Registry ( config ); class Request { constructor ( country ) { this . country = country ; } } // somehow get a request from the user... function getNextRequest () { return new Request ( 'AR' ); } function handleTermination () { registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); registry . start (); const server = new Server ( registry ); for ( let i = 0 ; i < 3 ; ++ i ) { const req = getNextRequest (); server . handle ( req ) } registry . stop (); Enable Runtime Metrics \u00b6 'use strict' ; function getConfig () { } const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const config = { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' }; const registry = new spectator . Registry ( config ); registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); // start collecting nodejs metrics // ... metrics . stop (); registry . stop (); Netflix Integration \u00b6 Create a Netflix Spectator Config to be used by spectator-js . Only applications should depend on the nflx-spectator-jsconf package. Libraries should get the Registry passed by the application, and therefore should only need to depend on spectator-js . Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" , \"nflx-spectator-jsconf\" : \"*\" } } This configuration also brings in spectator-js-nodejsmetrics to provide Node.js runtime metrics. You can override the logger used by the Spectator registry by setting the logger property. The specified logger should provide debug , info , and error methods. By default, spectator-js logs to stdout. const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const getSpectatorConfig = require ( 'nflx-spectator-jsconf' ); const logger = require ( 'pino' )(); //... const registry = new spectator . Registry ( getSpectatorConfig ()); registry . logger = logger ; registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); function handleTermination () { metrics . stop (); registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); //... your app handleTermination ();","title":"Usage"},{"location":"spectator/lang/nodejs/usage/#project","text":"","title":"Project"},{"location":"spectator/lang/nodejs/usage/#spectator-js","text":"Source NPM Product Lifecycle: GA Module Name: nflx-spectator This module can be used to instrument an application using counters, distribution summaries, gauges, long task timers, timers, and more complex meter types (like Bucket or Percentile Timers) using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator.","title":"spectator-js"},{"location":"spectator/lang/nodejs/usage/#spectator-js-nodejsmetrics","text":"Source NPM Product Lifecycle: GA Module Name: nflx-spectator-nodejsmetrics Generate Node.js runtime metrics using the spectator-js Node module.","title":"spectator-js-nodejsmetrics"},{"location":"spectator/lang/nodejs/usage/#install-libraries","text":"Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" } }","title":"Install Libraries"},{"location":"spectator/lang/nodejs/usage/#instrumenting-code","text":"'use strict' ; const spectator = require ( 'nflx-spectator' ); // Netflix applications can use the nflx-spectator-config node module available // internally through artifactory to generate the config required by nflx-spectator function getConfig () { return { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' , timeout : 1000 // milliseconds } } class Response { constructor ( status , size ) { this . status = status ; this . size = size ; } } class Server { constructor ( registry ) { this . registry = registry ; // create a base Id, to which we'll add some dynamic tags later this . requestCountId = registry . createId ( 'server.requestCount' , { version : 'v1' }); this . requestLatency = registry . timer ( 'server.requestLatency' ); this . responseSize = registry . distributionSummary ( 'server.responseSizes' ); } handle ( request ) { const start = this . registry . hrtime (); // do some work based on request and obtain a response const res = new Response ( 200 , 64 ); // update the counter id with dimensions based on the request. The // counter will then be looked up in the registry which should be // fairly cheap, such as a lookup of an id object in a map // However, it is more expensive than having a local variable set // to the counter const counterId = this . requestCountId . withTags ({ country : request . country , status : res . status }); this . registry . counter ( counterId ). increment (); this . requestLatency . record ( this . registry . hrtime ( start )); this . responseSize . record ( res . size ); return res ; } } const config = getConfig (); const registry = new spectator . Registry ( config ); class Request { constructor ( country ) { this . country = country ; } } // somehow get a request from the user... function getNextRequest () { return new Request ( 'AR' ); } function handleTermination () { registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); registry . start (); const server = new Server ( registry ); for ( let i = 0 ; i < 3 ; ++ i ) { const req = getNextRequest (); server . handle ( req ) } registry . stop ();","title":"Instrumenting Code"},{"location":"spectator/lang/nodejs/usage/#enable-runtime-metrics","text":"'use strict' ; function getConfig () { } const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const config = { commonTags : { 'nf.node' : 'i-1234' }, uri : 'http://atlas.example.org/v1/publish' }; const registry = new spectator . Registry ( config ); registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); // start collecting nodejs metrics // ... metrics . stop (); registry . stop ();","title":"Enable Runtime Metrics"},{"location":"spectator/lang/nodejs/usage/#netflix-integration","text":"Create a Netflix Spectator Config to be used by spectator-js . Only applications should depend on the nflx-spectator-jsconf package. Libraries should get the Registry passed by the application, and therefore should only need to depend on spectator-js . Add the following dependencies to package.json : { \"dependencies\" : { \"nflx-spectator\" : \"*\" , \"nflx-spectator-nodejsmetrics\" : \"*\" , \"nflx-spectator-jsconf\" : \"*\" } } This configuration also brings in spectator-js-nodejsmetrics to provide Node.js runtime metrics. You can override the logger used by the Spectator registry by setting the logger property. The specified logger should provide debug , info , and error methods. By default, spectator-js logs to stdout. const spectator = require ( 'nflx-spectator' ); const NodeMetrics = require ( 'nflx-spectator-nodejsmetrics' ); const getSpectatorConfig = require ( 'nflx-spectator-jsconf' ); const logger = require ( 'pino' )(); //... const registry = new spectator . Registry ( getSpectatorConfig ()); registry . logger = logger ; registry . start (); const metrics = new NodeMetrics ( registry ); metrics . start (); function handleTermination () { metrics . stop (); registry . stop (); } process . on ( 'SIGINT' , handleTermination ); process . on ( 'SIGTERM' , handleTermination ); //... your app handleTermination ();","title":"Netflix Integration"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/","text":"Node.js runtime CPU metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.cpuUsage \u00b6 Percentage of CPU time the Node.js process is consuming, from 0..100. The usage is divided into the following categories: system : CPU time spent running the kernel. user : CPU time spent running user space (non-kernel) processes. Unit: percent Dimensions: id : The category of CPU usage. Example: { \"tags\" : { \"id\" : \"system\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 0.8954088417692685 }, { \"tags\" : { \"id\" : \"user\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 4.659007745141895 }","title":"CPU"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-cpu/#nodejscpuusage","text":"Percentage of CPU time the Node.js process is consuming, from 0..100. The usage is divided into the following categories: system : CPU time spent running the kernel. user : CPU time spent running user space (non-kernel) processes. Unit: percent Dimensions: id : The category of CPU usage. Example: { \"tags\" : { \"id\" : \"system\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 0.8954088417692685 }, { \"tags\" : { \"id\" : \"user\" , \"name\" : \"nodejs.cpuUsage\" , /// nf .* ta gs \"nodejs.version\" : \"v6.5.0\" }, \"start\" : 1485813720000 , \"value\" : 4.659007745141895 }","title":"nodejs.cpuUsage"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/","text":"Node.js runtime event loop metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.eventLoop \u00b6 The time it takes for the event loop to complete. This is sampled twice per second. Unit: seconds nodejs.eventLoopLag \u00b6 The time that the event loop is running behind, as measured by attempting to execute a timer once per second. Unit: seconds","title":"Event Loop"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#nodejseventloop","text":"The time it takes for the event loop to complete. This is sampled twice per second. Unit: seconds","title":"nodejs.eventLoop"},{"location":"spectator/lang/nodejs/ext/nodejs-eventloop/#nodejseventlooplag","text":"The time that the event loop is running behind, as measured by attempting to execute a timer once per second. Unit: seconds","title":"nodejs.eventLoopLag"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/","text":"Node.js runtime file descriptor metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. openFileDescriptorsCount \u00b6 Number of file descriptors currently open. Unit: file descriptors maxFileDescriptorsCount \u00b6 The maximum number of file descriptors that can be open at the same time. Unit: file descriptors","title":"File Descriptor"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#openfiledescriptorscount","text":"Number of file descriptors currently open. Unit: file descriptors","title":"openFileDescriptorsCount"},{"location":"spectator/lang/nodejs/ext/nodejs-filedescriptor/#maxfiledescriptorscount","text":"The maximum number of file descriptors that can be open at the same time. Unit: file descriptors","title":"maxFileDescriptorsCount"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/","text":"Node.js runtime garbage collection metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.gc.allocationRate \u00b6 The rate at which the app is allocating memory. Unit: bytes/second nodejs.gc.liveDataSize \u00b6 The size of the old_space after a major GC event. Unit: bytes nodejs.gc.maxDataSize \u00b6 The maximum amount of memory the nodejs process is allowed to use. This is primarily used for gaining perspective on the liveDataSize . Unit: bytes nodejs.gc.pause \u00b6 The time it takes to complete different GC events. Event categories: scavenge : The most common garbage collection method. Node will typically trigger one of these every time the VM is idle. markSweepCompact : The heaviest type of garbage collection V8 may do. If you see many of these happening you will need to either keep fewer objects around in your process or increase V8's heap limit. incrementalMarking : A phased garbage collection that interleaves collection with application logic to reduce the amount of time the application is paused. processWeakCallbacks : After a garbage collection occurs, V8 will call any weak reference callbacks registered for objects that have been freed. This measurement is from the start of the first weak callback to the end of the last for a given garbage collection. Unit: seconds Dimensions: id : The GC event category. nodejs.gc.promotionRate \u00b6 The rate at which data is being moved from new_space to old_space . Unit: bytes/second","title":"Garbarge Collection"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcallocationrate","text":"The rate at which the app is allocating memory. Unit: bytes/second","title":"nodejs.gc.allocationRate"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgclivedatasize","text":"The size of the old_space after a major GC event. Unit: bytes","title":"nodejs.gc.liveDataSize"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcmaxdatasize","text":"The maximum amount of memory the nodejs process is allowed to use. This is primarily used for gaining perspective on the liveDataSize . Unit: bytes","title":"nodejs.gc.maxDataSize"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcpause","text":"The time it takes to complete different GC events. Event categories: scavenge : The most common garbage collection method. Node will typically trigger one of these every time the VM is idle. markSweepCompact : The heaviest type of garbage collection V8 may do. If you see many of these happening you will need to either keep fewer objects around in your process or increase V8's heap limit. incrementalMarking : A phased garbage collection that interleaves collection with application logic to reduce the amount of time the application is paused. processWeakCallbacks : After a garbage collection occurs, V8 will call any weak reference callbacks registered for objects that have been freed. This measurement is from the start of the first weak callback to the end of the last for a given garbage collection. Unit: seconds Dimensions: id : The GC event category.","title":"nodejs.gc.pause"},{"location":"spectator/lang/nodejs/ext/nodejs-gc/#nodejsgcpromotionrate","text":"The rate at which data is being moved from new_space to old_space . Unit: bytes/second","title":"nodejs.gc.promotionRate"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/","text":"Node.js runtime heap metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Data is gathered from the v8.getHeapStatistics method. Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.doesZapGarbage \u00b6 Whether or not the --zap_code_space option is enabled. This makes V8 overwrite heap garbage with a bit pattern. The RSS footprint (resident memory set) gets bigger because it continuously touches all heap pages and that makes them less likely to get swapped out by the operating system. Unit: boolean nodejs.heapSizeLimit \u00b6 The absolute limit the heap cannot exceed (default limit or --max_old_space_size ). Unit: bytes nodejs.mallocedMemory \u00b6 Current amount of memory, obtained via malloc . Unit: bytes nodejs.peakMallocedMemory \u00b6 Peak amount of memory, obtained via malloc . Unit: bytes nodejs.totalAvailableSize \u00b6 Available heap size. Unit: bytes nodejs.totalHeapSize \u00b6 Memory V8 has allocated for the heap. This can grow if usedHeap needs more. Unit: bytes nodejs.totalHeapSizeExecutable \u00b6 Memory for compiled bytecode and JITed code. Unit: bytes nodejs.totalPhysicalSize \u00b6 Committed size. Unit: bytes nodejs.usedHeapSize \u00b6 Memory used by application data. Unit: bytes","title":"Heap"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#metrics","text":"Data is gathered from the v8.getHeapStatistics method.","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsdoeszapgarbage","text":"Whether or not the --zap_code_space option is enabled. This makes V8 overwrite heap garbage with a bit pattern. The RSS footprint (resident memory set) gets bigger because it continuously touches all heap pages and that makes them less likely to get swapped out by the operating system. Unit: boolean","title":"nodejs.doesZapGarbage"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsheapsizelimit","text":"The absolute limit the heap cannot exceed (default limit or --max_old_space_size ). Unit: bytes","title":"nodejs.heapSizeLimit"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsmallocedmemory","text":"Current amount of memory, obtained via malloc . Unit: bytes","title":"nodejs.mallocedMemory"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejspeakmallocedmemory","text":"Peak amount of memory, obtained via malloc . Unit: bytes","title":"nodejs.peakMallocedMemory"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalavailablesize","text":"Available heap size. Unit: bytes","title":"nodejs.totalAvailableSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalheapsize","text":"Memory V8 has allocated for the heap. This can grow if usedHeap needs more. Unit: bytes","title":"nodejs.totalHeapSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalheapsizeexecutable","text":"Memory for compiled bytecode and JITed code. Unit: bytes","title":"nodejs.totalHeapSizeExecutable"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejstotalphysicalsize","text":"Committed size. Unit: bytes","title":"nodejs.totalPhysicalSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heap/#nodejsusedheapsize","text":"Memory used by application data. Unit: bytes","title":"nodejs.usedHeapSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/","text":"Node.js runtime heap space metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Data is gathered from the v8.getHeapSpaceStatistics method, for each space listed. Space categories: new_space : Where new allocations happen; it is fast to allocate and collect garbage here. Objects living in the New Space are called the Young Generation. old_space : Object that survived the New Space collector are promoted here; they are called the Old Generation. Allocation in the Old Space is fast, but collection is expensive so it is less frequently performed. code_space : Contains executable code and therefore is marked executable. map_space : Contains map objects only. large_object_space : Contains promoted large objects which exceed the size limits of other spaces. Each object gets its own mmap region of memory and these objects are never moved by GC. Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.spaceSize \u00b6 The allocated size of the space. Unit: bytes Dimensions: id : Space category. nodejs.spaceUsedSize \u00b6 The used size of the space. Unit: bytes Dimensions: id : Space category. nodejs.spaceAvailableSize \u00b6 The available size of the space. Unit: bytes Dimensions: id : Space category. nodejs.physicalSpaceSize \u00b6 The physical size of the space. Unit: bytes Dimensions: id : Space category.","title":"Heap Space"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#metrics","text":"Data is gathered from the v8.getHeapSpaceStatistics method, for each space listed. Space categories: new_space : Where new allocations happen; it is fast to allocate and collect garbage here. Objects living in the New Space are called the Young Generation. old_space : Object that survived the New Space collector are promoted here; they are called the Old Generation. Allocation in the Old Space is fast, but collection is expensive so it is less frequently performed. code_space : Contains executable code and therefore is marked executable. map_space : Contains map objects only. large_object_space : Contains promoted large objects which exceed the size limits of other spaces. Each object gets its own mmap region of memory and these objects are never moved by GC.","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspacesize","text":"The allocated size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspaceusedsize","text":"The used size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceUsedSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsspaceavailablesize","text":"The available size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.spaceAvailableSize"},{"location":"spectator/lang/nodejs/ext/nodejs-heapspace/#nodejsphysicalspacesize","text":"The physical size of the space. Unit: bytes Dimensions: id : Space category.","title":"nodejs.physicalSpaceSize"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/","text":"Node.js runtime memory metrics, provided by spectator-js-nodejsmetrics . Metrics \u00b6 Common Dimensions \u00b6 The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime. nodejs.rss \u00b6 Resident Set Size, which is the total memory allocated for the process execution. This includes the Code Segment, Stack (local variables and pointers) and Heap (objects and closures). Unit: bytes nodejs.heapTotal \u00b6 Total size of the allocated heap. Unit: bytes nodejs.heapUsed \u00b6 Memory used during the execution of our process. Unit: bytes nodejs.external \u00b6 Memory usage of C++ objects bound to JavaScript objects managed by V8. Unit: bytes","title":"Memory"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#metrics","text":"","title":"Metrics"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#common-dimensions","text":"The following dimensions are common to the metrics published by this module: nodejs.version : The version of the Node.js runtime.","title":"Common Dimensions"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsrss","text":"Resident Set Size, which is the total memory allocated for the process execution. This includes the Code Segment, Stack (local variables and pointers) and Heap (objects and closures). Unit: bytes","title":"nodejs.rss"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsheaptotal","text":"Total size of the allocated heap. Unit: bytes","title":"nodejs.heapTotal"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsheapused","text":"Memory used during the execution of our process. Unit: bytes","title":"nodejs.heapUsed"},{"location":"spectator/lang/nodejs/ext/nodejs-memory/#nodejsexternal","text":"Memory usage of C++ objects bound to JavaScript objects managed by V8. Unit: bytes","title":"nodejs.external"},{"location":"spectator/lang/nodejs/meters/counter/","text":"TBD","title":"Counters"},{"location":"spectator/lang/nodejs/meters/dist-summary/","text":"TBD","title":"Distribution Summaries"},{"location":"spectator/lang/nodejs/meters/gauge/","text":"TBD","title":"Gauges"},{"location":"spectator/lang/nodejs/meters/percentile-timer/","text":"TBD","title":"Percentile Timers"},{"location":"spectator/lang/nodejs/meters/timer/","text":"TBD","title":"Timers"},{"location":"spectator/lang/py/usage/","text":"Project \u00b6 Source PyPI Product Lifecycle: Beta Module Name: netflix-spectator-py This module can be used to instrument an application using Counters, Distribution Summaries, Gauges, Timers, and Percentile Timers using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator. Install Library \u00b6 Install the library from PyPI: pip install netflix-spectator-py Instrumenting Code \u00b6 from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment () GlobalRegistry . gauge ( 'server.capacity' ) . set ( 50 ) Usage \u00b6 The import of the GlobalRegistry will start a daemon thread that will publish metrics in the background. The cache will be flushed upon normal interpreter termination using atexit , with the following exceptions: The program is killed by a signal not handled by Python. A Python fatal internal error is detected. When os._exit() is called. If you do not want the GlobalRegistry to automatically start at module import, then set the following environment variable: SPECTATOR_PY_DISABLE_AUTO_START_GLOBAL = 1 With this configuration, you will have to manually call the GlobalRegistry.start() and GlobalRegistry.stop() methods. Failure to do so will prevent metric publishing. Common Tagging \u00b6 This library does not add the nf.node tag to the published metrics. If you need it, remember to add it. Gunicorn Preload \u00b6 If you are using this library while running behind Gunicorn, make sure that you do not use the --preload flag, because it can cause issues with how the background thread operates due to loading the application code before the worker processes are forked. On the Paved Path, with EzConfig, that is achieved with the following configuration: WSGI_GUNICORN_PRELOAD = undef Netflix Integration \u00b6 Add the internal configuration for the Spectator Python client, so that it can send metrics to an Atlas Aggregator cluster. Replace SMARTIPROXY_HOSTNAME with the hostname of the internal SmartiProxy service. pip install -i https://SMARTIPROXY_HOSTNAME/pypi netflix-spectator-pyconf","title":"Usage"},{"location":"spectator/lang/py/usage/#project","text":"Source PyPI Product Lifecycle: Beta Module Name: netflix-spectator-py This module can be used to instrument an application using Counters, Distribution Summaries, Gauges, Timers, and Percentile Timers using a dimensional data model. The generated metrics are periodically sent to an Atlas Aggregator.","title":"Project"},{"location":"spectator/lang/py/usage/#install-library","text":"Install the library from PyPI: pip install netflix-spectator-py","title":"Install Library"},{"location":"spectator/lang/py/usage/#instrumenting-code","text":"from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment () GlobalRegistry . gauge ( 'server.capacity' ) . set ( 50 )","title":"Instrumenting Code"},{"location":"spectator/lang/py/usage/#usage","text":"The import of the GlobalRegistry will start a daemon thread that will publish metrics in the background. The cache will be flushed upon normal interpreter termination using atexit , with the following exceptions: The program is killed by a signal not handled by Python. A Python fatal internal error is detected. When os._exit() is called. If you do not want the GlobalRegistry to automatically start at module import, then set the following environment variable: SPECTATOR_PY_DISABLE_AUTO_START_GLOBAL = 1 With this configuration, you will have to manually call the GlobalRegistry.start() and GlobalRegistry.stop() methods. Failure to do so will prevent metric publishing.","title":"Usage"},{"location":"spectator/lang/py/usage/#common-tagging","text":"This library does not add the nf.node tag to the published metrics. If you need it, remember to add it.","title":"Common Tagging"},{"location":"spectator/lang/py/usage/#gunicorn-preload","text":"If you are using this library while running behind Gunicorn, make sure that you do not use the --preload flag, because it can cause issues with how the background thread operates due to loading the application code before the worker processes are forked. On the Paved Path, with EzConfig, that is achieved with the following configuration: WSGI_GUNICORN_PRELOAD = undef","title":"Gunicorn Preload"},{"location":"spectator/lang/py/usage/#netflix-integration","text":"Add the internal configuration for the Spectator Python client, so that it can send metrics to an Atlas Aggregator cluster. Replace SMARTIPROXY_HOSTNAME with the hostname of the internal SmartiProxy service. pip install -i https://SMARTIPROXY_HOSTNAME/pypi netflix-spectator-pyconf","title":"Netflix Integration"},{"location":"spectator/lang/py/meters/counter/","text":"from spectator import GlobalRegistry GlobalRegistry . counter ( 'server.numRequests' ) . increment ()","title":"Counters"},{"location":"spectator/lang/py/meters/dist-summary/","text":"","title":"Distribution Summaries"},{"location":"spectator/lang/py/meters/gauge/","text":"","title":"Gauges"},{"location":"spectator/lang/py/meters/percentile-timer/","text":"","title":"Percentile Timers"},{"location":"spectator/lang/py/meters/timer/","text":"","title":"Timers"},{"location":"spectator/lang/rb/usage/","text":"Project \u00b6 Source RubyGems Product Lifecycle: Alpha Module Name: netflix-spectator-rb This implements a basic Spectator library for instrumenting Ruby applications, sending metrics to an Atlas aggregator service. Install Library \u00b6 Install the library from RubyGems: gem install spectator-rb Instrumenting Code \u00b6 require 'spectator' class Response attr_accessor :status , :size def initialize ( status , size ) @status = status @size = size end end class Request attr_reader :country def initialize ( country ) @country = country end end class ExampleServer def initialize ( registry ) @registry = registry @req_count_id = registry . new_id ( 'server.requestCount' ) @req_latency = registry . timer ( 'server.requestLatency' ) @resp_sizes = registry . distribution_summary ( 'server.responseSizes' ) end def expensive_computation ( request ) # ... end def handle_request ( request ) start = @registry . clock . monotonic_time # initialize response response = Response . new ( 200 , 64 ) # Update the counter id with dimensions based on the request. The # counter will then be looked up in the registry which should be # fairly cheap, such as lookup of id object in a map # However, it is more expensive than having a local variable set # to the counter. cnt_id = @req_count_id . with_tag ( :country , request . country ) . with_tag ( :status , response . status . to_s ) @registry . counter_with_id ( cnt_id ) . increment # ... @req_latency . record ( @registry . clock . monotonic_time - start ) @resp_sizes . record ( response . size ) # timers can also time a given block # this is equivalent to: # start = @registry.clock.monotonic_time # expensive_computation(request) # @registry.timer('server.computeTime').record(@registry.clock.monotonic_time - start) @registry . timer ( 'server.computeTime' ) . time { expensive_computation ( request ) } # ... end end config = { common_tags : { :'nf.app' => 'foo' }, frequency : 0 . 5 , uri : 'http://localhost:8080/api/v4/publish' } registry = Spectator :: Registry . new ( config ) registry . start server = ExampleServer . new ( registry ) # ... # process some requests requests = [ Request . new ( 'us' ), Request . new ( 'ar' ), Request . new ( 'ar' ) ] requests . each { | req | server . handle_request ( req ) } sleep ( 2 ) registry . stop Netflix Integration \u00b6 Add the internal configuration for the Spectator Ruby client, so that it can send metrics to an Atlas Aggregator cluster. If you are using the internal Artifactory, add the dependency to your Gemfile: gem 'netflix-spectator-config' gem 'netflix-spectator-rb' If you are not using the internal Artifactory, then you can do the following, replacing STASH_HOSTNAME_AND_PORT with appropriate values: gem 'netflix-spectator-config' , git : 'ssh://git@STASH_HOSTNAME_AND_PORT/cldmta/nflx-spectator-rb.git' gem 'netflix-spectator-rb' Once the configuration Gem is installed, it is used as follows: require 'spectator_config' require 'spectator' config = SpectatorConfig . config registry = Spectator :: Registry . new ( config ) registry . start # ... registry . stop","title":"Usage"},{"location":"spectator/lang/rb/usage/#project","text":"Source RubyGems Product Lifecycle: Alpha Module Name: netflix-spectator-rb This implements a basic Spectator library for instrumenting Ruby applications, sending metrics to an Atlas aggregator service.","title":"Project"},{"location":"spectator/lang/rb/usage/#install-library","text":"Install the library from RubyGems: gem install spectator-rb","title":"Install Library"},{"location":"spectator/lang/rb/usage/#instrumenting-code","text":"require 'spectator' class Response attr_accessor :status , :size def initialize ( status , size ) @status = status @size = size end end class Request attr_reader :country def initialize ( country ) @country = country end end class ExampleServer def initialize ( registry ) @registry = registry @req_count_id = registry . new_id ( 'server.requestCount' ) @req_latency = registry . timer ( 'server.requestLatency' ) @resp_sizes = registry . distribution_summary ( 'server.responseSizes' ) end def expensive_computation ( request ) # ... end def handle_request ( request ) start = @registry . clock . monotonic_time # initialize response response = Response . new ( 200 , 64 ) # Update the counter id with dimensions based on the request. The # counter will then be looked up in the registry which should be # fairly cheap, such as lookup of id object in a map # However, it is more expensive than having a local variable set # to the counter. cnt_id = @req_count_id . with_tag ( :country , request . country ) . with_tag ( :status , response . status . to_s ) @registry . counter_with_id ( cnt_id ) . increment # ... @req_latency . record ( @registry . clock . monotonic_time - start ) @resp_sizes . record ( response . size ) # timers can also time a given block # this is equivalent to: # start = @registry.clock.monotonic_time # expensive_computation(request) # @registry.timer('server.computeTime').record(@registry.clock.monotonic_time - start) @registry . timer ( 'server.computeTime' ) . time { expensive_computation ( request ) } # ... end end config = { common_tags : { :'nf.app' => 'foo' }, frequency : 0 . 5 , uri : 'http://localhost:8080/api/v4/publish' } registry = Spectator :: Registry . new ( config ) registry . start server = ExampleServer . new ( registry ) # ... # process some requests requests = [ Request . new ( 'us' ), Request . new ( 'ar' ), Request . new ( 'ar' ) ] requests . each { | req | server . handle_request ( req ) } sleep ( 2 ) registry . stop","title":"Instrumenting Code"},{"location":"spectator/lang/rb/usage/#netflix-integration","text":"Add the internal configuration for the Spectator Ruby client, so that it can send metrics to an Atlas Aggregator cluster. If you are using the internal Artifactory, add the dependency to your Gemfile: gem 'netflix-spectator-config' gem 'netflix-spectator-rb' If you are not using the internal Artifactory, then you can do the following, replacing STASH_HOSTNAME_AND_PORT with appropriate values: gem 'netflix-spectator-config' , git : 'ssh://git@STASH_HOSTNAME_AND_PORT/cldmta/nflx-spectator-rb.git' gem 'netflix-spectator-rb' Once the configuration Gem is installed, it is used as follows: require 'spectator_config' require 'spectator' config = SpectatorConfig . config registry = Spectator :: Registry . new ( config ) registry . start # ... registry . stop","title":"Netflix Integration"},{"location":"spectator/lang/rb/meters/counter/","text":"","title":"Counters"},{"location":"spectator/lang/rb/meters/dist-summary/","text":"","title":"Distribution Summaries"},{"location":"spectator/lang/rb/meters/gauge/","text":"","title":"Gauges"},{"location":"spectator/lang/rb/meters/percentile-timer/","text":"","title":"Percentile Timers"},{"location":"spectator/lang/rb/meters/timer/","text":"","title":"Timers"},{"location":"spectator/patterns/cardinality-limiter/","text":"Cardinality Limiter \u00b6 Helper functions to help manage the cardinality of tag values. This should be used anywhere you cannot guarantee that the tag values being used are strictly bounded. There is support for two different modes: (1) selecting the first N values that are seen, or (2) selecting the most frequent N values that are seen. Example usage: class WebServer { // Limiter instance, should be shared for all uses of that tag value private final Function & lt ; String , String & gt ; pathLimiter = CardinalityLimiters . mostFrequent ( 10 ); private final Registry registry ; private final Id baseId ; public WebServer ( Registry registry ) { this . registry = registry ; this . baseId = registry . createId ( \"server.requestCount\" ); } public Response handleRequest ( Request req ) { Response res = doSomething ( req ); // Update metrics, use limiter to restrict the set of values for the // path and avoid an explosion String pathValue = pathLimiter . apply ( req . getPath ()); Id id = baseId . withTag ( \"path\" , pathValue ) . withTag ( \"status\" , res . getStatus ()); registry . counter ( id ). increment (); } }","title":"Cardinality Limiter"},{"location":"spectator/patterns/cardinality-limiter/#cardinality-limiter","text":"Helper functions to help manage the cardinality of tag values. This should be used anywhere you cannot guarantee that the tag values being used are strictly bounded. There is support for two different modes: (1) selecting the first N values that are seen, or (2) selecting the most frequent N values that are seen. Example usage: class WebServer { // Limiter instance, should be shared for all uses of that tag value private final Function & lt ; String , String & gt ; pathLimiter = CardinalityLimiters . mostFrequent ( 10 ); private final Registry registry ; private final Id baseId ; public WebServer ( Registry registry ) { this . registry = registry ; this . baseId = registry . createId ( \"server.requestCount\" ); } public Response handleRequest ( Request req ) { Response res = doSomething ( req ); // Update metrics, use limiter to restrict the set of values for the // path and avoid an explosion String pathValue = pathLimiter . apply ( req . getPath ()); Id id = baseId . withTag ( \"path\" , pathValue ) . withTag ( \"status\" , res . getStatus ()); registry . counter ( id ). increment (); } }","title":"Cardinality Limiter"},{"location":"spectator/patterns/gauge-poller/","text":"Gauge Poller \u00b6 Helper for polling gauges in a background thread. A shared executor is used with a single thread. If registered gauge methods are cheap as they should be, then this should be plenty of capacity to process everything regularly. If not, then this will help limit the damage to a single core and avoid causing problems for the application.","title":"Gauge Poller"},{"location":"spectator/patterns/gauge-poller/#gauge-poller","text":"Helper for polling gauges in a background thread. A shared executor is used with a single thread. If registered gauge methods are cheap as they should be, then this should be plenty of capacity to process everything regularly. If not, then this will help limit the damage to a single core and avoid causing problems for the application.","title":"Gauge Poller"},{"location":"spectator/patterns/interval-counter/","text":"Interval Counter \u00b6 A counter that also keeps track of the time since last update.","title":"Interval Counter"},{"location":"spectator/patterns/interval-counter/#interval-counter","text":"A counter that also keeps track of the time since last update.","title":"Interval Counter"},{"location":"spectator/patterns/long-task-timer/","text":"Long Task Timer \u00b6 Timer intended to track a small number of long running tasks. Example would be something like a batch hadoop job. Though \"long running\" is a bit subjective the assumption is that anything over a minute is long running.","title":"Long Task Timer"},{"location":"spectator/patterns/long-task-timer/#long-task-timer","text":"Timer intended to track a small number of long running tasks. Example would be something like a batch hadoop job. Though \"long running\" is a bit subjective the assumption is that anything over a minute is long running.","title":"Long Task Timer"},{"location":"spectator/patterns/percentile-timer/","text":"Percentile Timers \u00b6 A Timer that buckets the counts, to allow for estimating percentiles. This Timer type will track the data distribution for the timer by maintaining a set of Counters. The distribution can then be used on the server side to estimate percentiles, while still allowing for arbitrary slicing and dicing based on dimensions. Percentile Timers are expensive compared to basic Timers from the Registry. In order to maintain the data distribution, they have a higher storage cost, with a worst-case of up to 300X that of a standard Timer. Be diligent about any additional dimensions added to Percentile Timers and ensure that they have a small bounded cardinality. In addition, it is highly recommended to set a range, whenever possible, to restrict the worst case overhead. When using the builder, the range will default from 10 ms to 1 minute. Based on data at Netflix, this is the most common range for request latencies and restricting to this window reduces the worst case multiple from 276X to 58X. Range Recommendations \u00b6 The range should be the SLA boundary or failure point for the activity. Explicitly setting the range allows us to optimize for the important range of values and reduce the overhead associated with tracking the data distribution. For example, suppose you are making a client call and timeout after 10 seconds. Setting the range to 10 seconds will restrict the possible set of buckets used to those approaching the boundary. So we can still detect if it is nearing failure, but percentiles that are further away from the range may be inflated compared to the actual value. Bucket Distribution \u00b6 The set of buckets is generated by using powers of 4 and incrementing by one-third of the previous power of 4 in between as long as the value is less than the next power of 4 minus the delta. Base: 1, 2, 3 4 (4^1), delta = 1 5, 6, 7, ..., 14, 16 (4^2), delta = 5 21, 26, 31, ..., 56, 64 (4^3), delta = 21 ...","title":"Percentile Timer"},{"location":"spectator/patterns/percentile-timer/#percentile-timers","text":"A Timer that buckets the counts, to allow for estimating percentiles. This Timer type will track the data distribution for the timer by maintaining a set of Counters. The distribution can then be used on the server side to estimate percentiles, while still allowing for arbitrary slicing and dicing based on dimensions. Percentile Timers are expensive compared to basic Timers from the Registry. In order to maintain the data distribution, they have a higher storage cost, with a worst-case of up to 300X that of a standard Timer. Be diligent about any additional dimensions added to Percentile Timers and ensure that they have a small bounded cardinality. In addition, it is highly recommended to set a range, whenever possible, to restrict the worst case overhead. When using the builder, the range will default from 10 ms to 1 minute. Based on data at Netflix, this is the most common range for request latencies and restricting to this window reduces the worst case multiple from 276X to 58X.","title":"Percentile Timers"},{"location":"spectator/patterns/percentile-timer/#range-recommendations","text":"The range should be the SLA boundary or failure point for the activity. Explicitly setting the range allows us to optimize for the important range of values and reduce the overhead associated with tracking the data distribution. For example, suppose you are making a client call and timeout after 10 seconds. Setting the range to 10 seconds will restrict the possible set of buckets used to those approaching the boundary. So we can still detect if it is nearing failure, but percentiles that are further away from the range may be inflated compared to the actual value.","title":"Range Recommendations"},{"location":"spectator/patterns/percentile-timer/#bucket-distribution","text":"The set of buckets is generated by using powers of 4 and incrementing by one-third of the previous power of 4 in between as long as the value is less than the next power of 4 minus the delta. Base: 1, 2, 3 4 (4^1), delta = 1 5, 6, 7, ..., 14, 16 (4^2), delta = 5 21, 26, 31, ..., 56, 64 (4^3), delta = 21 ...","title":"Bucket Distribution"},{"location":"spectator/patterns/polled-meter/","text":"Polled Meter \u00b6 Helper for configuring a meter that will receive a value by regularly polling the source in the background. Example usage: Registry registry = ... AtomicLong connections = PolledMeter . using ( registry ) . withName ( \"server.currentConnections\" ) . monitorValue ( new AtomicLong ()); // When a connection is added connections . incrementAndGet (); // When a connection is removed connections . decrementAndGet (); Polling frequency will depend on the underlying Registry implementation, but users should assume it will be frequently checked and that the provided function is cheap. Users should keep in mind that polling will not capture all activity, just sample it at some frequency. For example, if monitoring a queue, then a meter will only tell you the last sampled size when the value is reported. If more details are needed, then use an alternative type and ensure that all changes are reported when they occur. For example, consider tracking the number of currently established connections to a server. Using a polled meter will show the last sampled number when reported. An alternative would be to report the number of connections to a Distribution Summary every time a connection is added or removed. The distribution summary would provide more accurate tracking such as max and average number of connections across an interval of time. The polled meter would not provide that level of detail. If multiple values are monitored with the same id, then the values will be aggregated and the sum will be reported. For example, registering multiple meters for active threads in a thread pool with the same id would produce a value that is the overall number of active threads. For other behaviors, manage it on the user side and avoid multiple registrations.","title":"Polled Meter"},{"location":"spectator/patterns/polled-meter/#polled-meter","text":"Helper for configuring a meter that will receive a value by regularly polling the source in the background. Example usage: Registry registry = ... AtomicLong connections = PolledMeter . using ( registry ) . withName ( \"server.currentConnections\" ) . monitorValue ( new AtomicLong ()); // When a connection is added connections . incrementAndGet (); // When a connection is removed connections . decrementAndGet (); Polling frequency will depend on the underlying Registry implementation, but users should assume it will be frequently checked and that the provided function is cheap. Users should keep in mind that polling will not capture all activity, just sample it at some frequency. For example, if monitoring a queue, then a meter will only tell you the last sampled size when the value is reported. If more details are needed, then use an alternative type and ensure that all changes are reported when they occur. For example, consider tracking the number of currently established connections to a server. Using a polled meter will show the last sampled number when reported. An alternative would be to report the number of connections to a Distribution Summary every time a connection is added or removed. The distribution summary would provide more accurate tracking such as max and average number of connections across an interval of time. The polled meter would not provide that level of detail. If multiple values are monitored with the same id, then the values will be aggregated and the sum will be reported. For example, registering multiple meters for active threads in a thread pool with the same id would produce a value that is the overall number of active threads. For other behaviors, manage it on the user side and avoid multiple registrations.","title":"Polled Meter"},{"location":"spectator/specs/ipc/","text":"IPC \u00b6 This is a description of the Common IPC Metrics that can be published by various IPC libraries, with the goal of allowing consolidated monitoring and analysis across differing IPC implementations. Dimensions Common to All Metrics \u00b6 Not all dimensions are applicable for all of the metrics, and later in the sections for each specific metric, the applicable dimensions are specified. Also note that not all dimensions have been implemented or are applicable for all implementations. ipc.protocol : A short name of the network protocol in use, eg. grpc , http_1 , http_2 , udp , etc ... ipc.vip : The Eureka VIP address used to find the the server. ipc.result : Was this considered by the implementation to be successful. Allowed Values = [ success , failure ]. ipc.status : One of a predefined list of status values indicating the general result, eg. success, bad_request, timeout, etc\u2026 See the ipc.status values section below . ipc.status.detail : For cases where the ipc.status needs to be further subdivided, this tag can hold an additional more specific detail, likely ipc-implementation specific. eg status of connection_error and detail of no_servers / connect_timeout / ssl_handshake_failure. ipc.failure.injected : Indicates that an artificial failure was injected into the request processing for testing purposes. The outcome of that failure will be reflected in the other error tags. Allowed Values = [true] ipc.endpoint : The name of the endpoint/function/feature the message was sent to within the server (eg. the URL path prefix for a java servlet, or the grpc endpoint name). ipc.attempt : Which attempt at sending this message is this. Allowed Values = [ initial , second , third_up ] ( initial is the first attempt, second is 2nd attempt but first retry , third_up means third or higher attempt). ipc.attempt.final : Indicates if this request was the final attempt of potentially multiple retry attempts. Allowed Values = [ true , false ]. ipc.server.app : The nf.app of the server the message is being sent to . ipc.server.cluster : The nf.cluster of the server the message is being sent to . ipc.server.asg : The nf.asg of the server the message is being sent to . ipc.client.app : The nf.app of the server the message is being sent from . ipc.client.cluster : The nf.cluster of the server the message is being sent from . ipc.client.asg : The nf.asg of the server the message is being sent from . owner : The library/impl publishing the metrics, eg. evcache, zuul, grpc, nodequark, platform_1_ipc, geoclient, etc ... id : Conceptual name of service. Equivalent of RestClient name in NIWS. Allowed Values for ipc.status Dimension \u00b6 success : The request was successfully processed and responded to, as far as the client or server know. bad_request : There was a problem with the clients' request causing it not to be fulfilled. unexpected_error : The client or server encountered an unexpected error processing the request. connection_error : There was an error with the underlying network connection either during establishment or while in use. unavailable : There were no servers available to process the request. throttled : The request was rejected due to the client or server considering the server to be above capacity. timeout : The request could not or would not be complete within the configured threshold (either on client or server). cancelled : The client cancelled the request before it was completed. access_denied : The request was denied access for authentication or authorization reasons. Server Metrics \u00b6 ipc.server.call \u00b6 This is a percentile timer that is recorded for each inbound message to a server. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.call.size.inbound \u00b6 This is a distribution summary of the size in bytes of inbound messages received by a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.call.size.outbound \u00b6 This is a distribution summary of the size in bytes of outbound messages sent from a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id ipc.server.inflight \u00b6 This is a distribution summary that shows the number of inbound IPC messages currently being processed in a server. Unit: inflight message count Dimensions: ipc.protocol ipc.endpoint ipc.client.app ipc.client.cluster ipc.client.asg owner id Client Metrics \u00b6 ipc.client.call \u00b6 This is a percentile timer that is recorded for each outbound message from a client. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.attempt.final ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.call.size.inbound \u00b6 This is a distribution summary of the size in bytes of inbound messages received by a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.call.size.outbound \u00b6 This is a distribution summary of the size in bytes of outbound messages sent from a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id ipc.client.inflight \u00b6 This is a distribution summary that shows the number of currently outstanding outbound IPC messages from a client. Unit: inflight message count Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"IPC"},{"location":"spectator/specs/ipc/#ipc","text":"This is a description of the Common IPC Metrics that can be published by various IPC libraries, with the goal of allowing consolidated monitoring and analysis across differing IPC implementations.","title":"IPC"},{"location":"spectator/specs/ipc/#dimensions-common-to-all-metrics","text":"Not all dimensions are applicable for all of the metrics, and later in the sections for each specific metric, the applicable dimensions are specified. Also note that not all dimensions have been implemented or are applicable for all implementations. ipc.protocol : A short name of the network protocol in use, eg. grpc , http_1 , http_2 , udp , etc ... ipc.vip : The Eureka VIP address used to find the the server. ipc.result : Was this considered by the implementation to be successful. Allowed Values = [ success , failure ]. ipc.status : One of a predefined list of status values indicating the general result, eg. success, bad_request, timeout, etc\u2026 See the ipc.status values section below . ipc.status.detail : For cases where the ipc.status needs to be further subdivided, this tag can hold an additional more specific detail, likely ipc-implementation specific. eg status of connection_error and detail of no_servers / connect_timeout / ssl_handshake_failure. ipc.failure.injected : Indicates that an artificial failure was injected into the request processing for testing purposes. The outcome of that failure will be reflected in the other error tags. Allowed Values = [true] ipc.endpoint : The name of the endpoint/function/feature the message was sent to within the server (eg. the URL path prefix for a java servlet, or the grpc endpoint name). ipc.attempt : Which attempt at sending this message is this. Allowed Values = [ initial , second , third_up ] ( initial is the first attempt, second is 2nd attempt but first retry , third_up means third or higher attempt). ipc.attempt.final : Indicates if this request was the final attempt of potentially multiple retry attempts. Allowed Values = [ true , false ]. ipc.server.app : The nf.app of the server the message is being sent to . ipc.server.cluster : The nf.cluster of the server the message is being sent to . ipc.server.asg : The nf.asg of the server the message is being sent to . ipc.client.app : The nf.app of the server the message is being sent from . ipc.client.cluster : The nf.cluster of the server the message is being sent from . ipc.client.asg : The nf.asg of the server the message is being sent from . owner : The library/impl publishing the metrics, eg. evcache, zuul, grpc, nodequark, platform_1_ipc, geoclient, etc ... id : Conceptual name of service. Equivalent of RestClient name in NIWS.","title":"Dimensions Common to All Metrics"},{"location":"spectator/specs/ipc/#allowed-values-for-ipcstatus-dimension","text":"success : The request was successfully processed and responded to, as far as the client or server know. bad_request : There was a problem with the clients' request causing it not to be fulfilled. unexpected_error : The client or server encountered an unexpected error processing the request. connection_error : There was an error with the underlying network connection either during establishment or while in use. unavailable : There were no servers available to process the request. throttled : The request was rejected due to the client or server considering the server to be above capacity. timeout : The request could not or would not be complete within the configured threshold (either on client or server). cancelled : The client cancelled the request before it was completed. access_denied : The request was denied access for authentication or authorization reasons.","title":"Allowed Values for ipc.status Dimension"},{"location":"spectator/specs/ipc/#server-metrics","text":"","title":"Server Metrics"},{"location":"spectator/specs/ipc/#ipcservercall","text":"This is a percentile timer that is recorded for each inbound message to a server. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call"},{"location":"spectator/specs/ipc/#ipcservercallsizeinbound","text":"This is a distribution summary of the size in bytes of inbound messages received by a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call.size.inbound"},{"location":"spectator/specs/ipc/#ipcservercallsizeoutbound","text":"This is a distribution summary of the size in bytes of outbound messages sent from a server. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.call.size.outbound"},{"location":"spectator/specs/ipc/#ipcserverinflight","text":"This is a distribution summary that shows the number of inbound IPC messages currently being processed in a server. Unit: inflight message count Dimensions: ipc.protocol ipc.endpoint ipc.client.app ipc.client.cluster ipc.client.asg owner id","title":"ipc.server.inflight"},{"location":"spectator/specs/ipc/#client-metrics","text":"","title":"Client Metrics"},{"location":"spectator/specs/ipc/#ipcclientcall","text":"This is a percentile timer that is recorded for each outbound message from a client. Unit: seconds Dimensions: ipc.protocol ipc.result ipc.vip ipc.endpoint ipc.status ipc.status.detail ipc.failure.injected ipc.attempt ipc.attempt.final ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call"},{"location":"spectator/specs/ipc/#ipcclientcallsizeinbound","text":"This is a distribution summary of the size in bytes of inbound messages received by a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call.size.inbound"},{"location":"spectator/specs/ipc/#ipcclientcallsizeoutbound","text":"This is a distribution summary of the size in bytes of outbound messages sent from a client. Unit: bytes Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.result ipc.status ipc.status.detail ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.call.size.outbound"},{"location":"spectator/specs/ipc/#ipcclientinflight","text":"This is a distribution summary that shows the number of currently outstanding outbound IPC messages from a client. Unit: inflight message count Dimensions: ipc.protocol ipc.vip ipc.endpoint ipc.server.app ipc.server.cluster ipc.server.asg owner id","title":"ipc.client.inflight"}]}